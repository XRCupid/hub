{"version":3,"file":"static/js/4745.0e806819.chunk.js","mappings":"oMASO,MAAMA,EAAgDC,IAItD,IAJuD,cAC5DC,EAAa,cACbC,EAAa,cACbC,GACDH,EACC,MAAOI,EAAYC,IAAiBC,EAAAA,EAAAA,WAAS,IACtCC,EAAYC,IAAiBF,EAAAA,EAAAA,WAAS,IACtCG,EAASC,IAAcJ,EAAAA,EAAAA,UAA6B,MAErDK,GAAuBC,EAAAA,EAAAA,cAAY,KACvC,GAAKR,EAoDHS,QAAQC,IAAI,oCACRL,GACFA,EAAQM,OAEVL,EAAW,MACXL,GAAc,GACdG,GAAc,OA1DC,CAEfK,QAAQC,IAAI,0DAEZ,MAAME,EAAM,IAAIC,EAAAA,IAAaC,IAY3B,GAXAL,QAAQC,IAAI,sCAA6BI,GACrCjB,GACFA,EAAciB,GAIZA,EAAQC,SAAWjB,GACrBA,EAAcgB,EAAQC,SAIpBD,EAAQE,UAAYjB,EAAe,CAErC,MAAMkB,EAAUH,EAAQE,SAASE,QAAO,CAACC,EAAUC,KACjD,GAAkB,YAAdA,EAAKC,KAEP,OAAQD,EAAKE,OACX,IAAK,KACL,IAAK,KACHH,EAAII,QAAU,EACd,MACF,IAAK,KACL,IAAK,KACHJ,EAAIK,YAAc,GAClB,MACF,IAAK,KACL,IAAK,KACHL,EAAIM,eAAiB,GACrBN,EAAIO,gBAAkB,GACtB,MACF,QACEP,EAAII,QAAUI,KAAKC,IAAIT,EAAII,SAAW,EAAG,GAG/C,OAAOJ,IACN,CAAC,GAEJpB,EAAckB,EAChB,KAGFX,EAAWM,GACXA,EAAIiB,QACJ5B,GAAc,GACdG,GAAc,EAChB,IAUC,CAACJ,EAAYK,EAASR,EAAeC,EAAeC,IAEjD+B,GAAoBtB,EAAAA,EAAAA,cAAauB,IACrC,GAAIjC,EAAe,CACjB,MAAMkC,EAAaC,EAAAA,GAAgBF,GAC/BC,IACFvB,QAAQC,IAAI,qCAADwB,OAA4BH,GAAiBC,GACxDlC,EAAckC,GAElB,IACC,CAAClC,IAEJ,OACEqC,EAAAA,EAAAA,MAAA,OAAKC,MAAO,CACVC,SAAU,QACVC,IAAK,OACLC,KAAM,OACNC,WAAY,kBACZC,MAAO,QACPC,QAAS,OACTC,aAAc,MACdC,OAAQ,IACRC,SAAU,OACVC,MAAO3C,EAAa,QAAU,QAC9B4C,UAAW5C,EAAa,OAAS,OACjC6C,UAAW7C,EAAa,OAAS,SACjC8C,WAAY,gBACZC,OAAQ,mCACRC,SAAA,EACAhB,EAAAA,EAAAA,MAAA,OAAKC,MAAO,CAAEgB,QAAS,OAAQC,WAAY,SAAUC,eAAgB,gBAAiBC,aAAc,OAAQJ,SAAA,EAC1GK,EAAAA,EAAAA,KAAA,MAAIpB,MAAO,CAAEqB,OAAQ,IAAKZ,SAAU,QAASM,SAAC,2BAC7CnD,IACCwD,EAAAA,EAAAA,KAAA,UACEE,QAASA,IAAMtD,GAAeD,GAC9BiC,MAAO,CACLI,WAAY,cACZU,OAAQ,kCACRT,MAAO,QACPC,QAAS,UACTC,aAAc,MACdgB,OAAQ,UACRd,SAAU,QACVM,SAEDhD,EAAa,SAAM,eAK1BqD,EAAAA,EAAAA,KAAA,UACEE,QAASnD,EACT6B,MAAO,CACLI,WAAYxC,EAAa,UAAY,UACrCyC,MAAO,QACPS,OAAQ,OACRR,QAAS,WACTC,aAAc,MACdgB,OAAQ,UACRb,MAAO,OACPD,SAAU,OACVU,aAAcvD,GAAcG,EAAa,OAAS,KAClDgD,SAEDnD,EAAa,oBAAY,4BAG3BA,GAAcG,IACbgC,EAAAA,EAAAA,MAAA,OAAAgB,SAAA,EACEK,EAAAA,EAAAA,KAAA,KAAGpB,MAAO,CAAEqB,OAAQ,QAASZ,SAAU,OAAQe,WAAY,QAAST,SAAC,iCAGrEK,EAAAA,EAAAA,KAAA,OAAKpB,MAAO,CAAEgB,QAAS,OAAQS,oBAAqB,UAAWC,IAAK,MAAOP,aAAc,OAAQJ,SAC9FY,OAAOC,KAAK/B,EAAAA,IAAiBgC,KAAIC,IAChC/B,EAAAA,EAAAA,MAAA,UAEEuB,QAASA,IAAM5B,EAAkBoC,GACjC9B,MAAO,CACLI,WAAY,OACZC,MAAO,QACPS,OAAQ,kCACRR,QAAS,UACTC,aAAc,MACdgB,OAAQ,UACRd,SAAU,OACVsB,cAAe,aACflB,WAAY,mBAEdmB,aAAeC,GAAMA,EAAEC,cAAclC,MAAMI,WAAa,OACxD+B,aAAeF,GAAMA,EAAEC,cAAclC,MAAMI,WAAa,OAAOW,SAAA,CAE9DqB,EAAgBN,GAAY,IAAEA,IAhB1BA,QAqBX/B,EAAAA,EAAAA,MAAA,OAAKC,MAAO,CAAES,SAAU,MAAO4B,QAAS,GAAKC,WAAY,MAAOC,UAAW,kCAAmCC,WAAY,OAAQzB,SAAA,EAChIK,EAAAA,EAAAA,KAAA,UAAAL,SAAQ,4BAAqBK,EAAAA,EAAAA,KAAA,SAAK,WAChCA,EAAAA,EAAAA,KAAA,UAAAL,SAAQ,SAAa,6BAAyBK,EAAAA,EAAAA,KAAA,SAAK,WACnDA,EAAAA,EAAAA,KAAA,UAAAL,SAAQ,WAAe,qCAAiCK,EAAAA,EAAAA,KAAA,SAAK,WAC7DA,EAAAA,EAAAA,KAAA,UAAAL,SAAQ,SAAa,2BAAuBK,EAAAA,EAAAA,KAAA,SAAK,WACjDA,EAAAA,EAAAA,KAAA,UAAAL,SAAQ,cAAkB,+BAA2BK,EAAAA,EAAAA,KAAA,SAAK,WAC1DA,EAAAA,EAAAA,KAAA,UAAAL,SAAQ,UAAc,8BAA0BK,EAAAA,EAAAA,KAAA,SAAK,WACrDA,EAAAA,EAAAA,KAAA,UAAAL,SAAQ,aAAiB,+BAKhCnD,IAAeG,IACdqD,EAAAA,EAAAA,KAAA,OAAKpB,MAAO,CAAES,SAAU,OAAQ4B,QAAS,GAAKI,UAAW,OAAQ1B,SAAC,wCAQ1E,SAASqB,EAAgBM,GAWvB,MAVyC,CACvCC,MAAO,eACPC,IAAK,eACLC,MAAO,eACPC,UAAW,eACXC,OAAQ,eACRC,UAAW,eACXC,SAAU,eACVC,QAAS,gBAEKR,IAAY,cAC9B,C,oQC5KA,MAAMS,EAAkD,CACpD,GAAM,SACN,GAAM,OACN,GAAM,SACN,GAAM,SACN,GAAM,UACN,GAAM,UACN,EAAM,IACN,GAAM,UACN,EAAM,IACN,GAAM,OACN,GAAM,SACN,GAAM,SACN,GAAM,UACN,EAAM,IACN,EAAM,IACN,GAAM,IACN,GAAM,SACN,GAAM,IACN,GAAM,UACN,EAAM,IACN,EAAM,IACN,EAAM,IACN,EAAM,IACN,GAAM,SACN,GAAM,UACN,GAAM,eACN,EAAM,IACN,EAAM,SACN,EAAM,IACN,GAAM,SACN,EAAM,IACN,GAAM,SACN,GAAM,SACN,GAAM,IACN,EAAM,IACN,EAAM,IACN,EAAM,IACN,EAAM,IACN,GAAM,SACN,IAAO,UACP,IAAO,UACP,GAAM,UAKJC,EAA0E,CAC5E,CAAEC,WAAY,CAAC,WAAYC,GAAI,GAC/B,CAAED,WAAY,CAAC,OAAK,SAAK,UAAMC,GAAI,GACnC,CAAED,WAAY,CAAC,UAAMC,GAAI,GACzB,CAAED,WAAY,CAAC,UAAMC,GAAI,GACzB,CAAED,WAAY,CAAC,SAAK,UAAMC,GAAI,GAC9B,CAAED,WAAY,CAAC,UAAMC,GAAI,GACzB,CAAED,WAAY,CAAC,IAAK,IAAK,UAAMC,GAAI,GACnC,CAAED,WAAY,CAAC,IAAK,KAAMC,GAAI,GAC9B,CAAED,WAAY,CAAC,KAAMC,GAAI,GACzB,CAAED,WAAY,CAAC,WAAOC,GAAI,GAC1B,CAAED,WAAY,CAAC,gBAAOC,GAAI,IAC1B,CAAED,WAAY,CAAC,WAAOC,GAAI,IAC1B,CAAED,WAAY,CAAC,KAAMC,GAAI,IACzB,CAAED,WAAY,CAAC,UAAMC,GAAI,IACzB,CAAED,WAAY,CAAC,KAAMC,GAAI,IACzB,CAAED,WAAY,CAAC,IAAK,KAAMC,GAAI,IAC9B,CAAED,WAAY,CAAC,SAAK,UAAM,UAAM,UAAMC,GAAI,IAC1C,CAAED,WAAY,CAAC,QAAMC,GAAI,IACzB,CAAED,WAAY,CAAC,IAAK,KAAMC,GAAI,IAC9B,CAAED,WAAY,CAAC,IAAK,IAAK,IAAK,UAAMC,GAAI,IACxC,CAAED,WAAY,CAAC,IAAK,IAAK,UAAMC,GAAI,IACnC,CAAED,WAAY,CAAC,IAAK,IAAK,KAAMC,GAAI,KAwChC,SAASC,EACZC,EACAC,GAEA,MAAMC,EAA8B,GAC9BC,EAAkBpE,KAAKqE,MAA6B,IAAvBH,GAEnC,OAAKD,GAAwC,IAAxBA,EAAaK,QAKlCL,EAAaM,SAAQ,CAACC,EAAWC,KAC7B,MAAMC,EAAiBF,EAAUG,QAAQC,cAKnCC,EAvDd,SAAiCC,GAC7B,GAAkB,OAAdA,EAEA,OADAhG,QAAQiG,KAAK,6FACN,EAEX,GAAkB,YAAdD,EAAyB,OAAO,EAEpC,IAAK,MAAME,KAASnB,EAChB,GAAImB,EAAMlB,WAAWmB,SAASH,GAC1B,OAAOE,EAAMjB,GAKrB,MAAkB,YAAde,EAA2B,EACb,YAAdA,EAA2B,GAE/BhG,QAAQiG,KAAK,4CAADxE,OAA6CuE,EAAS,yCAC3D,EACX,CAoCyBI,MAJsCC,IAArCvB,EAAiBc,GACbd,EAAiBc,GACjB,MAGhBU,EAAmBpF,KAAKqE,MAAuB,IAAjBG,EAAUa,MAE9ClB,EAAamB,KAAK,CACdC,YAAaH,EACbP,SAAUA,EACVW,aAAcf,IAAUR,EAAaK,OAAS,OAKtDH,EAAasB,MAAK,CAACC,EAAGC,IAAMD,EAAEH,YAAcI,EAAEJ,cAM9CzG,QAAQC,IAAI,+DAAgEoF,GAC5ErF,QAAQC,IAAI,0CAA2CqF,GAEhD,CAAED,eAAcC,qBA9BnBtF,QAAQiG,KAAK,6EACN,CAAEZ,eAAcC,mBA8B/B,CASA,IAAIwB,EAEG,MAeMC,EAA8BC,eAEzCC,GAEkC,IADlCC,EAAiBC,UAAA3B,OAAA,QAAAa,IAAAc,UAAA,GAAAA,UAAA,GAAG,oBAGpB,OADAnH,QAAQC,IAAI,kEAADwB,OAAmEwF,EAAI,eAAAxF,OAAcyF,EAAS,kBAAAzF,OAAiB2F,KAAKC,QACxH,IAAIC,SAAQ,CAACC,EAASC,KAC3BxH,QAAQC,IAAI,gFAADwB,OAAiFwF,EAAI,kBAAAxF,OAAiB2F,KAAKC,QAEtH,MAAMI,EAAYC,uFACZC,EAAeD,SASrB,IACEZ,EAAec,EAAAA,aAAuBC,iBAAiBJ,EAAWE,GAClE3H,QAAQC,IAAI,uEAAwE6G,GACpFA,EAAagB,4BAA8BF,EAAAA,4BAAsCG,4BAGjF,MAAMC,EAAI,mKAAAvG,OAESyF,EAAS,yEAAAzF,OAEpBwF,EAAI,0CAIZ,IAAIgB,EAAc,IAAIL,EAAAA,kBAA4Bd,EAAc,MAChE9G,QAAQC,IAAI,uFAAwF+H,GAEpG,IAAIE,EAAkC,CAAEC,gBAAiB,GAAIC,iBAAkB,IAC3EC,GAAiB,EAErB,MAAMC,EAAUA,KACd,GAAIL,EAAa,CACf,IACEA,EAAYM,OACd,CAAE,MAAOC,GACPxI,QAAQyI,MAAM,wDAAyDD,EACzE,CACAP,EAAc,IAChB,GAGFA,EAAYS,eAAiB,CAACC,EAAG/E,KAE/B,GADA5D,QAAQC,IAAI,qDAADwB,OAAsDmC,EAAE6C,YAAc,IAAK,kBAAAhF,OAAiBmC,EAAEmC,WACrGnC,EAAEgF,WAAoC,KAAvBhF,EAAEgF,UAAUC,OAAe,CAC5C7I,QAAQC,IAAI,uDAADwB,OAAwDmC,EAAEgF,UAAUpD,SAC/ExF,QAAQC,IAAI,kEAADwB,OAAmEmC,EAAEgF,UAAUE,UAAU,EAAG,OAGvG,IACE,MAAMC,EAAgBC,KAAKC,MAAMrF,EAAEgF,WACnC,GAAIG,EAAcG,kBAA4C7C,IAA7B0C,EAAcI,WAA0B,CACvE,MAAMC,EAAiBL,EAAcI,WACrCJ,EAAcG,YAAYzD,SAAQ,CAAC4D,EAAsBC,KACvDpB,EAAoBE,iBAAiB5B,KAAK,CACxC+C,WAAYH,EAAiBE,EAC7BE,OAAQH,EACR5C,YAAc7C,EAAE6C,YAAc,IAAU6C,GAAmB,KAAQP,EAAcU,WAAa,SAGpG,MACEvB,EAAoBC,gBAAgB3B,KAAK,CACvCkD,SAAU9F,EAAEmC,SACZU,YAAa7C,EAAE6C,YAAc,KAGnC,CAAE,MAAOgC,GACPzI,QAAQyI,MAAM,kEAAmEA,EAAO,+BAAgC7E,EAAEgF,WAC1HV,EAAoBC,gBAAgB3B,KAAK,CACvCkD,SAAU9F,EAAEmC,SACZU,YAAa7C,EAAE6C,YAAc,KAEjC,CACF,MACEzG,QAAQC,IAAI,sEACZiI,EAAoBC,gBAAgB3B,KAAK,CACvCkD,SAAU9F,EAAEmC,SACZU,YAAa7C,EAAE6C,YAAc,OAKnCwB,EAAY0B,eACV3B,GACA4B,IACE,GAAIvB,EACFC,QADF,CAIA,GADAD,GAAiB,EACbuB,EAAOC,SAAWjC,EAAAA,aAAuBkC,2BAA4B,CACvE5B,EAAoBE,iBAAiBzB,MAAK,CAACC,EAAGC,IAAMD,EAAE2C,WAAa1C,EAAE0C,aACrErB,EAAoBC,gBAAgBxB,MAAK,CAACC,EAAEC,IAAMD,EAAEH,YAAcI,EAAEJ,cACpEzG,QAAQC,IAAI,+CAAgD2J,EAAOG,eACnE,MAAMzE,EAAkBsE,EAAOG,cAAgB,IAC/C/J,QAAQC,IAAI,+DAADwB,OAAgEmI,EAAOG,cAAa,qBAAAtI,OAAoB6D,EAAe,OAClItF,QAAQC,IAAI,gDAADwB,OAAiDwF,EAAI,wBAAAxF,OAAuB6D,EAAe,iBAAA7D,OAAgB2F,KAAKC,QAC3HE,EAAQ,CAAEyC,UAAWJ,EAAOI,UAAWC,WAAY/B,EAAqB5C,gBAAiBA,GAC3F,MAAO,GAAIsE,EAAOC,SAAWjC,EAAAA,aAAuBsC,SAAU,CAC5D,MAAMC,EAAevC,EAAAA,oBAA8BwC,WAAWR,GACxDS,OAAqChE,IAAxB8D,EAAaN,OAAuBjC,EAAAA,mBAA6BuC,EAAaN,QAAU,gBAC3G7J,QAAQyI,MAAM,sDAADhH,OAAuD4I,EAAU,eAAA5I,OAAc0I,EAAaG,aAAY,iBAAA7I,OAAgB0I,EAAaI,YAClJvK,QAAQC,IAAI,gDAADwB,OAAiDwF,EAAI,eAAAxF,OAAc4I,EAAU,iBAAA5I,OAAgB2F,KAAKC,QAC7GG,EAAO,uBAAD/F,OAAwB4I,EAAU,eAAA5I,OAAc0I,EAAaG,aAAY,iBAAA7I,OAAgB0I,EAAaI,WAC9G,MACEvK,QAAQyI,MAAM,oDAADhH,OAAqDmI,EAAOC,OAAM,eAAApI,OAAcmI,EAAOU,eACpGtK,QAAQC,IAAI,qDAADwB,OAAsDwF,EAAI,eAAAxF,OAAcmI,EAAOC,OAAM,iBAAApI,OAAgB2F,KAAKC,QACrHG,EAAO,IAAIgD,MAAM,qBAAD/I,OAAsBmI,EAAOU,cAAgB,mBAE/DhC,GArBA,KAuBFG,IACMJ,IAGJA,GAAiB,EACjBrI,QAAQC,IAAI,sDAADwB,OAAuDwF,EAAI,cAAAxF,OAAagH,EAAK,iBAAAhH,OAAgB2F,KAAKC,QAC7GG,EAAOiB,IAJLH,MAQR,CAAE,MAAOG,GACPzI,QAAQyI,MAAM,+DAAgEA,GAC9EzI,QAAQC,IAAI,mDAADwB,OAAoDwF,EAAI,cAAAxF,OAAagH,EAAK,iBAAAhH,OAAgB2F,KAAKC,QAC1GG,EAAOiB,EACT,IAEJ,E,eC/PA,MAyBMgC,EAAyCC,EAAAA,EAAyBjK,QAAO,CAACC,EAAKiK,KACnFjK,EAAIiK,GAAa,EACVjK,IACN,CAAC,GAGEkK,EAAgF,CACpF,EAAG,CAAE9J,QAAS,EAAG+J,WAAY,EAAGC,YAAa,EAAG/J,YAAa,EAAGC,eAAgB,EAAGC,gBAAiB,GACpG,EAAG,CAAEH,QAAS,GAAKiK,gBAAiB,IACpC,EAAG,CAAEjK,QAAS,GAAKC,YAAa,IAChC,EAAG,CAAED,QAAS,GAAKC,YAAa,GAAK+J,YAAa,IAClD,EAAG,CAAEhK,QAAS,GAAKE,eAAgB,GAAKC,gBAAiB,IACzD,EAAG,CAAEH,QAAS,IAAMiK,gBAAiB,GAAKC,SAAU,IACpD,EAAG,CAAElK,QAAS,GAAKE,eAAgB,GAAKC,gBAAiB,IACzD,EAAG,CAAEH,QAAS,IAAMgK,YAAa,GAAK/J,YAAa,IACnD,EAAG,CAAED,QAAS,GAAKgK,YAAa,GAAK/J,YAAa,IAClD,EAAG,CAAED,QAAS,GAAKC,YAAa,GAAK+J,YAAa,IAClD,GAAI,CAAEhK,QAAS,GAAKC,YAAa,GAAKC,eAAgB,IACtD,GAAI,CAAEF,QAAS,GAAKE,eAAgB,GAAKC,gBAAiB,IAC1D,GAAI,CAAEH,QAAS,GAAKiK,gBAAiB,KACrC,GAAI,CAAEjK,QAAS,GAAKgK,YAAa,GAAKE,SAAU,IAChD,GAAI,CAAElK,QAAS,IAAMkK,SAAU,GAAKhK,eAAgB,IACpD,GAAI,CAAEF,QAAS,IAAM+J,WAAY,GAAK7J,eAAgB,GAAKC,gBAAiB,IAC5E,GAAI,CAAEH,QAAS,IAAMgK,YAAa,GAAK/J,YAAa,IACpD,GAAI,CAAED,QAAS,GAAKkK,SAAU,GAAKC,mBAAoB,GAAKC,oBAAqB,IACjF,GAAI,CAAEpK,QAAS,IAAMmK,mBAAoB,GAAKC,oBAAqB,GAAKC,eAAgB,GAAKC,gBAAiB,IAC9G,GAAI,CAAEtK,QAAS,GAAKkK,SAAU,GAAKH,WAAY,IAC/C,GAAI,CAAE/J,QAAS,GAAKuK,gBAAiB,GAAKL,SAAU,IACpD,GAAI,CAAElK,QAAS,EAAG+J,WAAY,EAAGC,YAAa,MAI1CQ,EAAoCZ,EAAAA,EAAyBjK,QAAO,CAACC,EAAoB6K,KAC7F7K,EAAI6K,GAAO,EACJ7K,IACN,CAAC,GAGE8K,EAA0Cd,EAAAA,EAAyBjK,QAAO,CAACC,EAA6B6K,KAChG,iBAARA,GAAkC,kBAARA,IAE5B7K,EAAI6K,GAAO,GAEN7K,IACN,CAAC,GAuGJ,MAAM+K,EAA0B,CAC9B,2CACA,2CACA,2CACA,2CACA,2CACA,2CACA,2CACA,2CACA,2CACA,2CACA,wBAGIC,EAA0B,CAC9B,sCACA,sCACA,iDACA,iDACA,iDACA,iDACA,iDACA,iDACA,iDACA,iDACA,iDACA,iDACA,wBAMIC,EAAmC,CACvC5J,WAAY,kBACZU,OAAQ,kBACRP,aAAc,MACdD,QAAS,WACTD,MAAO,QACPkB,OAAQ,UACRd,SAAU,QACVY,OAAQ,OAGJ4I,EAAyBC,EAAAA,YAG7B,CAACC,EAAOC,KACR,MAAM,aACJC,EAAY,qBACZC,EAAoB,gBACpBC,EAAe,mBACfC,EAAkB,gBAClBC,EAAe,aACfC,EAAY,aACZC,GACER,EAEJ9L,QAAQC,IAAI,+DAADwB,OAAgEqK,EAAMK,qBAEjFnM,QAAQC,IAAI,gEAAiE6L,EAAMK,mBAAoB,mBAAsD,kBAA1BL,EAAMI,iBAAgCK,EAAAA,GAAgBT,EAAMI,iBAAoBK,EAAAA,GAAgBT,EAAMI,iBAAmBM,OAAOV,EAAMI,kBAEzQ,MAAM,YAAEO,IAAgBC,EAAAA,EAAAA,MAClB,KAAEC,EAAMC,QAASC,EAAapE,MAAOqE,IAAcC,EAAAA,EAAAA,OAEnD,aAAEC,KADSC,EAAAA,EAAAA,OACQC,EAAAA,EAAAA,OA6BlBC,EAAOC,IAPDtB,EAAMuB,iBAAsB,OAAJV,QAAI,IAAJA,GAAAA,EAAMW,YAOjB7N,EAAAA,EAAAA,WAA8B,KACtD,MAAM8N,EAAU,CACdC,SAAS,EACTC,YAAY,EACZC,WAAW,EACXC,YAAY,EACZC,SAAU,GACVC,WAAY,GACZC,YAAY,EACZC,oBAAoB,EACpBC,mBAAmB,EACnBC,eAAgB,UAChBC,cAAe,kBACfC,cAAe,OACfC,eAAgB,kBAChBC,oBAAqB7C,EACrB8C,kBAAmBhD,EACnBiD,eAAgB,KAChB9F,MAAO,KACP+F,WAAW,EACXlB,UAAWxB,EAAMuB,iBAAsB,OAAJV,QAAI,IAAJA,OAAI,EAAJA,EAAMW,YAAa,WACtDmB,iBAAiB,EACjBC,YAAY,EACZC,UAAU,EACVC,yBAA0BnE,GAG5B,OADAzK,QAAQC,IAAI,sEAADwB,OAAuE8L,EAAQD,UAAS,kCAAA7L,OAAiCqK,EAAMuB,eAAc,uBAAA5L,OAA0B,OAAJkL,QAAI,IAAJA,OAAI,EAAJA,EAAMW,YAC7KC,MAIHsB,GAAWC,EAAAA,EAAAA,QAAO3B,GAGlB4B,GAAoBhP,EAAAA,EAAAA,cAAasE,IAErC+I,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAEf,eAAgB5J,EAAQ6K,KAAKC,oBACzD,CAAC/B,IAGEgC,EAAa1H,oDAGjB2H,eAAgBC,EAChBC,gBAAiBC,EACjBC,UAAWC,IACTC,EAAAA,EAAAA,GACFP,EACAL,EACA,CACEa,yBAA0CzC,EAAMM,WAChDoC,UAAW1C,EAAMM,cAIrBqC,EAAAA,EAAAA,YAAU,KACR9P,QAAQC,IAAI,gDAAiDuP,EAAkC,cAAeE,GAC1GA,GACFtC,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAEd,cAAc,yBAADzM,OAA2BiO,SAEtE,CAACF,EAAkCE,IAGtC,MAAMK,GAAgBjB,EAAAA,EAAAA,QAAgC,MAChDkB,GAAelB,EAAAA,EAAAA,QAAgC,MAC/CmB,GAAkBnB,EAAAA,EAAAA,QAA4B,IAC9CoB,GAAsBpB,EAAAA,EAAAA,QAAsB,MAC5CqB,GAA2BrB,EAAAA,EAAAA,QAA8B,MACzDsB,GAAwCtB,EAAAA,EAAAA,QAAoB,MAC5DuB,GAAqCvB,EAAAA,EAAAA,QAAsB,MAC3DwB,GAAoBxB,EAAAA,EAAAA,SAAgB,GAEpCyB,IADuBzB,EAAAA,EAAAA,QAAqC,OACpCA,EAAAA,EAAAA,QAAOhD,EAAMK,sBAG3C2D,EAAAA,EAAAA,YAAU,KACRjB,EAAS2B,QAAUrD,IAClB,CAACA,KAEJ2C,EAAAA,EAAAA,YAAU,KAGR9P,QAAQC,IAAI,8DAADwB,OAA+D0L,EAAMG,UAAS,wCAAA7L,OAAuCqK,EAAMuB,eAAc,+BAAA5L,OAAkC,OAAJkL,QAAI,IAAJA,OAAI,EAAJA,EAAMW,cACvL,CAACH,EAAMG,UAAWxB,EAAMuB,eAAgBV,KAE3CmD,EAAAA,EAAAA,YAAU,KACR9P,QAAQC,IAAI,8EAADwB,OAA+EqK,EAAMuB,eAAc,+BAAA5L,OAA8B0L,EAAMG,YAC3I,KAELtN,QAAQC,IAAI,sFAADwB,OAAuFqK,EAAMuB,eAAc,kCAAA5L,OAAiCoN,EAAS2B,QAAQlD,eAGzK,IAEH,MAAMmD,IAAsB1Q,EAAAA,EAAAA,cAAa0I,IACvC2E,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAEd,cAAc,iBAADzM,OAAmBgH,EAAMpI,eAClE,IAEGqQ,IAAqB3Q,EAAAA,EAAAA,cAAY,KACrCqN,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAEd,cAAe,sBAC3C,KAGH4B,EAAAA,EAAAA,YAAU,KACRS,EAAsBC,QAAU1E,EAAMK,mBACtCnM,QAAQC,IAAI,mFAADwB,OAAoF8O,EAAsBC,QAAO,8BAAA/O,OAA6BqK,EAAMK,mBAAkB,QAChL,CAACL,EAAMK,qBAGV,MAAMwE,IAAiB7B,EAAAA,EAAAA,QAA8B,MAC/C8B,IAAiB9B,EAAAA,EAAAA,QAA2B,MAC5C+B,IAAqB/B,EAAAA,EAAAA,SAAegC,EAAAA,EAAAA,MAOpCC,IAAsBhR,EAAAA,EAAAA,cAAY,KAClCmQ,EAAoBM,UACtBQ,qBAAqBd,EAAoBM,SACzCN,EAAoBM,QAAU,MAGhCpD,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAEX,oBAAqB7C,EAAiBsC,YAAY,MAC/E9N,QAAQC,IAAI,wHACX,CAACmN,EAAU8C,EAAqB5E,IAE7B2F,IAAsBlR,EAAAA,EAAAA,cAAakK,IAEvC,GADAjK,QAAQC,IAAI,mEAAoEgK,IAC3EA,IAAeA,EAAW7B,kBAA2D,IAAvC6B,EAAW7B,iBAAiB5C,OAE7E,OADAxF,QAAQiG,KAAK,yEACN,GAGT,MAAMiL,EAA8BjH,EAAW7B,iBAAiB5E,KAAI2N,IAClE,MAAM3H,EAAiC,CAAC,EAIxC,OAHAkB,EAAAA,EAAyBjF,SAAQ,CAACyJ,EAAqBvJ,KACrD6D,EAAO0F,GAAQiC,EAAM3H,OAAO7D,MAEvB,CACLY,KAAM4K,EAAM1K,YACZ+C,OAAQA,MAIZ,OADAxJ,QAAQC,IAAI,0CAADwB,OAA2CyP,EAAO1L,OAAM,gBAC5D0L,IACN,IAEGE,IAAuCrR,EAAAA,EAAAA,cAAY,CACvDsF,EACAC,KAGA,GADAtF,QAAQC,IAAI,8DAADwB,OAA+D4D,EAAaG,OAAM,8BAAA/D,OAA6B6D,EAAe,QACpID,GAAwC,IAAxBA,EAAaG,OAChC,OAAIF,EAAkB,GAEpBtF,QAAQC,IAAI,+FACL,CACL,CAAEsG,KAAM,EAAGiD,OAAQoB,EAAsC,IAAMU,GAC/D,CAAE/E,KAAMjB,EAAkB,IAAMkE,OAAQoB,EAAsC,IAAMU,MAGxFtL,QAAQC,IAAI,+GACL,IAGT,MAAMoR,EAAiChM,EAAa7B,KAAI8N,IAAK,CAC3D/K,KAAM+K,EAAM7K,YAAc,IAC1B+C,OAAQoB,EAAsC0G,EAAMvL,WAAauF,MAI7DiG,EAA0BF,EAAU7L,OAAS,EAAI6L,EAAUA,EAAU7L,OAAS,GAAGe,KAAO,EACxFnB,EAAuBE,EAAkB,IAa/C,OAXyB,IAArB+L,EAAU7L,QAAgB+L,EAA0BnM,KAE3B,IAArBiM,EAAU7L,QAAgBJ,EAAuB,GACjDiM,EAAU7K,KAAK,CAAED,KAAM,EAAGiD,OAAQoB,EAAsC,IAAMU,IAElF+F,EAAU7K,KAAK,CACXD,KAAMnB,EACNoE,OAAQoB,EAAsC,IAAMU,KAG5DtL,QAAQC,IAAI,4DAADwB,OAA6D4P,EAAU7L,OAAM,gBACjF6L,IACN,CAAC/F,IAEEkG,IAAiBzR,EAAAA,EAAAA,cAAa0R,IAClC,MAAMC,GAAetK,KAAKC,MAAQoK,GAAa,IACzCE,EAAgB1B,EAAgBO,QAEtC,IAAKmB,GAA0C,IAAzBA,EAAcnM,OAGlC,OAFAxF,QAAQC,IAAI,0EACZ8Q,KAIF,IAAIa,EAAc,KAClB,IAAK,IAAIC,EAAI,EAAGA,EAAIF,EAAcnM,QAC5BkM,GAAeC,EAAcE,GAAGtL,KADIsL,IAEtCD,EAAcD,EAAcE,GAM5BD,IACFxE,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAEX,oBAAqBuD,EAAYpI,WAC9DxJ,QAAQC,IAAI,mEAAoE2R,EAAYpI,SAI9F,GAAIkI,GADkBC,EAAcnM,OAAS,EAAImM,EAAcA,EAAcnM,OAAS,GAAGe,KAAO,GAC9D,KAASqL,GAAeF,EAAc,IAAOC,EAAcnM,OAAS,EAGpG,OAFAxF,QAAQC,IAAI,6FACZ8Q,KAIFb,EAAoBM,QAAUsB,uBAAsB,IAAMN,GAAeC,OACxE,CAACrE,EAAU2D,GAAqBd,EAAiBC,IAE9C6B,IAAuBhS,EAAAA,EAAAA,cAAY,KAEvC,GADAC,QAAQC,IAAI,sDAAuDgQ,EAAgBO,QAAQhL,QACvFyK,EAAgBO,SAAWP,EAAgBO,QAAQhL,OAAS,EAAG,CACjExF,QAAQC,IAAI,wEAAyEgQ,EAAgBO,SACrG,MAAMiB,EAAYrK,KAAKC,MACnB6I,EAAoBM,SACtBQ,qBAAqBd,EAAoBM,SAE3CgB,GAAeC,EACjB,MACEzR,QAAQC,IAAI,+DACZ8Q,OAED,CAACS,GAAgBT,GAAqBb,EAAqBD,IAiExD+B,KA/D4BjS,EAAAA,EAAAA,cAAYiH,MAAOiL,EAAkB1R,KACrEP,QAAQC,IAAI,2EACZmN,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAEjB,oBAAoB,EAAOC,mBAAmB,MAGvE+B,EAAcS,UAChBT,EAAcS,QAAQ0B,QACtBnC,EAAcS,QAAQ2B,IAAM,IAE9BpB,KAEKf,EAAaQ,UAChBR,EAAaQ,QAAU,IAAI4B,MAC3BpC,EAAaQ,QAAQ6B,iBAAiB,SAAS,KAC7CrS,QAAQC,IAAI,uCACZ8Q,KACA3D,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAElB,YAAY,EAAOE,mBAAmB,SAErEgC,EAAaQ,QAAQ6B,iBAAiB,SAAUzO,IAC9C5D,QAAQyI,MAAM,4CAA6C7E,GAC3DmN,KACA3D,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAElB,YAAY,EAAOE,mBAAmB,UAIvEgC,EAAaQ,QAAQ2B,IAAMF,EAC3BjC,EAAaQ,QAAQ8B,OAErB,UACQtC,EAAaQ,QAAQ+B,OAC3BvS,QAAQC,IAAI,yCACZmN,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAElB,YAAY,MAGzC,MAAM1I,EAAuB4K,EAAaQ,QAAQgC,UAC9CC,MAAMrN,IAAkD,IAAzBA,IACjCpF,QAAQiG,KAAK,yHAKfjG,QAAQC,IAAI,4DAADwB,OAA6D2D,EAAoB,MAG5F,MAAMsN,EAAYxN,EAAkC3E,EAAU6E,GAGxDiM,EAAYD,GAAqCsB,EAAUrN,aAAcqN,EAAUpN,iBAErF+L,GAAaA,EAAU7L,OAAS,GAClCyK,EAAgBO,QAAUa,EAC1BU,MAEA/R,QAAQC,IAAI,uGAGhB,CAAE,MAAOwI,GACPzI,QAAQyI,MAAM,sFAAuFA,GACrGsI,KACA3D,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAElB,YAAY,EAAOE,mBAAmB,KACrE,IACC,CAAC+C,GAAqBgB,GAAsBX,GAAsC9F,EAAoBkG,MAEvEzR,EAAAA,EAAAA,cAAaM,IAC7CL,QAAQC,IAAI,2EACZmN,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAElB,YAAY,EAAMI,cAAe,+BAC7D,CAACd,KAEEuF,IAA4B5S,EAAAA,EAAAA,cAAY,KAC5CC,QAAQC,IAAI,+CACP4O,EAAS2B,QAAQzC,mBAKpB/N,QAAQC,IAAI,0HAJZD,QAAQC,IAAI,iHACZ8Q,KACA3D,GAAU4B,IAAyBC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAWD,GAAI,IAAElB,YAAY,EAAOI,cAAe,cAIvF,CAAC6C,GAAqB3D,EAAUyB,IAE7B+D,IAAwB7S,EAAAA,EAAAA,cAAY,KACxCC,QAAQC,IAAI,2CACZD,QAAQC,IAAI,6EACZqQ,EAAkBE,SAAU,EAC5BO,KACA3D,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAEjB,oBAAoB,MAC7CgC,EAAcS,SAAWT,EAAcS,QAAQ2B,IAAIU,WAAW,WAChEC,IAAIC,gBAAgBhD,EAAcS,QAAQ2B,KAC1CpC,EAAcS,QAAQ2B,IAAM,GAC5BpC,EAAcS,QAAQwC,UAAY,KAClChT,QAAQC,IAAI,oFAEb,CAAC8Q,GAAqB3D,EAAU2C,IAE7BkD,IAAwBlT,EAAAA,EAAAA,cAAauR,IACzC,MAAM4B,EAAe5B,EAAM6B,OAC3BnT,QAAQyI,MAAM,8DAA+DyK,EAAazK,OAC1FsI,KACA3D,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAEjB,oBAAoB,EAAOG,cAAe,iCACnE6B,EAAcS,SAAWT,EAAcS,QAAQ2B,IAAIU,WAAW,WAChEC,IAAIC,gBAAgBhD,EAAcS,QAAQ2B,KAC1CpC,EAAcS,QAAQ2B,IAAM,GAC5BpC,EAAcS,QAAQwC,UAAY,KAClChT,QAAQC,IAAI,gGAEb,CAAC8Q,GAAqB3D,EAAU2C,IAG7BqD,IAAoCrT,EAAAA,EAAAA,cAAYiH,MAAOC,EAAcC,KAEzE,GADAlH,QAAQC,IAAI,gFAADwB,OAAiFwF,EAAI,cAAAxF,OAAayF,IACxG6I,EAAcS,QAInB,IAAK,IAAD6C,EACFjG,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAEd,cAAe,qBAAsBJ,YAAY,EAAMC,oBAAoB,MACnGgC,EAAcS,QAAQ8C,SACzBvD,EAAcS,QAAQ0B,QACtBnC,EAAcS,QAAQkB,YAAc,GAEtC,MAAM6B,QAAsBC,EAAmCvM,EAAMC,GAAa2H,EAAS2B,QAAQpC,gBAInG,GADApO,QAAQC,IAAI,qFAAsF+I,KAAKyK,UAAUF,EAAe,KAAM,IACrH,OAAbA,QAAa,IAAbA,GAAyB,QAAZF,EAAbE,EAAetJ,kBAAU,IAAAoJ,GAAzBA,EAA2BjL,kBAAoBmL,EAActJ,WAAW7B,iBAAiB5C,OAAS,EAAG,CACvGxF,QAAQC,IAAI,4FACZ,IAAK,IAAI4R,EAAI,EAAGA,EAAI3Q,KAAKwS,IAAI,EAAGH,EAActJ,WAAW7B,iBAAiB5C,QAASqM,IAAK,CAAC,IAAD8B,EACtF3T,QAAQC,IAAI,WAADwB,OAAYoQ,EAAC,kBAAApQ,OAAiB8R,EAActJ,WAAW7B,iBAAiByJ,GAAGpL,YAAW,aAAAhF,OAAYuH,KAAKyK,UAA6D,QAApDE,EAACJ,EAActJ,WAAW7B,iBAAiByJ,GAAGrI,cAAM,IAAAmK,OAAA,EAAnDA,EAAqDC,MAAM,EAAE,IAAG,OAC9L,CACA,MAAMC,EAAeN,EAActJ,WAAW7B,iBAAiB5C,OAAS,EAChD,IAADsO,EAAvB,GAAID,GAAgB,EAChB7T,QAAQC,IAAI,gBAADwB,OAAiBoS,EAAY,kBAAApS,OAAiB8R,EAActJ,WAAW7B,iBAAiByL,GAAcpN,YAAW,aAAAhF,OAAYuH,KAAKyK,UAAwE,QAA/DK,EAACP,EAActJ,WAAW7B,iBAAiByL,GAAcrK,cAAM,IAAAsK,OAAA,EAA9DA,EAAgEF,MAAM,EAAE,IAAG,OAExO,MACE5T,QAAQC,IAAI,6HASd,GAPIsT,GAAiB,oBAAqBA,EACxCvT,QAAQC,IAAI,6EAA8EsT,EAAcjO,iBAExGtF,QAAQC,IAAI,+FAIVsT,GAAiBA,EAAcvJ,WAAauJ,EAActJ,WAAY,CACxE,MAAM8J,EAAiB9C,GAAoBsC,EAActJ,YACrD8J,GAAkBA,EAAevO,OAAS,GAC5CyK,EAAgBO,QAAUuD,EAC1BhC,MAEAhB,KAEF,MAAMiD,EAAY,IAAIC,KAAK,CAACV,EAAcvJ,WAAY,CAAEpJ,KAAM,eACxDqR,EAAWa,IAAIoB,gBAAgBF,GACrCjE,EAAcS,QAAQwC,UAAY,KAClCjD,EAAcS,QAAQ2B,IAAMF,EAC5BlC,EAAcS,QAAQ+B,OAAO4B,OAAMvQ,IACjC5D,QAAQyI,MAAM,wEAAyE7E,GACvFgP,OAEJ,MACE5S,QAAQyI,MAAM,yFACdmK,IAEJ,CAAE,MAAOnK,GACPzI,QAAQyI,MAAM,sEAAuEA,GACrFmK,IACF,MAvDE5S,QAAQyI,MAAM,gFAwDf,CAAC+K,EAAoCvC,GAAqBc,GAAsBhB,GAAqB6B,GAAuBxF,EAAUyB,EAAUkB,EAAeE,IAE5JmE,IAA4BrU,EAAAA,EAAAA,cAAYiH,UAG9C,GAFEhH,QAAQC,IAAI,oDAAqDgH,IAE9D8I,EAAcS,QAGjB,OAFAxQ,QAAQyI,MAAM,oGACdmK,KAIFxF,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAElB,YAAY,EAAMC,oBAAoB,EAAMG,cAAe,8BAElF6B,EAAcS,UAAYT,EAAcS,QAAQ8C,SAClDtT,QAAQiG,KAAK,kHACb8J,EAAcS,QAAQ0B,QACtBnC,EAAcS,QAAQkB,YAAc,GAGtC,IAAK,IAAD2C,EAAAC,EAAAC,EAAAC,EAAAC,EACFzU,QAAQC,IAAI,2EAA4EkN,EAAMiB,gBAC9F,MAAMsG,EAAoBvH,EAAMiB,eAE1BmF,QAAsBC,EAAmCvM,EAAMyN,GAGrE,GAFA1U,QAAQC,IAAI,+DAAgE+I,KAAKyK,UAAUF,EAAgB,CAAEoB,gBAAwC,QAAzBN,EAAEd,EAAcvJ,iBAAS,IAAAqK,OAAA,EAAvBA,EAAyBO,WAAYC,uBAAgD,QAA1BP,EAAEf,EAActJ,kBAAU,IAAAqK,GAAkB,QAAlBC,EAAxBD,EAA0BlM,wBAAgB,IAAAmM,OAAlB,EAAxBA,EAA4C/O,OAAQsP,sBAA+C,QAA1BN,EAAEjB,EAActJ,kBAAU,IAAAuK,GAAiB,QAAjBC,EAAxBD,EAA0BrM,uBAAe,IAAAsM,OAAjB,EAAxBA,EAA2CjP,QAAW,OAExT+N,GAAiBA,EAAcvJ,WAAauJ,EAActJ,WAAY,CAAC,IAAD8K,EAAAC,EACxEhV,QAAQC,IAAI,8FAAwI,QAA3C8U,EAAExB,EAActJ,WAAW7B,wBAAgB,IAAA2M,OAAA,EAAzCA,EAA2CvP,OAAQ,yBAAkE,QAA1CwP,EAAEzB,EAActJ,WAAW9B,uBAAe,IAAA6M,OAAA,EAAxCA,EAA0CxP,OAAQ,sCAC1O,MAAMuO,EAAiB9C,GAAoBsC,EAActJ,YACzDjK,QAAQC,IAAI,sDAAuD8T,GAC/DA,GAAkBA,EAAevO,OAAS,GAC5CyK,EAAgBO,QAAUuD,EAC1B/T,QAAQC,IAAI,2EAA4EgQ,EAAgBO,QAAQhL,UAEhHyK,EAAgBO,QAAU,GAC1BxQ,QAAQC,IAAI,qHAEd,MAAM+T,EAAY,IAAIC,KAAK,CAACV,EAAcvJ,WAAY,CAAEpJ,KAAM,eACxDqR,EAAWa,IAAIoB,gBAAgBF,GACrCjE,EAAcS,QAAQwC,UAAY,KAClCjD,EAAcS,QAAQ2B,IAAMF,EAC5BlC,EAAcS,QAAQ+B,OAAO0C,MAAK,KAChCjV,QAAQC,IAAI,4EACRgQ,EAAgBO,SAAWP,EAAgBO,QAAQhL,OAAS,GAC9DxF,QAAQC,IAAI,8FACZ8R,OAEA/R,QAAQC,IAAI,kHACZ8Q,SAEDoD,OAAMvQ,IACP5D,QAAQyI,MAAM,iEAAkE7E,GAChFgP,QAEF5S,QAAQC,IAAI,gHACZqQ,EAAkBE,SAAU,CAC9B,MACExQ,QAAQyI,MAAM,uGAAwG8K,GACtHX,IAEJ,CAAE,MAAOnK,GACPzI,QAAQyI,MAAM,wEAAyEA,GACvFmK,IACF,IACC,CAACY,EAAoCzD,EAAe4C,GAA2B1B,GAAqBc,GAAsBhB,GAAqBR,EAAuBqC,GAAuB3C,KAIhMH,EAAAA,EAAAA,YAAU,KACR,GAAkC,IAA9BhE,EAAME,aAAaxG,OACrB,OAGF,MAAM0P,EAAiBpJ,EAAME,aAAaF,EAAME,aAAaxG,OAAS,GACtE,GAAK0P,EAAL,CAGA,GAA4B,iBAAxBA,EAAetU,MACjB,GAAIwP,EAAsCI,SAAW0E,EAAeC,YAAc/E,EAAsCI,QAEtH,YAEG,GAA4B,sBAAxB0E,EAAetU,MAAgCsU,EAAejQ,IACnEiQ,EAAejQ,KAAOoL,EAAmCG,QAE3D,OASJ,OAJ4B,sBAAxB0E,EAAetU,MAAgCsU,EAAeE,QAChEpV,QAAQC,IAAI,8DAA+D+I,KAAKyK,UAAUyB,EAAgB,KAAM,IAG1GA,EAAetU,MACrB,IAAK,eAAgB,CACnB,IAAKsU,EAAe7U,SAAqD,kBAAnC6U,EAAe7U,QAAQwE,SAAwD,SAAhCqQ,EAAe7U,QAAQgV,KAAiB,CAC3HrV,QAAQiG,KAAK,sFAAuF+C,KAAKyK,UAAUyB,EAAgB,KAAM,IACzI,KACF,CACAlV,QAAQC,IAAI,iCAADwB,OAAkCyT,EAAe7U,QAAQgV,KAAI,gBAAgBH,EAAe7U,QAAQwE,SAC/G,MAAMyQ,EAAkC,CACtCrQ,IAAI6L,EAAAA,EAAAA,KACJ7J,KAAMiO,EAAe7U,QAAQwE,QAC7B0Q,OAAQ,OACRC,UAAWN,EAAeC,YAE5B/H,GAAU4B,IAAyBC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAWD,GAAI,IAAEpB,SAAU,IAAIoB,EAAKpB,SAAU0H,OACjFlF,EAAsCI,QAAU0E,EAAeC,WAC/D,KACF,CACA,IAAK,oBAAqB,CAAC,IAADM,EAAAC,EAAAC,EAAAC,EAQxB,GAPA5V,QAAQC,IAAI,+DAAgEiV,EAAejQ,IAClE,QAAzBwQ,EAAIP,EAAeE,cAAM,IAAAK,GAAS,QAATC,EAArBD,EAAuBnV,eAAO,IAAAoV,GAA9BA,EAAgCG,OAClC7V,QAAQC,IAAI,qCAAsC+I,KAAKyK,UAAUyB,EAAeE,OAAO9U,QAAQuV,OAAQ,KAAM,IAE7G7V,QAAQC,IAAI,8DAA+D+I,KAAKyK,UAAUyB,EAAeE,OAAQ,KAAM,KAGpHF,EAAe7U,SAAqD,kBAAnC6U,EAAe7U,QAAQwE,SAAwD,cAAhCqQ,EAAe7U,QAAQgV,KAAsB,CAChIrV,QAAQiG,KAAK,2FAA4F+C,KAAKyK,UAAUyB,EAAgB,KAAM,IAC9I,KACF,CACAlV,QAAQC,IAAI,sCAADwB,OAAuCyT,EAAe7U,QAAQgV,KAAI,qBAAqBH,EAAe7U,QAAQwE,SACzH,MAAMiR,EAAuC,CAC3C7Q,GAAIiQ,EAAejQ,KAAM6L,EAAAA,EAAAA,KACzB7J,KAAMiO,EAAe7U,QAAQwE,QAC7B0Q,OAAQ,MACRC,UAAWN,EAAeC,YAG5B,IAAIY,EAAyB,CAAC,CAAE7G,KAAM,UAAW8G,MAAO,IACpDC,EAAiB,oBAKrB,GAAyB,QAArBN,EAAAT,EAAeE,cAAM,IAAAO,GAAS,QAATC,EAArBD,EAAuBrV,eAAO,IAAAsV,GAA9BA,EAAgCC,QAAUK,MAAMC,QAAQjB,EAAeE,OAAO9U,QAAQuV,SAAWX,EAAeE,OAAO9U,QAAQuV,OAAOrQ,OAAS,EAAG,CAEpJ,MAAM4Q,EAAoBlB,EAAeE,OAAO9U,QAAQuV,OAAOQ,QAC5D1N,GAA6B,kBAAXA,EAAEuG,MAAwC,kBAAZvG,EAAEqN,QAEjDI,EAAkB5Q,OAAS,GAC7BuQ,EAAcK,EACdH,EAAiB,mBAEjBjW,QAAQiG,KAAK,kFAAmF+C,KAAKyK,UAAUyB,EAAeE,OAAO9U,QAAQuV,SAC7II,EAAiB,qCAErB,MACEjW,QAAQC,IAAI,qFAEdD,QAAQC,IAAI,2DAADwB,OAA4DwU,EAAc,MAAMjN,KAAKyK,UAAUsC,IAE1G3I,GAAU4B,IACR,MAAMsH,EAA8BP,EAAYtV,QAAO,CAACC,EAA6B2D,KACnF3D,EAAI2D,EAAQ6K,MAAQ7K,EAAQ2R,MACrBtV,IACN,CAAC,GAEJV,QAAQC,IAAI,oEAAqE+I,KAAKyK,UAAUsC,IAChG/V,QAAQC,IAAI,oFAAqF+I,KAAKyK,UAAU6C,IAEhH,MAAMC,GAAwBC,EAAAA,EAAAA,GAAqBF,GAInD,OAFAtW,QAAQC,IAAI,uFAADwB,OAAwF6B,OAAOC,KAAKgT,GAAuB/Q,OAAM,MAAMwD,KAAKyK,UAAU8C,KAEjKtH,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GACKD,GAAI,IACPpB,SAAU,IAAIoB,EAAKpB,SAAUkI,GAC7BW,wBAAyBV,EACzBjI,WAAYhC,EAAMK,mBAClB+B,cAAepC,EAAMK,mBAAqB,0BAA6B6C,EAAKd,cAAc/H,SAAS,SAAW6I,EAAKd,cAAgB,WACnIU,yBAA0B2H,OAI1BrB,EAAe7U,QAAQwE,QAIvBqQ,EAAejQ,KACjBoL,EAAmCG,QAAU0E,EAAejQ,IAE9D,KACF,CACA,IAAK,gBACHjF,QAAQC,IAAI,4CAA6CiV,GAEzD,MAEF,IAAK,QACHlV,QAAQyI,MAAM,6CAA8CO,KAAKyK,UAAUyB,EAAgB,KAAM,IACjG9H,GAAU4B,IAAyBC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAWD,GAAI,IAAEd,cAAc,mBAADzM,OAAqByT,EAAe7U,aACrG,MAEF,IAAK,gBACHL,QAAQC,IAAI,4CAA6CiV,GACzD,MAEF,IAAK,oBACHlV,QAAQC,IAAI,gDAAiDiV,GACzDnF,EAAcS,UAChBT,EAAcS,QAAQ0B,QACtBnC,EAAcS,QAAQ2B,IAAM,IAE9BpB,KACA3D,GAAU4B,IAAyBC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAWD,GAAI,IAAElB,YAAY,EAAOC,oBAAoB,EAAOG,cAAe,4BACjH,MAEF,IAAK,YACHlO,QAAQC,IAAI,wCAAyCiV,GACrD,MAEF,IAAK,gBACHlV,QAAQC,IAAI,4CAA6CiV,GACzD,MAEF,IAAK,aACHlV,QAAQC,IAAI,yCAA0CiV,GACtD,MAEF,QAEElV,QAAQC,IAAI,2DAA6DiV,EAAuBtU,KAAM,gBAAiBoI,KAAKyK,UAAUyB,EAAgB,KAAM,IAKhK,MAAO,KACD/E,EAAyBK,UAC3BkG,aAAavG,EAAyBK,SACtCL,EAAyBK,QAAU,MA9JZ,IAiK1B,CAAC1E,EAAME,aAAcoI,GAA2BtI,EAAMK,sBAOzD2D,EAAAA,EAAAA,YAAU,KACJhE,EAAMK,oBAEJgE,EAAyBK,UAC3BkG,aAAavG,EAAyBK,SACtCL,EAAyBK,QAAU,MAIrCwB,MAKInD,EAAS2B,QAAQ1C,aACfqC,EAAyBK,SAC3BkG,aAAavG,EAAyBK,SAExCL,EAAyBK,QAAUmG,YAAW,KAC5C3W,QAAQC,IAAI,oGACZ0S,OA3tBgB,MAiuBf,KACDxC,EAAyBK,UAC3BkG,aAAavG,EAAyBK,SACtCL,EAAyBK,QAAU,SAGtC,CAAC1E,EAAMK,mBAAoB6F,GAA2BW,GAA2B9D,EAAUsB,IAE9F,MAAMyG,IAAoB7W,EAAAA,EAAAA,cAAauR,IACrClE,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAEnB,WAAYyD,EAAM6B,OAAOtS,YACrD,CAACuM,IAEEyJ,IAAoB9W,EAAAA,EAAAA,cAAYiH,UAChCsK,GACFA,EAAMwF,iBAER,MAAM7P,EAAO4H,EAAS2B,QAAQ3C,WAAWhF,OACzC,IAAK5B,EAAM,OAEX,MAAM8P,EAAiB,CACrB9R,IAAI6L,EAAAA,EAAAA,KACJ7J,KAAMA,EACNsO,OAAQ,OACRC,UAAW,IAAIpO,MAGjBgG,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GACRD,GAAI,IACPpB,SAAU,IAAIoB,EAAKpB,SAAUmJ,GAC7BC,UAAW,OAGbhX,QAAQC,IAAI,8CAA+CgH,GACvD6E,EAAMG,qBACNH,EAAMG,qBAAqBhF,GAE3BjH,QAAQiG,KAAK,yEAWhB,CAACmH,EAAUtB,EAAMG,qBAAsBH,EAAMkB,aAAcP,EAAaoC,EAAUgC,KAuB/EoG,KArB8BlX,EAAAA,EAAAA,cAAamX,IAC/ClX,QAAQC,IAAI,qCAAsCiX,KACjD,KAEuBnX,EAAAA,EAAAA,cAAa0I,IACrCzI,QAAQyI,MAAM,0BAA2BA,GAEzCzI,QAAQyI,MAAM,sCAAuCA,GACrD,MAAM0O,EAAe1O,aAAiB+B,MAAQ/B,EAAMpI,QAAUmM,OAAO/D,GACrE2E,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAEd,cAAc,iBAADzM,OAAmB0V,SAC5D,CAAC/J,KAEqBrN,EAAAA,EAAAA,cAAY,KACnCC,QAAQC,IAAI,yCACZmN,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAEd,cAAe,uBAC3C,CAACd,KAEiBrN,EAAAA,EAAAA,cAAY,KAC/BqN,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAEP,iBAAkBO,EAAKP,sBACnD,CAACrB,KAEcrN,EAAAA,EAAAA,cAAY,KAC5BqN,GAAU4B,IACRhP,QAAQC,IAAI,yDAADwB,OA74BjB,SAAgC0L,GAC9B,OAAQA,GACN,KAAKZ,EAAAA,GAAgB6K,KAAM,MAAO,OAClC,KAAK7K,EAAAA,GAAgB8K,WAAY,MAAO,aACxC,KAAK9K,EAAAA,GAAgB+K,KAAM,MAAO,OAElC,KAAK/K,EAAAA,GAAgBgL,OAAQ,MAAO,SACpC,QAKE,OAFAvX,QAAQiG,KAAK,0EAADxE,OAA2E0L,IAEjF,kBAAN1L,OAAyB+K,OAAOW,GAAM,KAE5C,CA+3B2EqK,CAAuB1L,EAAMI,iBAAgB,0CAAAzK,OAAyCuN,EAAKN,aAChK,MAAM+I,GAAoBzI,EAAKN,WAmB/B,OAlBI+I,GACFzX,QAAQC,IAAI,4DACZ6L,EAAMM,oBAENpM,QAAQC,IAAI,sEACZD,QAAQC,IAAI,+DAADwB,OAAgEoN,EAAS2B,QAAQkH,gBAAkB,MAAQ,UAAS,oBAAAjW,OAAmBoN,EAAS2B,QAAQlE,cAAgB,yCAEnLR,EAAMO,eACH4I,MAAK,KACJjV,QAAQC,IAAI,8EAGbkU,OAAM1L,IACLzI,QAAQyI,MAAM,mDAAoDA,GAClE2E,GAAUzE,IAAsBsG,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUtG,GAAC,IAAEuF,cAAc,yBAADzM,OAA4BgH,EAAgBpI,kBAI5G4O,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAYD,GAAI,IAAEN,WAAY+I,EAAkBjK,SAAUiK,EAAkBvJ,cAAeuJ,EAAmB,aAAe,+CAE9H,CAACrK,EAAUtB,EAAMO,aAAcP,EAAMM,gBAAiByC,EAAU/C,EAAMI,mBAEnEyL,IAAa5X,EAAAA,EAAAA,cAAY,KAC7BqN,GAAU4B,IAAyBC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAWD,GAAI,IAAEL,UAAWK,EAAKL,eACnE,CAACvB,IAEEwK,IAAc7X,EAAAA,EAAAA,cAAY,KAC9BqN,GAAU4B,IACR,MAAM6I,GAAgB7I,EAAKtB,UAI3B,OAHIqC,EAAcS,UAChBT,EAAcS,QAAQsH,OAASD,IAEjC5I,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAYD,GAAI,IAAEtB,UAAWmK,EAAc3J,cAAe2J,EAAe,iBAAmB,sBAE7F,CAACzK,EAAU2C,IAERgI,IAAoBhY,EAAAA,EAAAA,cAAY,KACpCC,QAAQC,IAAI,+CAER8P,EAAcS,UAAYT,EAAcS,QAAQ8C,SAClDvD,EAAcS,QAAQ0B,QAClBnC,EAAcS,QAAQ2B,KAAOpC,EAAcS,QAAQ2B,IAAIU,WAAW,UACpEC,IAAIC,gBAAgBhD,EAAcS,QAAQ2B,KAE5CpC,EAAcS,QAAQ2B,IAAM,IAE9BpB,KAEA3D,GAAU4B,IAAyBC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAC9BD,GAAI,IAEPjB,oBAAoB,EAEpBG,cAAe,sDAIhB,CAAC6B,EAAegB,GAAqB3D,KAExC4K,EAAAA,EAAAA,qBAAoBjM,GAAK,MACvBkM,sBAAuBA,KACrBjY,QAAQC,IAAI,gEACR8P,EAAcS,UAAYT,EAAcS,QAAQ8C,SAClDvD,EAAcS,QAAQ0B,QAClBnC,EAAcS,QAAQ2B,KAAOpC,EAAcS,QAAQ2B,IAAIU,WAAW,UACpEC,IAAIC,gBAAgBhD,EAAcS,QAAQ2B,KAE5CpC,EAAcS,QAAQ2B,IAAM,IAG9BpB,KACA3D,GAAU8K,IAA8BjJ,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GACnCiJ,GAAS,IAEZnK,oBAAoB,EACpBG,cAAegK,EAAUnK,mBAAqB,0CAA4C,iDAM5F,CAACgC,EAAegB,GAAqB3D,IAGzC,IAAI+K,GAAoBhL,EAAMW,WAC1BsK,GAAuBjL,EAAMe,cAmBjC,OAhBIf,EAAMY,qBAGCjC,EAAMK,oBAEfgM,IAAoB,EACpBC,GAAuB,4BAGvBD,IAAoB,EACpBC,GAAuB,SAIzBpY,QAAQC,IAAI,sDAAuD6L,EAAMK,mBAAoB,4BAA6BgB,EAAMW,WAAY,qBAAsBqK,GAAmB,wBAAyBC,KAG5M1W,EAAAA,EAAAA,MAAA,OAAK2W,UAAU,kBAAkB1W,MAAO,CAAEgB,QAAS,OAAQ2V,OAAQ,QAASC,cAAe,SAAUxW,WAAY,WAAYW,SAAA,EAE3HhB,EAAAA,EAAAA,MAAA,OAAK2W,UAAU,aAAa1W,MAAO,CAAEM,QAAS,OAAQF,WAAY,UAAWC,MAAO,QAASwW,UAAW,SAAUpW,SAAU,QAASqW,WAAY,GAAI/V,SAAA,CAAC,WAC3I0V,GAAqB,YAA4C,kBAA1BtM,EAAMI,iBAAgCK,EAAAA,GAAgBT,EAAMI,iBAAoBK,EAAAA,GAAgBT,EAAMI,iBAAmBM,OAAOV,EAAMI,iBAAiB,gBAAciM,GAAoB,MAAQ,KAAK,oBAAkBrM,EAAMK,mBAAqB,MAAQ,SAI7SpJ,EAAAA,EAAAA,KAAC7D,EAAAA,eAAc,CACbE,cAAgBiB,IAGd,GAFAL,QAAQC,IAAI,sCAA6BI,GAErCA,EAAQC,QAAS,CAEnB,MAAMoY,EAAgBrY,EAAQC,QAAQG,QAAO,CAACC,EAA6B2D,KACzE3D,EAAI2D,EAAQ6K,MAAQ7K,EAAQ2R,MACrBtV,IACN,CAAC,GACEiY,GAAqBnC,EAAAA,EAAAA,GAAqBkC,GAChDtL,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAEJ,yBAA0B+J,KACzD,CACA,GAAItY,EAAQE,SAAU,CAEpB,MAAMqY,EAAc,CAAE9X,QAAS,GAAKC,YAAa,IACjDqM,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAEX,oBAAqBuK,KACpD,CACqB,iBAAjBvY,EAAQO,OACVwM,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAElB,YAAY,MAEzC6I,YAAW,KACTvJ,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAElB,YAAY,EAAOO,oBAAqB,CAAC,QACrE,OAGPhP,cAAgBwZ,IACd7Y,QAAQC,IAAI,sCAA6B4Y,GAEzC,MAAMH,EAAgBG,EAASpY,QAAO,CAACC,EAA6B2D,KAClE3D,EAAI2D,EAAQ6K,MAAQ7K,EAAQ2R,MACrBtV,IACN,CAAC,GACEiY,GAAqBnC,EAAAA,EAAAA,GAAqBkC,GAChDtL,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAEJ,yBAA0B+J,OAEzDrZ,cAAgBkB,IACdR,QAAQC,IAAI,sCAA6BO,GACzC4M,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAEX,oBAAqB7N,UAKtDkB,EAAAA,EAAAA,MAAA,OAAKC,MAAO,CAAEgB,QAAS,OAAQmW,SAAU,EAAGlX,SAAU,WAAYmX,SAAU,UAAWrW,SAAA,EAErFhB,EAAAA,EAAAA,MAAA,OAAK2W,UAAU,mBAAmB1W,MAAO,CAAEmX,SAAU,EAAGnW,QAAS,OAAQE,eAAgB,SAAUD,WAAY,SAAUhB,SAAU,WAAYG,WAAY,WAAYW,SAAA,EAC3J1C,QAAQC,IAAI,8EAA+EkY,IAA2B,OAChIpV,EAAAA,EAAAA,KAACiW,EAAAA,QAAmB,CAClBC,mBAAiBhK,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAO9B,EAAMmB,mBAAsBnB,EAAMyB,0BAE1D7C,IAAK6E,GACLtD,UAAWH,EAAMG,UACjBQ,WAAYqK,GACZlO,WAAYkD,EAAMkB,oBAClBJ,eAAgBd,EAAMc,eACtBiL,WAAY1N,EACZ2N,iBAAkBhM,EAAMc,eAAiB,CAAC,CAAEiB,KAAM/B,EAAMc,eAAgB+H,MAAO,IAAS,GACxFoD,cAAejM,EAAMsB,gBACrB4K,mBAAoB5N,EACpB6N,mBAAoB5N,EACpB6N,QAAS9I,GACT+I,OAAQ9I,IAZHvD,EAAMG,YAcbvK,EAAAA,EAAAA,KAAA,UACEE,QAASA,IAAMmQ,GAAkC,+BAAgCjG,EAAMiB,gBAAkB,qBACzGzM,OAAKsN,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAOtD,GAAW,IAAE/J,SAAU,WAAYC,IAAK,OAAQC,KAAM,OAAQK,OAAQ,KAAKO,SACxF,oBAMFyK,EAAMwB,WACLjN,EAAAA,EAAAA,MAAA,OAAK2W,UAAU,UAAU1W,MAAO,CAC9BU,MAAO,QACPoX,SAAU,QACV1X,WAAY,yBACZC,MAAO,QACPW,QAAS,OACT4V,cAAe,SACftW,QAAS,OACTyX,WAAY,oBACZC,UAAW,aACX/X,SAAU,WACVgY,MAAO,EACP/X,IAAK,EACLgY,OAAQ,EACR1X,OAAQ,EACRK,WAAY,6BACZsX,UAAW,kBACXpX,SAAA,EACAhB,EAAAA,EAAAA,MAAA,OAAKC,MAAO,CAAEgB,QAAS,OAAQE,eAAgB,gBAAiBD,WAAY,SAAUE,aAAc,OAAQ2V,WAAY,GAAI/V,SAAA,EAC1HK,EAAAA,EAAAA,KAAA,MAAIpB,MAAO,CAAEqB,OAAQ,EAAGZ,SAAU,SAAUM,SAAC,kBAC7CK,EAAAA,EAAAA,KAAA,UAAQE,QAAS0U,GAAYhW,MAAO,CAAEI,WAAY,OAAQU,OAAQ,OAAQT,MAAO,OAAQkB,OAAQ,UAAWd,SAAU,QAASH,QAAS,OAAQS,SAAC,UAInJhB,EAAAA,EAAAA,MAAA,OAAK2W,UAAU,gBAAgB1W,MAAO,CAAEmX,SAAU,EAAGvW,UAAW,OAAQO,aAAc,OAAQiX,aAAc,QAASrX,SAAA,CAClHyK,EAAMS,SAASpK,KAAKwW,IACnBtY,EAAAA,EAAAA,MAAA,OAAkB2W,UAAS,WAAA5W,OAAauY,EAAIzE,QAAU5T,MAAO,CAC3DmB,aAAc,OACdb,QAAS,YACTC,aAAc,OACd+X,SAAU,MACVC,UAA0B,SAAfF,EAAIzE,OAAoB,WAAa,aAChDxT,WAA2B,SAAfiY,EAAIzE,OAAoB,UAAY,UAChDvT,MAAO,QACPwW,UAA0B,SAAfwB,EAAIzE,OAAoB,QAAU,OAC7C4E,WAA2B,QAAfH,EAAIzE,OAAmB,IAAM,OACzC6E,YAA4B,SAAfJ,EAAIzE,OAAoB,IAAM,OAC3C8E,SAAU,aACVjY,SAAU,UACVM,SAAA,CACCsX,EAAI/S,KACJ+S,EAAI3V,UAAW3C,EAAAA,EAAAA,MAAA,MAAIC,MAAO,CAACS,SAAU,QAASO,QAAS,QAASqB,QAAS,GAAKI,UAAW,OAAO1B,SAAA,CAAC,IAAEsX,EAAI3V,QAAQ,SAfxG2V,EAAI/U,OAkBhBlC,EAAAA,EAAAA,KAAA,OAAKgJ,IAAK4E,SAEZ5N,EAAAA,EAAAA,KAAA,SACEgJ,IAAKgE,EACLuK,QAAS1H,GACT2G,QAAStG,GACTtR,MAAO,CAAEgB,QAAS,WAEpBjB,EAAAA,EAAAA,MAAA,QAAM6Y,SAAU1D,GAAmBlV,MAAO,CAAEgB,QAAS,OAAQyB,UAAW,OAAQD,WAAY,OAAQD,UAAW,oBAAqBuU,WAAY,GAAI/V,SAAA,EAClJK,EAAAA,EAAAA,KAAA,SACEnC,KAAK,OACLC,MAAOsM,EAAMU,WACb2M,SAAU5D,GACV6D,YAAY,uBACZ9Y,MAAO,CACLmX,SAAU,EACV7W,QAAS,YACTQ,OAAQ,iBACRP,aAAc,OACdkY,YAAa,OACbM,QAAS,OACT1Y,MAAO,UACPD,WAAY,UACZK,SAAU,UAGdW,EAAAA,EAAAA,KAAA,UACEnC,KAAK,SACL+Z,UAAWxN,EAAMU,WAAWhF,OAC5BlH,MAAO,CACLI,WAAYoL,EAAMU,WAAWhF,OAAS,UAAY,OAClD7G,MAAO,QACPS,OAAQ,OACRP,aAAc,MACdG,MAAO,OACPiW,OAAQ,OACR3V,QAAS,OACTC,WAAY,SACZC,eAAgB,SAChBK,OAAQiK,EAAMU,WAAWhF,OAAS,UAAY,cAC9C7E,QAASmJ,EAAMU,WAAWhF,OAAS,EAAI,GACvCrG,WAAY,8BACZE,SACH,aAOLyK,EAAMwB,WACN5L,EAAAA,EAAAA,KAAA,UACEE,QAAS0U,GACThW,MAAO,CACLC,SAAU,WACViY,OAAQ,OACRD,MAAO,OACP7X,WAAY,UACZC,MAAO,QACPS,OAAQ,OACRP,aAAc,MACdG,MAAO,OACPiW,OAAQ,OACR3V,QAAS,OACTC,WAAY,SACZC,eAAgB,SAChBK,OAAQ,UACR0X,UAAW,6BACXzY,OAAQ,GACRK,WAAY,uBAEdqY,YAAcjX,GAAMA,EAAEC,cAAclC,MAAMmY,UAAY,aACtDgB,WAAalX,GAAMA,EAAEC,cAAclC,MAAMmY,UAAY,WAAWpX,SACjE,aAOLhB,EAAAA,EAAAA,MAAA,OAAK2W,UAAU,eAAe1W,MAAO,CACnCgB,QAAS,OACTE,eAAgB,SAChBD,WAAY,SACZX,QAAS,OACTF,WAAY,UACZ0W,WAAY,EACZpV,IAAK,QACLX,SAAA,EACAhB,EAAAA,EAAAA,MAAA,UAAQuB,QAASgU,GAAWtV,MAAOgK,EAAaoP,MAAO5N,EAAMuB,WAAa,oBAAsB,kBAAkBhM,SAAA,CAC/GyK,EAAMuB,YAAa3L,EAAAA,EAAAA,KAACiY,EAAAA,IAAiB,KAAMjY,EAAAA,EAAAA,KAACkY,EAAAA,IAAY,IAAI,IAAE9N,EAAMuB,WAAa,UAAY,aAEhGhN,EAAAA,EAAAA,MAAA,UAAQuB,QAAS2U,GAAajW,MAAOgK,EAAaoP,MAAO5N,EAAMO,UAAY,aAAe,eAAehL,SAAA,CACtGyK,EAAMO,WAAY3K,EAAAA,EAAAA,KAACmY,EAAAA,IAAU,KAAMnY,EAAAA,EAAAA,KAACoY,EAAAA,IAAY,IAAI,IAAEhO,EAAMO,UAAY,WAAa,gBAExFhM,EAAAA,EAAAA,MAAA,UAAQuB,QAAS8U,GAAmBpW,OAAKsN,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAMtD,GAAW,IAAE5J,WAAY,YAAYgZ,MAAM,kCAAiCrY,SAAA,EACzHK,EAAAA,EAAAA,KAACqY,EAAAA,IAAY,IAAG,8BAObC,EAAiDvP,IAC5D,MAAOwP,EAA2BC,IAAgC9b,EAAAA,EAAAA,UAAiB,oBAC7E+b,GAAkB1M,EAAAA,EAAAA,QAAqC,MACvDM,EAAavD,EAAAA,SAAc,KAC/B,MAAMN,EAAM7D,mDAIZ,OAAO6D,GAAO,KACb,IAEGe,EAAe5E,wCAGnBkG,SAAU5B,EACVyP,cAAexP,EACfyP,WAAYxP,EACZyP,UAAWxP,EACXyP,WAAYxP,EACZyP,QAASxP,EAAY,OACrByP,IACEC,EAAAA,EAAAA,MAEJ/b,QAAQC,IACN,gDAAiDkM,EACjD,UAAW2P,GAGb,MAAME,GAAkBjc,EAAAA,EAAAA,cAAY,KAClCC,QAAQC,IAAI,qDACX,IAEGgc,GAAYC,EAAAA,EAAAA,UAAQ,MACxBtb,KAAM,SACNC,MAAOuO,KACL,CAACA,IAEC+M,GAAqBpc,EAAAA,EAAAA,cAAaM,IACtC,MAAM+b,EAAc/b,EAAQO,KAI5B,OAFAZ,QAAQC,IAAI,uCAADwB,OAAwC2a,IAE3CA,GACN,IAAK,oBAEH,GADApc,QAAQC,IAAI,wCAAyCI,GACjDA,EAAQ+U,OAAQ,CAClB,MAAMiH,EAAchc,EAAQ+U,OACxBiH,EAAYC,MAAQD,EAAYC,KAAKC,aAEvCvc,QAAQC,IAAI,qCAAsCoc,EAAYC,KAAKC,YAMvE,CACA,MACF,IAAK,oBACHvc,QAAQC,IAAI,wCAAyCI,GACrD,MACF,IAAK,YACHL,QAAQC,IAAI,gCAAiCI,GAC7C,MACF,IAAK,gBACHL,QAAQC,IAAI,oCAAqCI,GACjD,MACF,IAAK,aACHL,QAAQyI,MAAM,iCAAkCpI,MAUnD,IAEGmc,GAAmBzc,EAAAA,EAAAA,cAAa6D,IAAY,IAAD6Y,EAC/Czc,QAAQC,IAAI,wDAAyD2D,EAAE8Y,KAAM,UAAW9Y,EAAEiG,OAAQ,YAAajG,EAAE+Y,UAC1F,QAAvBF,EAAAjB,EAAgBhL,eAAO,IAAAiM,GAAvBA,EAAyBxE,0BACxB,IAEG2E,GAAmB7c,EAAAA,EAAAA,cAAa0I,IACpC,IAAIoU,EAAU,0CAAApb,OAA6CgH,EAAM7H,KAAI,YAAAa,OAAWgH,EAAMiU,KAAI,eAAAjb,OAAcgH,EAAMpI,SAC9GL,QAAQyI,MAAMoU,EAAYpU,KACzB,IAWH,OATK2G,GACFpP,QAAQiG,KAAK,8FAKhBjG,QAAQC,IAAI,uEAADwB,OAAwE2N,EAAU,sBAAA3N,OAAqB6K,EAAY,MAC9HtM,QAAQC,IAAI,kDAAmDgc,IAG7Dva,EAAAA,EAAAA,MAAAob,EAAAA,SAAA,CAAApa,SAAA,EACEhB,EAAAA,EAAAA,MAAA,OAAKC,MAAO,CAAEM,QAAS,MAAO8a,gBAAiB,UAAW/a,MAAO,QAASwW,UAAW,SAAUrV,WAAY,OAAQf,SAAU,QAASD,OAAQ,KAAMP,SAAU,YAAac,SAAA,CAAC,4BAChJ4Y,MAE5BvY,EAAAA,EAAAA,KAACia,EAAAA,GAAa,CACdC,aAAcA,KACZjd,QAAQC,IAAI,mDACZsb,EAA6B,wBAE/B2B,WAAYA,KACVld,QAAQC,IAAI,iDACZsb,EAA6B,sBAE/B4B,KAAMlB,EACNmB,SAAU9Q,EACV+Q,OAAO,EACPC,OAAQtB,EACRuB,UAAWpB,EACXqB,QAAShB,EACTjD,QAASqD,EAAiBla,UAE1BK,EAAAA,EAAAA,KAAC6I,EAAsB,CACnBG,IAAKyP,EACLxO,aAAclB,EAAMkB,aACpBK,eAAgBvB,EAAMuB,eACtBrB,aAAcA,EACdC,qBAAsBA,EACtBC,gBAAiBA,EACjBC,mBAAoBA,EACpBC,gBAAiBA,EACjBC,aAAcA,EAEdC,aAAcA,S,sGC1hDxB,MAGMmR,EAAwE,CAC5EC,IAAK,CACH1c,eAAgB,IAChBC,gBAAiB,IACjB0c,gBAAiB,GACjBC,iBAAkB,GAClBC,YAAa,GACbC,aAAc,GACdC,gBAAiB,EACjBC,iBAAkB,EAClBC,UAAW,IAEbC,QAAS,CACPC,eAAgB,IAChBC,gBAAiB,IACjBC,YAAa,EACbC,aAAc,GACdC,cAAe,GACfC,gBAAiB,GACjBC,iBAAkB,GAClBxT,mBAAoB,GACpBC,oBAAqB,IAEvBwT,MAAO,CACLJ,aAAc,IACdC,cAAe,IACfF,YAAa,GACblT,eAAgB,GAChBC,gBAAiB,GACjBuT,cAAe,GACfC,eAAgB,GAChBC,cAAe,GACfC,eAAgB,GAChBC,iBAAkB,GAClBC,kBAAmB,IAErBC,SAAU,CACRne,QAAS,GACT+c,YAAa,IACbC,aAAc,IACdC,gBAAiB,IACjBC,iBAAkB,IAClBK,YAAa,GACbtd,YAAa,IAEfme,KAAM,CACJC,iBAAkB,GAClBC,kBAAmB,GACnBvB,YAAa,EACbC,aAAc,EACdO,YAAa,EACbN,gBAAiB,GACjBC,iBAAkB,GAClBld,QAAS,IAEXue,QAAS,CACPR,cAAe,GACfC,eAAgB,GAChBC,iBAAkB,GAClBC,kBAAmB,GACnBL,cAAe,GACfC,eAAgB,GAChBN,aAAc,GACdC,cAAe,IAEjBe,SAAU,CACRte,eAAgB,GAChBC,gBAAiB,GACjB0d,cAAe,GACfC,eAAgB,GAChBN,aAAc,GACdC,cAAe,IAEjBgB,WAAY,CACVve,eAAgB,EAChBC,gBAAiB,EACjB4c,YAAa,GACbC,aAAc,GACdC,gBAAiB,GACjBC,iBAAkB,GAClBL,gBAAiB,GACjBC,iBAAkB,GAClB9c,QAAS,IAEX0e,YAAa,CACXxe,eAAgB,GAChBC,gBAAiB,GACjB0d,cAAe,GACfC,eAAgB,GAChBb,gBAAiB,GACjBC,iBAAkB,IAEpByB,UAAW,CACTnB,aAAc,GACdN,iBAAkB,GAClBK,YAAa,GACbvT,YAAa,GACb6T,cAAe,GACf7d,QAAS,IAEX4e,KAAM,CACJ1e,eAAgB,IAChBC,gBAAiB,IACjB0d,cAAe,GACfC,eAAgB,IAGlBe,UAAW,CACT3e,eAAgB,GAChBC,gBAAiB,GACjB0c,gBAAiB,GACjBC,iBAAkB,GAClBG,gBAAiB,GACjBC,iBAAkB,IAEpB4B,cAAe,CACbtB,aAAc,GACdC,cAAe,GACfF,YAAa,GACbM,cAAe,GACfC,eAAgB,KAIPpI,EAAwBqJ,IACnC7f,QAAQC,IAAI,qDAAsDqD,OAAOC,KAAKsc,GAAeC,KAAK,OAGlG,MAAMC,EAA6BrV,EAAAA,EAAyBjK,QAAO,CAACC,EAAoBiK,KACtFjK,EAAIiK,GAAa,EACVjK,IACN,CAAC,GAEJV,QAAQC,IAAI,iDACZ,IAAK,MAAOoE,EAAS2R,KAAU1S,OAAO0c,QAAQH,GAAgB,CAC5D,MAAMI,EAAUxC,EAA8BpZ,EAAQ8K,eACtD,GAAK8Q,EAAL,CAIAjgB,QAAQC,IAAI,8CAADwB,OAA+C4C,EAAO,aAAA5C,OAAYuU,EAAMkK,QAAQ,KAC3F,IAAK,MAAOC,EAAOC,KAAW9c,OAAO0c,QAAQC,GAAU,CACrD,MAAMI,EAAWF,EACXG,EAAaP,EAAYM,IAAa,EACtCE,EAAWvK,EAASoK,EAnJgB,EAoJ1CL,EAAYM,GAAYnf,KAAKwS,IAAI,EAAGxS,KAAKC,IAAI,EAAGmf,EAAaC,GAG/D,CATA,MAFEvgB,QAAQC,IAAI,wDAADwB,OAAyD4C,EAAO,eAY/E,CAEA,OADArE,QAAQC,IAAI,4DAADwB,OAA6D6B,OAAOC,KAAKwc,GAAava,OAAM,qBAAA/D,OAAoB6B,OAAOkd,OAAOT,GAAatf,QAAO,CAACkI,EAAG8X,IAAM9X,EAAI8X,GAAG,GAAGP,QAAQ,KAClLH,E","sources":["components/TestModeToggle.tsx","services/VisemeService.ts","components/SimulationView.tsx","utils/prosodyToBlendshapes.ts"],"sourcesContent":["import React, { useState, useCallback } from 'react';\nimport { MockHumeEVI, mockHumeProsodyData, mockEmotionSets } from '../utils/mockHumeData';\n\ninterface TestModeToggleProps {\n  onMockMessage?: (message: any) => void;\n  onMockProsody?: (emotions: any[]) => void;\n  onMockVisemes?: (visemes: any) => void;\n}\n\nexport const TestModeToggle: React.FC<TestModeToggleProps> = ({\n  onMockMessage,\n  onMockProsody,\n  onMockVisemes\n}) => {\n  const [isTestMode, setIsTestMode] = useState(false);\n  const [isExpanded, setIsExpanded] = useState(false);\n  const [mockEVI, setMockEVI] = useState<MockHumeEVI | null>(null);\n\n  const handleTestModeToggle = useCallback(() => {\n    if (!isTestMode) {\n      // Start test mode\n      console.log('🧪 Enabling Test Mode - Using Mock Hume Data');\n      \n      const evi = new MockHumeEVI((message) => {\n        console.log('🎭 Mock message received:', message);\n        if (onMockMessage) {\n          onMockMessage(message);\n        }\n        \n        // Extract prosody for emotions\n        if (message.prosody && onMockProsody) {\n          onMockProsody(message.prosody);\n        }\n        \n        // Extract timeline for visemes\n        if (message.timeline && onMockVisemes) {\n          // Convert timeline to viseme format\n          const visemes = message.timeline.reduce((acc: any, item: any) => {\n            if (item.type === 'phoneme') {\n              // Map phonemes to basic visemes\n              switch (item.value) {\n                case 'eh':\n                case 'ae':\n                  acc.jawOpen = 0.0; // FIXED: was 0.6 - this was causing mouth stuck open\n                  break;\n                case 'ow':\n                case 'aw':\n                  acc.mouthFunnel = 0.7;\n                  break;\n                case 'iy':\n                case 'ih':\n                  acc.mouthSmileLeft = 0.3;\n                  acc.mouthSmileRight = 0.3;\n                  break;\n                default:\n                  acc.jawOpen = Math.max(acc.jawOpen || 0, 0.0); // FIXED: was 0.2 - forcing minimum jaw open\n              }\n            }\n            return acc;\n          }, {});\n          \n          onMockVisemes(visemes);\n        }\n      });\n      \n      setMockEVI(evi);\n      evi.start();\n      setIsTestMode(true);\n      setIsExpanded(true); // Auto-expand when starting test mode\n    } else {\n      // Stop test mode\n      console.log('🧪 Disabling Test Mode');\n      if (mockEVI) {\n        mockEVI.stop();\n      }\n      setMockEVI(null);\n      setIsTestMode(false);\n      setIsExpanded(false);\n    }\n  }, [isTestMode, mockEVI, onMockMessage, onMockProsody, onMockVisemes]);\n\n  const handleTestEmotion = useCallback((emotionSetKey: string) => {\n    if (onMockProsody) {\n      const emotionSet = mockEmotionSets[emotionSetKey as keyof typeof mockEmotionSets];\n      if (emotionSet) {\n        console.log(`🧪 Testing emotion set: ${emotionSetKey}`, emotionSet);\n        onMockProsody(emotionSet);\n      }\n    }\n  }, [onMockProsody]);\n\n  return (\n    <div style={{\n      position: 'fixed',\n      top: '10px',\n      left: '10px', // Moved to left side\n      background: 'rgba(0,0,0,0.9)',\n      color: 'white',\n      padding: '10px',\n      borderRadius: '8px',\n      zIndex: 1000,\n      fontSize: '12px',\n      width: isExpanded ? '280px' : '160px',\n      maxHeight: isExpanded ? '70vh' : 'auto',\n      overflowY: isExpanded ? 'auto' : 'hidden',\n      transition: 'all 0.3s ease',\n      border: '1px solid rgba(255,255,255,0.2)'\n    }}>\n      <div style={{ display: 'flex', alignItems: 'center', justifyContent: 'space-between', marginBottom: '8px' }}>\n        <h4 style={{ margin: '0', fontSize: '13px' }}>🧪 Test Mode</h4>\n        {isTestMode && (\n          <button\n            onClick={() => setIsExpanded(!isExpanded)}\n            style={{\n              background: 'transparent',\n              border: '1px solid rgba(255,255,255,0.3)',\n              color: 'white',\n              padding: '2px 6px',\n              borderRadius: '3px',\n              cursor: 'pointer',\n              fontSize: '10px'\n            }}\n          >\n            {isExpanded ? '▲' : '▼'}\n          </button>\n        )}\n      </div>\n      \n      <button\n        onClick={handleTestModeToggle}\n        style={{\n          background: isTestMode ? '#ff4444' : '#44ff44',\n          color: 'white',\n          border: 'none',\n          padding: '6px 10px',\n          borderRadius: '4px',\n          cursor: 'pointer',\n          width: '100%',\n          fontSize: '11px',\n          marginBottom: isTestMode && isExpanded ? '10px' : '0'\n        }}\n      >\n        {isTestMode ? '🛑 Stop' : '▶️ Start Test'}\n      </button>\n\n      {isTestMode && isExpanded && (\n        <div>\n          <p style={{ margin: '5px 0', fontSize: '11px', fontWeight: 'bold' }}>\n            🎭 Test Emotions:\n          </p>\n          <div style={{ display: 'grid', gridTemplateColumns: '1fr 1fr', gap: '4px', marginBottom: '8px' }}>\n            {Object.keys(mockEmotionSets).map(emotionKey => (\n              <button\n                key={emotionKey}\n                onClick={() => handleTestEmotion(emotionKey)}\n                style={{\n                  background: '#555',\n                  color: 'white',\n                  border: '1px solid rgba(255,255,255,0.2)',\n                  padding: '4px 6px',\n                  borderRadius: '3px',\n                  cursor: 'pointer',\n                  fontSize: '10px',\n                  textTransform: 'capitalize',\n                  transition: 'background 0.2s'\n                }}\n                onMouseEnter={(e) => e.currentTarget.style.background = '#777'}\n                onMouseLeave={(e) => e.currentTarget.style.background = '#555'}\n              >\n                {getEmotionEmoji(emotionKey)} {emotionKey}\n              </button>\n            ))}\n          </div>\n          \n          <div style={{ fontSize: '9px', opacity: 0.7, lineHeight: '1.2', borderTop: '1px solid rgba(255,255,255,0.1)', paddingTop: '6px' }}>\n            <strong>🎯 Look for:</strong><br/>\n            • <strong>Joy:</strong> Big smile + raised brows<br/>\n            • <strong>Anger:</strong> Furrowed brows + flared nostrils<br/>\n            • <strong>Sad:</strong> Frown + puppy dog eyes<br/>\n            • <strong>Surprise:</strong> Wide eyes + dramatic brows<br/>\n            • <strong>Fear:</strong> Wide eyes + worried brows<br/>\n            • <strong>Disgust:</strong> Nose sneer + lip curl\n          </div>\n        </div>\n      )}\n      \n      {isTestMode && !isExpanded && (\n        <div style={{ fontSize: '10px', opacity: 0.8, marginTop: '5px' }}>\n          Click ▼ to expand controls\n        </div>\n      )}\n    </div>\n  );\n};\n\nfunction getEmotionEmoji(emotion: string): string {\n  const emojiMap: Record<string, string> = {\n    happy: '😊',\n    sad: '😢', \n    angry: '😠',\n    surprised: '😲',\n    scared: '😨',\n    disgusted: '🤢',\n    confused: '😕',\n    content: '😌'\n  };\n  return emojiMap[emotion] || '😐';\n}\n","// /Users/douglasgoldstein/XRCupid_Clone/hub/src/services/VisemeService.ts\nimport process from 'process';\nimport * as SpeechSDK from 'microsoft-cognitiveservices-speech-sdk';\n\n// Type definitions for the data structures\nexport interface BlendshapeFrame {\n  frameIndex: number; // Absolute frame index, sorted\n  shapes: number[];   // Array of 55 blendshape values\n  audioOffset: number; // In milliseconds, from the start of the audio, corresponding to this frame or start of this animation chunk\n}\n\nexport interface StandardViseme {\n  visemeID: number;\n  audioOffset: number; // In milliseconds, from the start of the audio\n}\n\nexport interface VisemeData {\n  standardVisemes: StandardViseme[];\n  blendShapeFrames: BlendshapeFrame[];\n}\n\nexport interface VisemeServiceResult {\n  audioData: ArrayBuffer;\n  visemeData: VisemeData;\n  audioDurationMs?: number; // Added to store audio duration in milliseconds\n}\n\n// Keep VisemeEvent for the raw SDK event if needed, or remove if fully superseded\n// TypeScript interfaces and enums remain the same\n// ... existing VisemeEvent, etc.\n\n// Mappings for Hume EVI to Azure Viseme Conversion\n\n// Based on Hume's likely SAPI-like output (e.g., 'AA', 'AE', 'SIL')\n// and Azure's IPA-based viseme table.\nconst HUME_SAPI_TO_IPA: Record<string, string | null> = {\n    'AA': 'ɑ',  // father\n    'AE': 'æ',  // cat\n    'AH': 'ʌ',  // but\n    'AO': 'ɔ',  // dog\n    'AW': 'aʊ', // cow\n    'AY': 'aɪ', // say\n    'B':  'b',\n    'CH': 'tʃ',\n    'D':  'd',\n    'DH': 'ð',\n    'EH': 'ɛ',  // bed\n    'ER': 'ɝ',  // her\n    'EY': 'eɪ', // they (diphthong, special handling for viseme ID)\n    'F':  'f',\n    'G':  'g',\n    'HH': 'h',\n    'IH': 'ɪ',  // sit\n    'IY': 'i',  // see\n    'JH': 'dʒ',\n    'K':  'k',\n    'L':  'l',\n    'M':  'm',\n    'N':  'n',\n    'NG': 'ŋ',  // sing\n    'OW': 'oʊ', // go (diphthong, special handling for viseme ID)\n    'OY': 'ɔɪ', // boy\n    'P':  'p',\n    'R':  'ɹ',  // red\n    'S':  's',\n    'SH': 'ʃ',\n    'T':  't',\n    'TH': 'θ',  // thin\n    'UH': 'ʊ',  // book\n    'UW': 'u',  // blue\n    'V':  'v',\n    'W':  'w',\n    'Y':  'j',  // yes\n    'Z':  'z',\n    'ZH': 'ʒ',\n    'SIL': 'silence', // Hume 'sil'\n    'PAU': 'silence', // Hume 'pau'\n    'AX': 'ə', // Schwa, common SAPI phoneme\n    // Add any other SAPI phonemes Hume might output\n};\n\n// Based on Azure's Viseme ID to IPA mapping table\nconst IPA_TO_AZURE_VISEME_ID_MAP: Array<{ ipaSymbols: string[], id: number }> = [\n    { ipaSymbols: ['silence'], id: 0 }, // Special case for silence\n    { ipaSymbols: ['æ', 'ə', 'ʌ'], id: 1 },\n    { ipaSymbols: ['ɑ'], id: 2 },\n    { ipaSymbols: ['ɔ'], id: 3 },\n    { ipaSymbols: ['ɛ', 'ʊ'], id: 4 },\n    { ipaSymbols: ['ɝ'], id: 5 },\n    { ipaSymbols: ['j', 'i', 'ɪ'], id: 6 },\n    { ipaSymbols: ['w', 'u'], id: 7 },\n    { ipaSymbols: ['o'], id: 8 },\n    { ipaSymbols: ['aʊ'], id: 9 },\n    { ipaSymbols: ['ɔɪ'], id: 10 },\n    { ipaSymbols: ['aɪ'], id: 11 },\n    { ipaSymbols: ['h'], id: 12 },\n    { ipaSymbols: ['ɹ'], id: 13 },\n    { ipaSymbols: ['l'], id: 14 },\n    { ipaSymbols: ['s', 'z'], id: 15 },\n    { ipaSymbols: ['ʃ', 'tʃ', 'dʒ', 'ʒ'], id: 16 },\n    { ipaSymbols: ['ð'], id: 17 },\n    { ipaSymbols: ['f', 'v'], id: 18 },\n    { ipaSymbols: ['d', 't', 'n', 'θ'], id: 19 },\n    { ipaSymbols: ['k', 'g', 'ŋ'], id: 20 },\n    { ipaSymbols: ['p', 'b', 'm'], id: 21 },\n];\n\nfunction getAzureVisemeIdFromIpa(ipaSymbol: string | null): number {\n    if (ipaSymbol === null) { // Indicates an unmapped Hume SAPI phoneme\n        console.warn(`[VisemeConversion] Received unmapped Hume SAPI phoneme, defaulting to Viseme 0 (silence).`);\n        return 0;\n    }\n    if (ipaSymbol === 'silence') return 0;\n\n    for (const entry of IPA_TO_AZURE_VISEME_ID_MAP) {\n        if (entry.ipaSymbols.includes(ipaSymbol)) {\n            return entry.id;\n        }\n    }\n\n    // Fallback for specific diphthongs not directly in Azure's simple IPA list for a single viseme ID\n    if (ipaSymbol === 'eɪ') return 4; // For SAPI 'EY' -> IPA 'eɪ'. Map to viseme for 'ɛ' (ID 4) or 'ɪ' (ID 6). Chosen 4.\n    if (ipaSymbol === 'oʊ') return 8; // For SAPI 'OW' -> IPA 'oʊ'. Map to viseme for 'o' (ID 8). Good match.\n\n    console.warn(`[VisemeConversion] Unmapped IPA symbol: '${ipaSymbol}', defaulting to Viseme 0 (silence).`);\n    return 0; // Default to silence for unmapped IPA symbols\n}\n\nexport interface HumeTimelineEvent {\n    time: number; // in seconds\n    phoneme: string; // SAPI-like phoneme string (e.g., 'AA', 'P', 'SIL')\n}\n\nexport interface ConvertedHumeVisemes {\n    visemeEvents: VisemeEvent[];\n    audioDurationMs: number;\n}\n\n/**\n * Converts a Hume EVI timeline to Azure-compatible VisemeEvents.\n * @param humeTimeline An array of HumeTimelineEvent objects.\n * @param audioDurationSeconds The total duration of the Hume audio in seconds.\n * @returns An object containing an array of VisemeEvents and the audio duration in milliseconds.\n */\nexport function convertHumeTimelineToAzureVisemes(\n    humeTimeline: HumeTimelineEvent[],\n    audioDurationSeconds: number\n): ConvertedHumeVisemes {\n    const visemeEvents: VisemeEvent[] = [];\n    const audioDurationMs = Math.round(audioDurationSeconds * 1000);\n\n    if (!humeTimeline || humeTimeline.length === 0) {\n        console.warn('[VisemeConversion] Hume timeline is empty. Returning empty viseme events.');\n        return { visemeEvents, audioDurationMs };\n    }\n\n    humeTimeline.forEach((humeEvent, index) => {\n        const humePhonemeKey = humeEvent.phoneme.toUpperCase(); // Normalize to uppercase for map lookup\n        const ipaSymbol = HUME_SAPI_TO_IPA[humePhonemeKey] !== undefined \n                            ? HUME_SAPI_TO_IPA[humePhonemeKey]\n                            : null; // null if not found\n        \n        const visemeId = getAzureVisemeIdFromIpa(ipaSymbol);\n        const audioOffsetTicks = Math.round(humeEvent.time * 10_000_000); // Convert seconds to 100-nanosecond ticks\n\n        visemeEvents.push({\n            audioOffset: audioOffsetTicks,\n            visemeId: visemeId,\n            isLastViseme: index === humeTimeline.length - 1,\n        });\n    });\n\n    // Ensure visemes are sorted by audioOffset, though Hume timeline should already be sorted.\n    visemeEvents.sort((a, b) => a.audioOffset - b.audioOffset);\n    \n    // It's possible the last viseme from Hume doesn't extend to the full audio duration.\n    // The animation might need a final 'silence' viseme at the actual audio end if not covered.\n    // For now, we rely on the provided audioDurationMs for animation timing.\n\n    console.log('[VisemeConversion] Converted Hume timeline to Azure visemes:', visemeEvents);\n    console.log('[VisemeConversion] Audio duration (ms):', audioDurationMs);\n\n    return { visemeEvents, audioDurationMs };\n}\n\nexport interface VisemeEvent {\n    audioOffset: number; // In ticks (100 nanoseconds)\n    visemeId: number;\n    isLastViseme?: boolean; // Optional: true if this is the last viseme in the sequence\n    animation?: string; // JSON string with blend shapes and frame index for SSML\n  }\n\nlet speechConfig: SpeechSDK.SpeechConfig | undefined;\n\nexport const initializeAzureSdk = (): void => {\n  console.log('[VisemeService.ts initializeAzureSdk] Called');\n  const speechKey = process.env.REACT_APP_AZURE_SPEECH_KEY;\n  const speechRegion = process.env.REACT_APP_AZURE_SPEECH_REGION;\n\n  if (!speechKey || !speechRegion) {\n    console.error('[VisemeService.ts] Azure Speech Key or Region not configured.');\n    // Consider throwing an error to make initialization failure more explicit\n    // throw new Error('Azure Speech Key or Region not configured.');\n    return;\n  }\n  // speechConfig is created on demand in synthesizeSpeechWithVisemes\n  console.log('[VisemeService.ts] Azure SDK credentials check passed.');\n};\n\nexport const synthesizeSpeechWithVisemes = async (\n  // CASCADE_DEBUG_V6\n  text: string,\n  voiceName: string = 'en-US-JennyNeural' // Default voice\n): Promise<VisemeServiceResult> => {\n  console.log(`[VisemeService.ts synthesizeSpeechWithVisemes ENTRY_V6] Text: \"${text}\", Voice: \"${voiceName}\", Timestamp: ${Date.now()}`); // CASCADE_DEBUG_V6\n  return new Promise((resolve, reject) => {\n    console.log(`[VisemeService.ts synthesizeSpeechWithVisemes PROMISE_CONSTRUCTOR_V6] Text: \"${text}\", Timestamp: ${Date.now()}`); // CASCADE_DEBUG_V6\n    // Removed duplicated V4 log and Promise constructor here\n    const speechKey = process.env.REACT_APP_AZURE_SPEECH_KEY;\n    const speechRegion = process.env.REACT_APP_AZURE_SPEECH_REGION;\n\n    if (!speechKey || !speechRegion) {\n      console.error('[VisemeService.ts] Azure Speech Key or Region not found.');\n      console.log(`[VisemeService.ts REJECT_V6_NO_KEY_REGION] Text: \"${text}\", Timestamp: ${Date.now()}`); // CASCADE_DEBUG_V6\n      reject(new Error('Azure Speech Key or Region not configured.'));\n      return;\n    }\n\n    try {\n      speechConfig = SpeechSDK.SpeechConfig.fromSubscription(speechKey, speechRegion);\n      console.log('[VisemeService.ts synthesizeSpeechWithVisemes] SpeechConfig created:', speechConfig);\n      speechConfig.speechSynthesisOutputFormat = SpeechSDK.SpeechSynthesisOutputFormat.Audio16Khz32KBitRateMonoMp3;\n      // speechConfig.speechSynthesisVoiceName = voiceName; // Set by SSML\n\n      const ssml = `\n        <speak version=\"1.0\" xmlns=\"http://www.w3.org/2001/10/synthesis\" xmlns:mstts=\"http://www.w3.org/2001/mstts\" xml:lang=\"en-US\">\n          <voice name=\"${voiceName}\">\n            <mstts:viseme type=\"FacialExpression\"/>\n            ${text}\n          </voice>\n        </speak>`;\n\n      let synthesizer = new SpeechSDK.SpeechSynthesizer(speechConfig, null);\n      console.log('[VisemeService.ts synthesizeSpeechWithVisemes] Synthesizer created. SSML to be used:', ssml);\n\n      let collectedVisemeData: VisemeData = { standardVisemes: [], blendShapeFrames: [] };\n      let promiseHandled = false;\n\n      const cleanup = () => {\n        if (synthesizer) {\n          try {\n            synthesizer.close();\n          } catch (closeError) {\n            console.error('[VisemeService.ts cleanup] Error closing synthesizer:', closeError);\n          }\n          synthesizer = null as any; // Allow reassignment to null\n        }\n      };\n\n      synthesizer.visemeReceived = (s, e: SpeechSDK.SpeechSynthesisVisemeEventArgs) => {\n        console.log(`[VisemeService.ts visemeReceived] FIRING! Offset: ${e.audioOffset / 10000}ms, VisemeId: ${e.visemeId}`);\n        if (e.animation && e.animation.trim() !== '') {\n          console.log(`[VisemeService.ts visemeReceived] Animation length: ${e.animation.length}`);\n          console.log(`[VisemeService.ts visemeReceived] Animation (first 100 chars): ${e.animation.substring(0, 100)}`);\n        // Note: There was a nested 'if (e.animation && e.animation.trim() !== '')' here which might be redundant.\n        // Keeping the outer one for now.\n          try {\n            const animationData = JSON.parse(e.animation);\n            if (animationData.BlendShapes && animationData.FrameIndex !== undefined) {\n              const baseFrameIndex = animationData.FrameIndex;\n              animationData.BlendShapes.forEach((shapeFrame: number[], frameIdxInChunk: number) => {\n                collectedVisemeData.blendShapeFrames.push({\n                  frameIndex: baseFrameIndex + frameIdxInChunk,\n                  shapes: shapeFrame,\n                  audioOffset: (e.audioOffset / 10000) + (frameIdxInChunk * (1000 / (animationData.FrameRate || 60))),\n                });\n              });\n            } else {\n              collectedVisemeData.standardVisemes.push({\n                visemeID: e.visemeId,\n                audioOffset: e.audioOffset / 10000,\n              });\n            }\n          } catch (error) {\n            console.error('[VisemeService.ts visemeReceived] Error parsing animation JSON:', error, 'Animation string (on error):', e.animation);\n            collectedVisemeData.standardVisemes.push({\n              visemeID: e.visemeId,\n              audioOffset: e.audioOffset / 10000,\n            });\n          }\n        } else {\n          console.log(`[VisemeService.ts visemeReceived] Animation is EMPTY or UNDEFINED.`);\n          collectedVisemeData.standardVisemes.push({\n            visemeID: e.visemeId,\n            audioOffset: e.audioOffset / 10000\n          });\n        }\n      };\n\n      synthesizer.speakSsmlAsync(\n        ssml,\n        result => {\n          if (promiseHandled) {\n            cleanup(); return;\n          }\n          promiseHandled = true;\n          if (result.reason === SpeechSDK.ResultReason.SynthesizingAudioCompleted) {\n            collectedVisemeData.blendShapeFrames.sort((a, b) => a.frameIndex - b.frameIndex);\n            collectedVisemeData.standardVisemes.sort((a,b) => a.audioOffset - b.audioOffset);\n            console.log('[VisemeService V6] Raw result.audioDuration:', result.audioDuration);\n            const audioDurationMs = result.audioDuration / 10000; // Azure returns duration in 100-nanosecond units (ticks)\n            console.log(`[VisemeService.ts speakSsmlAsync COMPLETED] Audio Duration: ${result.audioDuration} (100ns units) = ${audioDurationMs}ms`);\n            console.log(`[VisemeService.ts RESOLVE_V6_SUCCESS] Text: \"${text}\", audioDurationMs: ${audioDurationMs}, Timestamp: ${Date.now()}`); // CASCADE_DEBUG_V6\n            resolve({ audioData: result.audioData, visemeData: collectedVisemeData, audioDurationMs: audioDurationMs });\n          } else if (result.reason === SpeechSDK.ResultReason.Canceled) {\n            const cancellation = SpeechSDK.CancellationDetails.fromResult(result);\n            const reasonText = cancellation.reason !== undefined ? SpeechSDK.CancellationReason[cancellation.reason] : 'UnknownReason';\n            console.error(`[VisemeService.ts speakSsmlAsync CANCELED] Reason: ${reasonText}, Details: ${cancellation.errorDetails}, ErrorCode: ${cancellation.ErrorCode}`);\n            console.log(`[VisemeService.ts REJECT_V6_CANCELED] Text: \"${text}\", Reason: ${reasonText}, Timestamp: ${Date.now()}`); // CASCADE_DEBUG_V6\n            reject(`Synthesis Canceled: ${reasonText}. Details: ${cancellation.errorDetails}. ErrorCode: ${cancellation.ErrorCode}`);\n          } else {\n            console.error(`[VisemeService.ts speakSsmlAsync FAILED] Reason: ${result.reason}, Details: ${result.errorDetails}`);\n            console.log(`[VisemeService.ts REJECT_V6_FAILED_RESULT] Text: \"${text}\", Reason: ${result.reason}, Timestamp: ${Date.now()}`); // CASCADE_DEBUG_V6\n            reject(new Error(`Synthesis failed: ${result.errorDetails || 'Unknown error'}`));\n          }\n          cleanup();\n        },\n        error => {\n          if (promiseHandled) {\n            cleanup(); return;\n          }\n          promiseHandled = true;\n          console.log(`[VisemeService.ts REJECT_V6_ERROR_CALLBACK] Text: \"${text}\", Error: ${error}, Timestamp: ${Date.now()}`); // CASCADE_DEBUG_V6\n          reject(error);\n          cleanup();\n        }\n      );\n    } catch (error) {\n      console.error('[VisemeService.ts] Error initializing or starting synthesis:', error);\n      console.log(`[VisemeService.ts REJECT_V6_OUTER_CATCH] Text: \"${text}\", Error: ${error}, Timestamp: ${Date.now()}`); // CASCADE_DEBUG_V6\n      reject(error);\n    }\n  });\n};\n\n","import process from 'process';\nimport React, { useCallback, useEffect, useMemo, useRef, useState, useContext, useImperativeHandle } from 'react';\nimport { useParams, useNavigate } from 'react-router-dom';\nimport { getFirestore, Timestamp, doc, getDoc, updateDoc, arrayUnion } from 'firebase/firestore';\nimport { prosodyToBlendshapes } from '../utils/prosodyToBlendshapes';\nimport * as Hume from 'hume';\n\nimport { v4 as uuidv4 } from 'uuid';\nimport { \n  FaMicrophone, \n  FaMicrophoneSlash, \n  FaVolumeUp, \n  FaVolumeMute, \n  FaPaperPlane, \n  FaTimes, \n  FaExpand, \n  FaCompress, \n  FaCommentDots, \n  FaCog, \n  FaPlay, \n  FaPause, \n  FaRedo, \n  FaStopCircle \n} from 'react-icons/fa';\nimport { IoMdSend } from 'react-icons/io';\nimport { Canvas } from '@react-three/fiber';\nimport { OrbitControls } from '@react-three/drei';\nimport { Group, Vector3 } from 'three';\nimport * as THREE from 'three';\nimport { VoiceProvider, useVoice, VoiceReadyState, type JSONMessage, type ConnectionMessage } from '@humeai/voice-react'; // Assuming JSONMessage is exported\nimport { TestModeToggle } from './TestModeToggle';\n\nimport { useAuth } from '../contexts/AuthContext';\nimport { useUser } from '../contexts/UserContext';\nimport firebase from 'firebase/app';\nimport 'firebase/firestore';\nimport ReadyPlayerMeAvatar from './ReadyPlayerMeAvatar';\nimport EmotionDrivenAvatar, { type Emotion } from './EmotionDrivenAvatar';\n\nimport { ARKitBlendshapeNamesList, type BlendShapeMap, type BlendshapeKey } from '../types/blendshapes';\nimport { useHumeEmotionStream } from '../hooks/useHumeEmotionStream';\nimport { initializeAzureSdk, synthesizeSpeechWithVisemes as untypedSynthesizeSpeechWithVisemes, VisemeEvent, convertHumeTimelineToAzureVisemes, type HumeTimelineEvent, type ConvertedHumeVisemes } from '../services/VisemeService';\n// import { saveChatMessageToFirebase } from '../firebase/firebaseServices'; \n// import { getTopEmotion } from '../utils/emotionMappings'; // emotionToBlendshapes removed, getTopEmotion also unused due to local definition\nimport * as SpeechSDK from 'microsoft-cognitiveservices-speech-sdk'; \n\n\n// Data structures from VisemeService.js\ninterface BlendshapeFrame {\n  frameIndex: number; // Absolute frame index\n  shapes: number[];   // Array of 55 blendshape values\n  audioOffset: number; // In milliseconds, from the start of the audio\n}\n\ninterface StandardViseme {\n  visemeID: number;\n  audioOffset: number; // In milliseconds, already converted by VisemeService\n}\n\ninterface VisemeData {\n  standardVisemes: StandardViseme[];\n  blendShapeFrames: BlendshapeFrame[];\n}\n\ninterface VisemeServiceResult {\n  audioData: ArrayBuffer;\n  visemeData: VisemeData;\n}\n\n// The old AzureViseme and AzureVisemeAnimation interfaces might be removable if no longer used\n// For now, keeping them commented out or to be removed if prepareVisemeFrames fully transitions\n/*\ninterface SpeakTextWithVisemesResult {\n  audioData: ArrayBuffer; \n  visemes: AzureViseme[]; \n}\ninterface AzureVisemeAnimation {\n  frameIndex: number;\n  blendShapes: number[][];\n}\ninterface AzureViseme {\n  audioOffset: number; \n  visemeID: number;\n  animation?: AzureVisemeAnimation | null; \n}\n*/\n\n// ARKit blendshape names corresponding to Azure's 55 blendshapes\nconst ARKIT_BLENDSHAPE_NAMES_AZURE = [\n  \"browDownLeft\", \"browDownRight\", \"browInnerUp\", \"browOuterUpLeft\", \"browOuterUpRight\",\n  \"cheekPuff\", \"cheekSquintLeft\", \"cheekSquintRight\", \"eyeBlinkLeft\", \"eyeBlinkRight\",\n  \"eyeLookDownLeft\", \"eyeLookDownRight\", \"eyeLookInLeft\", \"eyeLookInRight\", \"eyeLookOutLeft\",\n  \"eyeLookOutRight\", \"eyeLookUpLeft\", \"eyeLookUpRight\", \"eyeSquintLeft\", \"eyeSquintRight\",\n  \"eyeWideLeft\", \"eyeWideRight\", \"jawForward\", \"jawLeft\", \"jawOpen\", \"jawRight\",\n  \"mouthClose\", \"mouthDimpleLeft\", \"mouthDimpleRight\", \"mouthFrownLeft\", \"mouthFrownRight\",\n  \"mouthFunnel\", \"mouthLeft\", \"mouthLowerDownLeft\", \"mouthLowerDownRight\", \"mouthPressLeft\",\n  \"mouthPressRight\", \"mouthPucker\", \"mouthRight\", \"mouthRollLower\", \"mouthRollUpper\",\n  \"mouthShrugLower\", \"mouthShrugUpper\", \"mouthSmileLeft\", \"mouthSmileRight\", \"mouthStretchLeft\",\n  \"mouthStretchRight\", \"mouthUpperUpLeft\", \"mouthUpperUpRight\", \"noseSneerLeft\", \"noseSneerRight\",\n  // Assuming the last 3 are for tongue, though Azure docs might need to be checked for exact mapping\n  \"tongueOut\", \"tongueUp\", \"tongueDown\" // Placeholder if 55th is tongue related, adjust as needed\n];\n\ninterface AnimationKeyframe {\n  time: number; // in seconds\n  shapes: Partial<BlendShapeMap>;\n}\n\nconst FRAME_RATE_HZ = 50; // Azure viseme animation frame rate\nconst FRAME_DURATION_MS = 1000 / FRAME_RATE_HZ;\nconst BLENDSHAPE_AMPLIFICATION_FACTOR = 1.5; // Adjusted for more natural amplification\n\n// Helper for initializing an empty BlendShapeMap\nconst initialEmptyBlendshapes: BlendShapeMap = ARKitBlendshapeNamesList.reduce((acc, shapeName) => {\n  acc[shapeName] = 0;\n  return acc;\n}, {} as BlendShapeMap);\n\n// Static Blendshape mapping for Azure Viseme IDs\nconst AZURE_VISEME_ID_TO_STATIC_BLENDSHAPES: Record<number, Partial<BlendShapeMap>> = {\n  0: { jawOpen: 0, mouthClose: 1, mouthPucker: 0, mouthFunnel: 0, mouthSmileLeft: 0, mouthSmileRight: 0 }, // Silence\n  1: { jawOpen: 0.2, mouthShrugUpper: 0.1 }, // æ, ə, ʌ (e.g., cat, but, about)\n  2: { jawOpen: 0.6, mouthFunnel: 0.1 },     // ɑ (e.g., father)\n  3: { jawOpen: 0.4, mouthFunnel: 0.3, mouthPucker: 0.2 }, // ɔ (e.g., dog, caught)\n  4: { jawOpen: 0.3, mouthSmileLeft: 0.1, mouthSmileRight: 0.1 }, // ɛ, ʊ (e.g., bed, book)\n  5: { jawOpen: 0.25, mouthShrugUpper: 0.2, tongueUp: 0.3 }, // ɝ (e.g., her)\n  6: { jawOpen: 0.1, mouthSmileLeft: 0.4, mouthSmileRight: 0.4 }, // j, i, ɪ (e.g., yes, see, sit)\n  7: { jawOpen: 0.15, mouthPucker: 0.6, mouthFunnel: 0.4 },// w, u (e.g., way, blue)\n  8: { jawOpen: 0.3, mouthPucker: 0.5, mouthFunnel: 0.3 }, // o (e.g., go - o part of oʊ)\n  9: { jawOpen: 0.5, mouthFunnel: 0.2, mouthPucker: 0.3 }, // aʊ (e.g., cow) - starts open, moves to pucker\n  10: { jawOpen: 0.3, mouthFunnel: 0.2, mouthSmileLeft: 0.2 }, // ɔɪ (e.g., boy) - starts round, moves to smile\n  11: { jawOpen: 0.5, mouthSmileLeft: 0.3, mouthSmileRight: 0.3 }, // aɪ (e.g., buy) - starts open, moves to smile\n  12: { jawOpen: 0.1, mouthShrugUpper: 0.05 }, // h (slight breath)\n  13: { jawOpen: 0.2, mouthPucker: 0.1, tongueUp: 0.4 }, // ɹ (e.g., red)\n  14: { jawOpen: 0.25, tongueUp: 0.5, mouthSmileLeft: 0.1 }, // l (e.g., lay)\n  15: { jawOpen: 0.05, mouthClose: 0.5, mouthSmileLeft: 0.2, mouthSmileRight: 0.2 }, // s, z (teeth close)\n  16: { jawOpen: 0.15, mouthPucker: 0.4, mouthFunnel: 0.2 }, // ʃ, tʃ, dʒ, ʒ (e.g., shy, chin, joy, vision)\n  17: { jawOpen: 0.1, tongueUp: 0.2, mouthLowerDownLeft: 0.1, mouthLowerDownRight: 0.1 }, // ð (e.g., they - tongue slightly visible)\n  18: { jawOpen: 0.05, mouthLowerDownLeft: 0.3, mouthLowerDownRight: 0.3, mouthPressLeft: 0.2, mouthPressRight: 0.2 }, // f, v (upper teeth on lower lip)\n  19: { jawOpen: 0.1, tongueUp: 0.6, mouthClose: 0.2 }, // d, t, n, θ (tongue tip to alveolar ridge or teeth)\n  20: { jawOpen: 0.3, mouthShrugLower: 0.1, tongueUp: 0.2 }, // k, g, ŋ (back of tongue)\n  21: { jawOpen: 0, mouthClose: 1, mouthPucker: 0.05 }, // p, b, m (lips together)\n};\n\n// Define default blendshapes for initialization\nconst defaultBlendShapes: BlendShapeMap = ARKitBlendshapeNamesList.reduce((acc: BlendShapeMap, key: BlendshapeKey) => {\n  acc[key] = 0;\n  return acc;\n}, {} as BlendShapeMap);\n\n// Define blendshapes for idle state, allowing autonomous blinking\nconst idleBlendShapes: Partial<BlendShapeMap> = ARKitBlendshapeNamesList.reduce((acc: Partial<BlendShapeMap>, key: BlendshapeKey) => {\n  if (key !== 'eyeBlinkLeft' && key !== 'eyeBlinkRight') {\n    // Initialize non-blink shapes to 0, consistent with defaultBlendShapes for those keys\n    acc[key] = 0;\n  }\n  return acc;\n}, {} as Partial<BlendShapeMap>);\n\n// Processed animation keyframe structure\ninterface AnimationKeyframe {\n  time: number; // in seconds, relative to the start of the audio\n  shapes: Partial<BlendShapeMap>; // ARKit blendshape values\n}\n\n// Define ChatMessage structure\ninterface ChatMessage {\n  id: string;\n  text: string;\n  sender: 'user' | 'bot';\n  timestamp: Timestamp | Date; // Allow both Firebase Timestamp and JS Date\n  emotion?: string;\n}\n\n// Define Simulation data structure (placeholder, adjust as needed)\ninterface Simulation {\n  id: string;\n  name?: string;\n  description?: string;\n  avatarUrl?: string; // Assuming simulation might also define an avatar\n  // Add other fields as necessary based on your Firestore structure\n}\n\n// Interface for methods exposed by SimulationViewInternal via ref\nexport interface SimulationViewInternalHandle {\n  forceStopHumeSpeaking: () => void;\n}\n\n// State for the SimulationViewInternal component\n// Helper function to get string name for VoiceReadyState\nfunction getVoiceReadyStateName(state: VoiceReadyState): string {\n  switch (state) {\n    case VoiceReadyState.IDLE: return \"IDLE\";\n    case VoiceReadyState.CONNECTING: return \"CONNECTING\";\n    case VoiceReadyState.OPEN: return \"OPEN\";\n    // case VoiceReadyState.CLOSING: return \"CLOSING\"; // Removed as 'CLOSING' might not be a valid enum member\n    case VoiceReadyState.CLOSED: return \"CLOSED\";\n    default:\n      // This path might be reached if 'state' is a valid VoiceReadyState member not explicitly cased,\n      // or an unexpected value. It could also be a state like 'CLOSING' if that enum member doesn't exist by that name.\n      console.warn(`[getVoiceReadyStateName] Encountered unhandled or unknown state value: ${state}`);\n      // const _exhaustiveCheck: never = state; // Ensures all enum cases are handled\n      return `UNKNOWN_STATE_(${String(state)})`;\n  }\n}\n\ninterface SimulationViewState {\n  isMicOn: boolean;\n  isCameraOn: boolean;\n  isSoundOn: boolean;\n  isChatOpen: boolean;\n  messages: ChatMessage[];\n  inputValue: string;\n  isSpeaking: boolean;\n  isAzureAudioActive: boolean; // New: Tracks if Azure audio is playing\n  isHumeAudioActive: boolean; // Tracks if Hume EVI audio is playing\n  currentEmotion: string;\n  statusMessage: string;\n  humeSessionId?: string;\n  humeAccessToken?: string;\n  humeVoiceId?: string;\n  humeVoiceName?: string;\n  humeConfigId?: string;\n  azureVoiceName?: string; \n  currentVisemeShapes: Partial<BlendShapeMap>; // Use Partial to allow omitting blink shapes for idle\n  simulationId?: string; // Added to store the simulation ID from URL\n  simulationData?: Simulation | null; // Added to store fetched simulation data\n  error?: string | null; // Added for error handling\n  isSending: boolean; // Tracks if a message is currently being sent via text input\n  manualBlendshapes: Partial<BlendShapeMap>; // For direct blendshape control via sliders\n  // Re-add properties that were removed but are used by UI\n  avatarUrl: string;\n  isCameraEnabled: boolean;\n  isMicMuted: boolean;\n  showChat: boolean;\n  prosodyDrivenBlendshapes: BlendShapeMap; // For expressions from EVI voice prosody\n}\n\ninterface SimulationViewProps {\n  simulationId?: string; \n  avatarModelUrl: string; \n  // Add any other props that SimulationView might pass down\n}\n\n// Define types for props passed from SimulationView (parent) to SimulationViewInternal\n// Placeholder types are used; replace with actual types from useVoice or Hume SDK if available\ninterface SimulationViewInternalPassedProps {\n  humeMessages: (JSONMessage | ConnectionMessage)[]; // Ensured type\n  sendUserInputToVoice: (input: string, type?: 'chat' | 'text_input') => void;\n  voiceReadyState: VoiceReadyState;\n  isHumeVoicePlaying: boolean;\n  disconnectVoice: () => void;\n  connectVoice: () => Promise<void>;\n  // lastHumeVoiceMessage is removed as it's not provided by useVoice()\n  humeConfigId?: string;\n}\n\n// Combined props for SimulationViewInternal\ntype SimulationViewInternalFullProps = SimulationViewProps & SimulationViewInternalPassedProps;\n\nconst ALL_TALK_ANIMATION_GLBS = [\n  \"/animations/M_Talking_Variations_001.glb\",\n  \"/animations/M_Talking_Variations_002.glb\",\n  \"/animations/M_Talking_Variations_003.glb\",\n  \"/animations/M_Talking_Variations_004.glb\",\n  \"/animations/M_Talking_Variations_005.glb\",\n  \"/animations/M_Talking_Variations_006.glb\",\n  \"/animations/M_Talking_Variations_007.glb\",\n  \"/animations/M_Talking_Variations_008.glb\",\n  \"/animations/M_Talking_Variations_009.glb\",\n  \"/animations/M_Talking_Variations_010.glb\",\n  \"/animations/talk.glb\"\n];\n\nconst ALL_IDLE_ANIMATION_GLBS = [\n  \"/animations/M_Standing_Idle_001.glb\",\n  \"/animations/M_Standing_Idle_002.glb\",\n  \"/animations/M_Standing_Idle_Variations_001.glb\",\n  \"/animations/M_Standing_Idle_Variations_002.glb\",\n  \"/animations/M_Standing_Idle_Variations_003.glb\",\n  \"/animations/M_Standing_Idle_Variations_004.glb\",\n  \"/animations/M_Standing_Idle_Variations_005.glb\",\n  \"/animations/M_Standing_Idle_Variations_006.glb\",\n  \"/animations/M_Standing_Idle_Variations_007.glb\",\n  \"/animations/M_Standing_Idle_Variations_008.glb\",\n  \"/animations/M_Standing_Idle_Variations_009.glb\",\n  \"/animations/M_Standing_Idle_Variations_010.glb\",\n  \"/animations/idle.glb\"\n];\n\nconst DEBOUNCE_DURATION = 250; // milliseconds\n\n// Helper function for button styles (to avoid repetition)\nconst buttonStyle: React.CSSProperties = {\n  background: 'rgba(0,0,0,0.5)',\n  border: '1px solid white',\n  borderRadius: '5px',\n  padding: '8px 12px',\n  color: 'white',\n  cursor: 'pointer',\n  fontSize: '0.9em',\n  margin: '5px',\n};\n\nconst SimulationViewInternal = React.forwardRef<\n  SimulationViewInternalHandle,\n  SimulationViewInternalFullProps\n>((props, ref): React.ReactElement | null => {\n  const { \n    humeMessages, \n    sendUserInputToVoice, \n    voiceReadyState, \n    isHumeVoicePlaying, \n    disconnectVoice, \n    connectVoice, \n    humeConfigId \n  } = props;\n\n  console.log(`[SimViewInternal FUNC_START] isHumeVoicePlaying from props: ${props.isHumeVoicePlaying}`);\n\n  console.log('[SimViewInternal RENDER START] isHumeVoicePlaying from props:', props.isHumeVoicePlaying, 'voiceReadyState:', (typeof props.voiceReadyState === 'number' && VoiceReadyState[props.voiceReadyState]) ? VoiceReadyState[props.voiceReadyState] : String(props.voiceReadyState));\n\n  const { currentUser } = useAuth(); \n  const { user, loading: userLoading, error: userError } = useUser(); \n  const navigate = useNavigate();\n  const { simulationId } = useParams<{ simulationId: string }>();\n\n  const initialState: SimulationViewState = {\n    isMicOn: false,\n    isCameraOn: true, // Default to camera being on conceptually\n    isSoundOn: true,\n    isChatOpen: false,\n    messages: [],\n    inputValue: '',\n    isSpeaking: false,\n    isAzureAudioActive: false,\n    isHumeAudioActive: false, // To track Hume EVI audio state\n    currentEmotion: 'neutral',\n    statusMessage: 'Initializing...', \n    humeVoiceName: 'KARL',\n    azureVoiceName: 'en-US-AvaNeural',\n    currentVisemeShapes: idleBlendShapes, // Initialize with idle blendshapes to allow autonomous blinking\n    manualBlendshapes: defaultBlendShapes, // Initialize manual blendshapes with defaults\n    simulationData: null,\n    error: null,\n    isSending: false,\n    // Initialize re-added properties\n    avatarUrl: props.avatarModelUrl || user?.avatarUrl || '/bro.glb', // Ensure user is checked after loading\n    isCameraEnabled: true, // UI toggle for camera view\n    isMicMuted: false,     // UI toggle for mic input to Hume\n    showChat: true,        // UI toggle for chat panel\n    prosodyDrivenBlendshapes: initialEmptyBlendshapes, // Initialize with all shapes at 0\n  };\n\n  const [state, setState] = useState<SimulationViewState>(() => {\n    const initial = {\n      isMicOn: false,\n      isCameraOn: true, // Default to camera being on conceptually\n      isSoundOn: true,\n      isChatOpen: false,\n      messages: [],\n      inputValue: '',\n      isSpeaking: false,\n      isAzureAudioActive: false, // New\n      isHumeAudioActive: false, // Added missing property\n      currentEmotion: 'neutral',\n      statusMessage: 'Initializing...', \n      humeVoiceName: 'KARL',\n      azureVoiceName: 'en-US-AvaNeural',\n      currentVisemeShapes: idleBlendShapes, // Initialize with idle blendshapes to allow autonomous blinking\n      manualBlendshapes: defaultBlendShapes, // Initialize manual blendshapes with defaults\n      simulationData: null,\n      error: null,\n      isSending: false,\n      avatarUrl: props.avatarModelUrl || user?.avatarUrl || '/bro.glb', // Ensure user is checked after loading\n      isCameraEnabled: true, // UI toggle for camera view\n      isMicMuted: false,     // UI toggle for mic input to Hume\n      showChat: true,        // UI toggle for chat panel\n      prosodyDrivenBlendshapes: initialEmptyBlendshapes, // Initialize with all shapes at 0\n    };\n    console.log(`[SimViewInternal INITIAL_STATE] Calculated initialState.avatarUrl: ${initial.avatarUrl}, Using props.avatarModelUrl: ${props.avatarModelUrl}, user?.avatarUrl: ${user?.avatarUrl}`);\n    return initial;\n  });\n\n  // Refs\n  const stateRef = useRef(state);\n\n  // --- Hume Emotion Stream Integration ---\n  const handleEmotionData = useCallback((emotion: { name: string; score: number }) => {\n    // console.log('[SimView handleEmotionData] Received emotion:', emotion);\n    setState(prev => ({ ...prev, currentEmotion: emotion.name.toLowerCase() }));\n  }, [setState]);\n\n  // TODO: API Key should ideally be passed via props or context, not directly accessed from process.env here.\n  const humeApiKey = process.env.REACT_APP_HUME_API_KEY || process.env.NEXT_PUBLIC_HUME_API_KEY;\n\n  const { \n    sendVideoFrame: sendEmotionVideoFrame, // Renamed to avoid conflict if another sendVideoFrame exists\n    connectionState: humeEmotionStreamConnectionState, \n    lastError: humeEmotionStreamLastError \n  } = useHumeEmotionStream(\n    humeApiKey,\n    handleEmotionData,\n    {\n      isEmotionDetectionActive: !!humeApiKey && state.isCameraOn, // Only active if key and camera are on\n      isVideoOn: state.isCameraOn, // Tied to the main camera state\n    }\n  );\n\n  useEffect(() => {\n    console.log('[SimView HumeEmotionStream] Connection State:', humeEmotionStreamConnectionState, 'Last Error:', humeEmotionStreamLastError);\n    if (humeEmotionStreamLastError) {\n      setState(prev => ({ ...prev, statusMessage: `Emotion Stream Error: ${humeEmotionStreamLastError}` }));\n    }\n  }, [humeEmotionStreamConnectionState, humeEmotionStreamLastError]);\n  // --- End Hume Emotion Stream Integration ---\n\n  const azureAudioRef = useRef<HTMLAudioElement | null>(null);\n  const humeAudioRef = useRef<HTMLAudioElement | null>(null);\n  const visemeFramesRef = useRef<AnimationKeyframe[]>([]);\n  const animationFrameIdRef = useRef<number | null>(null);\n  const speakingDebounceTimerRef = useRef<NodeJS.Timeout | null>(null);\n  const lastProcessedUserMessageReceivedAtRef = useRef<Date | null>(null);\n  const lastProcessedAssistantMessageIdRef = useRef<string | null>(null);\n  const isAzurePlayingRef = useRef<boolean>(false); // CASCADE: Restored for tracking Azure playback state \n  const audioBufferSourceRef = useRef<AudioBufferSourceNode | null>(null);\n  const isHumeVoicePlayingRef = useRef(props.isHumeVoicePlaying);\n\n  // Effect to keep stateRef updated with the latest state\n  useEffect(() => {\n    stateRef.current = state;\n  }, [state]);\n\n  useEffect(() => {\n    // This log helps track the state.avatarUrl specifically when it's established or changes.\n    // Note: user object might not be immediately available from useUser(), so initial user?.avatarUrl could be undefined.\n    console.log(`[SimViewInternal AVATAR_URL_STATE] state.avatarUrl is now: ${state.avatarUrl}. Initial props.avatarModelUrl was: ${props.avatarModelUrl}. Current user?.avatarUrl: ${user?.avatarUrl}`);\n  }, [state.avatarUrl, props.avatarModelUrl, user]); // Log when these key values change\n\n  useEffect(() => {\n    console.log(`[SimViewInternal MOUNT] Component did mount. Current props.avatarModelUrl: ${props.avatarModelUrl}, Current state.avatarUrl: ${state.avatarUrl}`);\n    return () => {\n      // stateRef.current can be used here to get the state at the time of unmount if direct state access is stale\n      console.log(`[SimViewInternal UNMOUNT] Component will unmount. props.avatarModelUrl at unmount: ${props.avatarModelUrl}, state.avatarUrl at unmount: ${stateRef.current.avatarUrl}`);\n    };\n  // eslint-disable-next-line react-hooks/exhaustive-deps\n  }, []); // Empty dependency array for mount/unmount effect\n\n  const handleAvatarErrorCb = useCallback((error: Error) => {\n    setState(prev => ({ ...prev, statusMessage: `Avatar Error: ${error.message}` }));\n  }, []); // setState is stable, so empty deps or [setState] is fine.\n\n  const handleAvatarLoadCb = useCallback(() => {\n    setState(prev => ({ ...prev, statusMessage: 'Avatar Loaded' }));\n  }, []); // setState is stable\n\n  // Effect to keep isHumeVoicePlayingRef updated\n  useEffect(() => {\n    isHumeVoicePlayingRef.current = props.isHumeVoicePlaying;\n    console.log(`[SimViewEffect isHumeVoicePlayingRef] Updated isHumeVoicePlayingRef.current to: ${isHumeVoicePlayingRef.current} (raw isHumeVoicePlaying: ${props.isHumeVoicePlaying})`);\n  }, [props.isHumeVoicePlaying]);\n\n  // Additional Refs\n  const messagesEndRef = useRef<HTMLDivElement | null>(null);\n  const avatarGroupRef = useRef<THREE.Group | null>(null); // For direct avatar manipulation if needed\n  const customSessionIdRef = useRef<string>(uuidv4()); // Generate a unique session ID for this component instance\n\n  const getTopEmotion = (emotions: { name: string; score: number }[]): { name: string; score: number } | undefined => {\n    if (!emotions || emotions.length === 0) return undefined;\n    return emotions.reduce((prev, current) => (prev.score > current.score) ? prev : current);\n  };\n\n  const stopVisemeAnimation = useCallback(() => {\n    if (animationFrameIdRef.current) {\n      cancelAnimationFrame(animationFrameIdRef.current);\n      animationFrameIdRef.current = null;\n    }\n    // Reset to idle blendshapes (neutral face, allows autonomous blinking)\n    setState(prev => ({ ...prev, currentVisemeShapes: idleBlendShapes, isSpeaking: false }));\n    console.log('[SimView stopVisemeAnimation] Viseme animation stopped and blendshapes reset to idle (allowing autonomous blinks).');\n  }, [setState, animationFrameIdRef, defaultBlendShapes]);\n\n  const prepareVisemeFrames = useCallback((visemeData: VisemeData): AnimationKeyframe[] => {\n    console.log('[SimView prepareVisemeFrames] Preparing viseme frames from data:', visemeData);\n    if (!visemeData || !visemeData.blendShapeFrames || visemeData.blendShapeFrames.length === 0) {\n      console.warn('[SimView prepareVisemeFrames] No blendshape data received from Azure.');\n      return [];\n    }\n\n    const frames: AnimationKeyframe[] = visemeData.blendShapeFrames.map(frame => {\n      const shapes: Record<string, number> = {};\n      ARKitBlendshapeNamesList.forEach((name: BlendshapeKey, index: number) => {\n        shapes[name] = frame.shapes[index]; // Assuming Azure provides 0-1 values directly\n      });\n      return {\n        time: frame.audioOffset, // Already in seconds from VisemeService.js\n        shapes: shapes,\n      };\n    });\n    console.log(`[SimView prepareVisemeFrames] Prepared ${frames.length} keyframes.`);\n    return frames;\n  }, []);\n\n  const humeVisemeEventsToAnimationKeyframes = useCallback((\n    visemeEvents: VisemeEvent[], // From VisemeService's convertHumeTimelineToAzureVisemes\n    audioDurationMs: number\n  ): AnimationKeyframe[] => {\n    console.log(`[SimView humeVisemeEventsToAnimationKeyframes] Called with ${visemeEvents.length} viseme events, duration: ${audioDurationMs}ms`);\n    if (!visemeEvents || visemeEvents.length === 0) {\n      if (audioDurationMs > 0) {\n        // If there's audio but no visemes, create bookend silence frames\n        console.log('[SimView humeVisemeEventsToAnimationKeyframes] No viseme events, creating silence bookends.');\n        return [\n          { time: 0, shapes: AZURE_VISEME_ID_TO_STATIC_BLENDSHAPES[0] || defaultBlendShapes },\n          { time: audioDurationMs / 1000, shapes: AZURE_VISEME_ID_TO_STATIC_BLENDSHAPES[0] || defaultBlendShapes }, // time in seconds\n        ];\n      }\n      console.log('[SimView humeVisemeEventsToAnimationKeyframes] No viseme events and no duration, returning empty keyframes.');\n      return [];\n    }\n\n    const keyframes: AnimationKeyframe[] = visemeEvents.map(event => ({\n      time: event.audioOffset / 10_000_000, // Convert 100ns ticks to seconds\n      shapes: AZURE_VISEME_ID_TO_STATIC_BLENDSHAPES[event.visemeId] || defaultBlendShapes, // Fallback to default neutral shapes\n    }));\n\n    // Ensure the animation covers the full audio duration with a final silence/neutral pose\n    const lastKeyframeTimeSeconds = keyframes.length > 0 ? keyframes[keyframes.length - 1].time : 0;\n    const audioDurationSeconds = audioDurationMs / 1000;\n\n    if (keyframes.length === 0 || lastKeyframeTimeSeconds < audioDurationSeconds) {\n        // Add a start frame if empty, or ensure the last frame extends to audio duration\n        if (keyframes.length === 0 && audioDurationSeconds > 0) {\n            keyframes.push({ time: 0, shapes: AZURE_VISEME_ID_TO_STATIC_BLENDSHAPES[0] || defaultBlendShapes });\n        }\n        keyframes.push({\n            time: audioDurationSeconds,\n            shapes: AZURE_VISEME_ID_TO_STATIC_BLENDSHAPES[0] || defaultBlendShapes, // End with silence\n        });\n    }\n    console.log(`[SimView humeVisemeEventsToAnimationKeyframes] Generated ${keyframes.length} keyframes.`);\n    return keyframes;\n  }, [defaultBlendShapes]);\n\n  const animateVisemes = useCallback((startTime: number) => {\n    const currentTime = (Date.now() - startTime) / 1000;\n    const currentFrames = visemeFramesRef.current;\n\n    if (!currentFrames || currentFrames.length === 0) {\n      console.log('[SimView animateVisemes] No frames to animate or animation ended.');\n      stopVisemeAnimation();\n      return;\n    }\n\n    let activeFrame = null;\n    for (let i = 0; i < currentFrames.length; i++) {\n      if (currentTime >= currentFrames[i].time) {\n        activeFrame = currentFrames[i];\n      } else {\n        break; \n      }\n    }\n\n    if (activeFrame) {\n      setState(prev => ({ ...prev, currentVisemeShapes: activeFrame.shapes as BlendShapeMap }));\n      console.log('[SimView animateVisemes] Updated state.currentVisemeShapes with:', activeFrame.shapes);\n    }\n\n    const lastFrameTime = currentFrames.length > 0 ? currentFrames[currentFrames.length - 1].time : 0;\n    if (currentTime > lastFrameTime + 0.5 || (!activeFrame && currentTime > 0.1 && currentFrames.length > 0)) { \n      console.log('[SimView animateVisemes] Animation sequence likely complete or past last frame time.');\n      stopVisemeAnimation();\n      return;\n    }\n    \n    animationFrameIdRef.current = requestAnimationFrame(() => animateVisemes(startTime));\n  }, [setState, stopVisemeAnimation, visemeFramesRef, animationFrameIdRef]);\n\n  const startVisemeAnimation = useCallback(() => {\n    console.log('[SimView startVisemeAnimation] CALLED. Frame count:', visemeFramesRef.current.length);\n    if (visemeFramesRef.current && visemeFramesRef.current.length > 0) {\n      console.log('[SimView startVisemeAnimation] Starting viseme animation with frames:', visemeFramesRef.current);\n      const startTime = Date.now();\n      if (animationFrameIdRef.current) {\n        cancelAnimationFrame(animationFrameIdRef.current);\n      }\n      animateVisemes(startTime);\n    } else {\n      console.log('[SimView startVisemeAnimation] No viseme frames to animate.');\n      stopVisemeAnimation();\n    }\n  }, [animateVisemes, stopVisemeAnimation, animationFrameIdRef, visemeFramesRef]);\n\n  const playHumeOutputWithVisemes = useCallback(async (audioUrl: string, timeline: HumeTimelineEvent[]) => {\n    console.log('[SimView playHumeOutputWithVisemes] Called with audio URL and timeline.');\n    setState(prev => ({ ...prev, isAzureAudioActive: false, isHumeAudioActive: true })); // Indicate Hume audio is now primary\n\n    // Stop any ongoing Azure speech and animation\n    if (azureAudioRef.current) {\n      azureAudioRef.current.pause();\n      azureAudioRef.current.src = ''; // Release resource\n    }\n    stopVisemeAnimation();\n\n    if (!humeAudioRef.current) {\n      humeAudioRef.current = new Audio();\n      humeAudioRef.current.addEventListener('ended', () => {\n        console.log('[SimView HumeAudio] Playback ended.');\n        stopVisemeAnimation();\n        setState(prev => ({ ...prev, isSpeaking: false, isHumeAudioActive: false }));\n      });\n      humeAudioRef.current.addEventListener('error', (e) => {\n        console.error('[SimView HumeAudio] Audio playback error:', e);\n        stopVisemeAnimation();\n        setState(prev => ({ ...prev, isSpeaking: false, isHumeAudioActive: false }));\n      });\n    }\n\n    humeAudioRef.current.src = audioUrl;\n    humeAudioRef.current.load();\n\n    try {\n      await humeAudioRef.current.play();\n      console.log('[SimView HumeAudio] Playback started.');\n      setState(prev => ({ ...prev, isSpeaking: true }));\n\n      // Once playback starts, audio duration should be available\n      const audioDurationSeconds = humeAudioRef.current.duration;\n      if (isNaN(audioDurationSeconds) || audioDurationSeconds === 0) {\n        console.warn('[SimView playHumeOutputWithVisemes] Hume audio duration not available or zero after play. Visemes might be incorrect.');\n        // Potentially fall back to a default or estimated duration if timeline is rich\n        // For now, we'll proceed, but this could be an issue.\n      }\n\n      console.log(`[SimView playHumeOutputWithVisemes] Hume audio duration: ${audioDurationSeconds}s`);\n\n      // Convert Hume timeline to Azure VisemeEvents (which are like standard visemes)\n      const converted = convertHumeTimelineToAzureVisemes(timeline, audioDurationSeconds);\n      \n      // Convert these VisemeEvents to AnimationKeyframes for our animation system\n      const keyframes = humeVisemeEventsToAnimationKeyframes(converted.visemeEvents, converted.audioDurationMs);\n\n      if (keyframes && keyframes.length > 0) {\n        visemeFramesRef.current = keyframes;\n        startVisemeAnimation(); // This will use visemeFramesRef.current\n      } else {\n        console.log('[SimView playHumeOutputWithVisemes] No keyframes generated for Hume output. Lip sync will not occur.');\n      }\n\n    } catch (error) {\n      console.error('[SimView playHumeOutputWithVisemes] Error playing Hume audio or processing visemes:', error);\n      stopVisemeAnimation();\n      setState(prev => ({ ...prev, isSpeaking: false, isHumeAudioActive: false }));\n    }\n  }, [stopVisemeAnimation, startVisemeAnimation, humeVisemeEventsToAnimationKeyframes, defaultBlendShapes, animateVisemes]); // Added animateVisemes to depsRef]);\n\n  const handleHumeSpeakingStarted = useCallback((message?: ChatMessage) => {\n    console.log('[SimView handleHumeSpeakingStarted] Called. Setting isSpeaking to true.');\n    setState(prev => ({ ...prev, isSpeaking: true, statusMessage: 'Bot Speaking (Hume)...' }));\n  }, [setState]);\n\n  const handleHumeSpeakingStopped = useCallback(() => {\n    console.log('[SimView handleHumeSpeakingStopped] Called.');\n    if (!stateRef.current.isAzureAudioActive) {\n      console.log('[SimView handleHumeSpeakingStopped] Azure audio not active. Stopping visemes and setting isSpeaking to false.');\n      stopVisemeAnimation(); \n      setState((prev: SimulationViewState) => ({ ...prev, isSpeaking: false, statusMessage: 'Idle' }));\n    } else {\n      console.log('[SimView handleHumeSpeakingStopped] Azure audio is active, not stopping visemes or changing speaking state from here.');\n    }\n  }, [stopVisemeAnimation, setState, stateRef]);\n\n  const handleAzureAudioEnded = useCallback(() => {\n    console.log('[SimView handleAzureAudioEnded] CALLED.');\n    console.log('[SimView handleAzureAudioEnded] Setting isAzurePlayingRef.current = false');\n    isAzurePlayingRef.current = false;\n    stopVisemeAnimation();\n    setState(prev => ({ ...prev, isAzureAudioActive: false }));\n    if (azureAudioRef.current && azureAudioRef.current.src.startsWith('blob:')) {\n      URL.revokeObjectURL(azureAudioRef.current.src);\n      azureAudioRef.current.src = ''; \n      azureAudioRef.current.srcObject = null;\n      console.log('[SimView handleAzureAudioEnded] Revoked blob URL and reset azureAudioRef src.');\n    }\n  }, [stopVisemeAnimation, setState, azureAudioRef]);\n\n  const handleAzureAudioError = useCallback((event: React.SyntheticEvent<HTMLAudioElement, Event>) => {\n    const audioElement = event.target as HTMLAudioElement;\n    console.error('[SimView handleAzureAudioError] Azure audio playback error:', audioElement.error);\n    stopVisemeAnimation();\n    setState(prev => ({ ...prev, isAzureAudioActive: false, statusMessage: 'Error playing Azure audio.' }));\n    if (azureAudioRef.current && azureAudioRef.current.src.startsWith('blob:')) {\n      URL.revokeObjectURL(azureAudioRef.current.src);\n      azureAudioRef.current.src = '';\n      azureAudioRef.current.srcObject = null;\n      console.log('[SimView handleAzureAudioError] Revoked blob URL and reset azureAudioRef src after error.');\n    }\n  }, [stopVisemeAnimation, setState, azureAudioRef]);\n\n  // Test function (can be removed or kept for debugging)\n  const generateAndAnimateVisemesFromText = useCallback(async (text: string, voiceName?: string) => {\n    console.log(`[SimView generateAndAnimateVisemesFromText] Test function called with text: \"${text}\", voice: ${voiceName}`);\n    if (!azureAudioRef.current) {\n      console.error(\"[SimView generateAndAnimateVisemesFromText] azureAudioRef.current is null.\");\n      return;\n    }\n    try {\n      setState(prev => ({ ...prev, statusMessage: 'Testing Visemes...', isSpeaking: true, isAzureAudioActive: true }));\n      if (!azureAudioRef.current.paused) {\n        azureAudioRef.current.pause();\n        azureAudioRef.current.currentTime = 0;\n      }\n      const serviceResult = await untypedSynthesizeSpeechWithVisemes(text, voiceName || stateRef.current.azureVoiceName);\n\n      // ===== DETAILED LOGGING START =====\n      console.log('[SimView generateAndAnimateVisemesFromText] Full serviceResult from VisemeService:', JSON.stringify(serviceResult, null, 2));\n      if (serviceResult?.visemeData?.blendShapeFrames && serviceResult.visemeData.blendShapeFrames.length > 0) {\n        console.log('[SimView generateAndAnimateVisemesFromText] First 3 blendShapeFrames from serviceResult:');\n        for (let i = 0; i < Math.min(3, serviceResult.visemeData.blendShapeFrames.length); i++) {\n          console.log(`  Frame ${i}: audioOffset=${serviceResult.visemeData.blendShapeFrames[i].audioOffset}, shapes=${JSON.stringify(serviceResult.visemeData.blendShapeFrames[i].shapes?.slice(0,5))}...`);\n        }\n        const lastFrameIdx = serviceResult.visemeData.blendShapeFrames.length - 1;\n        if (lastFrameIdx >= 0) {\n            console.log(`  Last Frame ${lastFrameIdx}: audioOffset=${serviceResult.visemeData.blendShapeFrames[lastFrameIdx].audioOffset}, shapes=${JSON.stringify(serviceResult.visemeData.blendShapeFrames[lastFrameIdx].shapes?.slice(0,5))}...`);\n        }\n      } else {\n        console.log('[SimView generateAndAnimateVisemesFromText] serviceResult.visemeData.blendShapeFrames is missing, empty, or not an array.');\n      }\n      if (serviceResult && 'audioDurationMs' in serviceResult) {\n        console.log('[SimView generateAndAnimateVisemesFromText] serviceResult.audioDurationMs:', serviceResult.audioDurationMs);\n      } else {\n        console.log('[SimView generateAndAnimateVisemesFromText] serviceResult.audioDurationMs is not available.');\n      }\n      // ===== DETAILED LOGGING END =====\n\n      if (serviceResult && serviceResult.audioData && serviceResult.visemeData) {\n        const preparedFrames = prepareVisemeFrames(serviceResult.visemeData);\n        if (preparedFrames && preparedFrames.length > 0) {\n          visemeFramesRef.current = preparedFrames;\n          startVisemeAnimation();\n        } else {\n          stopVisemeAnimation();\n        }\n        const audioBlob = new Blob([serviceResult.audioData], { type: 'audio/mpeg' });\n        const audioUrl = URL.createObjectURL(audioBlob);\n        azureAudioRef.current.srcObject = null;\n        azureAudioRef.current.src = audioUrl;\n        azureAudioRef.current.play().catch(e => {\n          console.error('[SimView generateAndAnimateVisemesFromText] Error playing test audio:', e);\n          handleAzureAudioEnded();\n        });\n      } else {\n        console.error('[SimView generateAndAnimateVisemesFromText] Failed to get audio/viseme data for test.');\n        handleAzureAudioEnded();\n      }\n    } catch (error) {\n      console.error('[SimView generateAndAnimateVisemesFromText] Error in test function:', error);\n      handleAzureAudioEnded();\n    }\n  }, [untypedSynthesizeSpeechWithVisemes, prepareVisemeFrames, startVisemeAnimation, stopVisemeAnimation, handleAzureAudioEnded, setState, stateRef, azureAudioRef, visemeFramesRef]);\n\n  const processAndPlayAzureSpeech = useCallback(async (text: string) => {\n    console.log('[SimView processAndPlayAzureSpeech] CALLED. Text:', text);\n\n  if (!azureAudioRef.current) {\n    console.error(\"[SimView processAndPlayAzureSpeech] azureAudioRef.current is null. Cannot play Azure audio.\");\n    handleAzureAudioEnded(); // Call to clean up state\n    return;\n  }\n\n  setState(prev => ({ ...prev, isSpeaking: true, isAzureAudioActive: true, statusMessage: 'Bot Speaking (Azure)...' }));\n\n    if (azureAudioRef.current && !azureAudioRef.current.paused) {\n      console.warn('[SimView processAndPlayAzureSpeech] Azure audio player (azureAudioRef) was active. Stopping existing playback.');\n      azureAudioRef.current.pause();\n      azureAudioRef.current.currentTime = 0;\n    }\n\n    try {\n      console.log('[SimView processAndPlayAzureSpeech] Attempting to synthesize with voice:', state.azureVoiceName);\n      const resolvedVoiceName = state.azureVoiceName;\n      // Ensure untypedSynthesizeSpeechWithVisemes is the imported one, not the placeholder\n      const serviceResult = await untypedSynthesizeSpeechWithVisemes(text, resolvedVoiceName);\n      console.log('[SimView processAndPlayAzureSpeech] serviceResult (summary):', JSON.stringify(serviceResult ? { audioDataLength: serviceResult.audioData?.byteLength, blendShapeFramesLength: serviceResult.visemeData?.blendShapeFrames?.length, standardVisemesLength: serviceResult.visemeData?.standardVisemes?.length } : null));\n\n      if (serviceResult && serviceResult.audioData && serviceResult.visemeData) {\n        console.log('[SimView processAndPlayAzureSpeech] VisemeData received from Azure. BlendShapeFrames count:', serviceResult.visemeData.blendShapeFrames?.length, 'StandardVisemes count:', serviceResult.visemeData.standardVisemes?.length, '. Now calling prepareVisemeFrames.');\n        const preparedFrames = prepareVisemeFrames(serviceResult.visemeData);\n        console.log('[SimView processAndPlayAzureSpeech] preparedFrames:', preparedFrames); // CASCADE ADDED LOG\n        if (preparedFrames && preparedFrames.length > 0) {\n          visemeFramesRef.current = preparedFrames;\n          console.log('[SimView processAndPlayAzureSpeech] visemeFramesRef.current SET. Length:', visemeFramesRef.current.length);\n        } else {\n          visemeFramesRef.current = [];\n          console.log('[SimView processAndPlayAzureSpeech] preparedFrames EMPTY or invalid. visemeFramesRef.current set to empty array.');\n        }\n        const audioBlob = new Blob([serviceResult.audioData], { type: 'audio/mpeg' });\n        const audioUrl = URL.createObjectURL(audioBlob);\n        azureAudioRef.current.srcObject = null;\n        azureAudioRef.current.src = audioUrl;\n        azureAudioRef.current.play().then(() => {\n          console.log('[SimView processAndPlayAzureSpeech] Azure audio playback REALLY STARTED.');\n          if (visemeFramesRef.current && visemeFramesRef.current.length > 0) {\n            console.log('[SimView processAndPlayAzureSpeech] Calling startVisemeAnimation after audio play().then()');\n            startVisemeAnimation();\n          } else {\n            console.log('[SimView processAndPlayAzureSpeech] No viseme frames to animate after audio play().then(), stopping animation.');\n            stopVisemeAnimation(); // Ensure any prior animation is stopped\n          }\n        }).catch(e => {\n          console.error('[SimView processAndPlayAzureSpeech] Error playing Azure audio:', e);\n          handleAzureAudioEnded(); // Ensure state is cleaned up\n        });\n        console.log('[SimView processAndPlayAzureSpeech] Azure audio playback INITIATED. Setting isAzurePlayingRef.current = true');\n        isAzurePlayingRef.current = true;\n      } else {\n        console.error('[SimView processAndPlayAzureSpeech] Failed to synthesize Azure speech or missing audio data. Result:', serviceResult);\n        handleAzureAudioEnded(); // Ensure state is cleaned up\n      }\n    } catch (error) {\n      console.error('[SimView processAndPlayAzureSpeech] Error during Azure TTS synthesis:', error);\n      handleAzureAudioEnded(); // Ensure state is cleaned up\n    }\n  }, [untypedSynthesizeSpeechWithVisemes, azureAudioRef, handleHumeSpeakingStopped, prepareVisemeFrames, startVisemeAnimation, stopVisemeAnimation, isHumeVoicePlayingRef, handleAzureAudioEnded, visemeFramesRef]);\n\n  // ... (rest of the code remains the same)\n\n  useEffect(() => {\n    if (props.humeMessages.length === 0) {\n      return;\n    }\n\n    const currentMessage = props.humeMessages[props.humeMessages.length - 1];\n    if (!currentMessage) return;\n\n    // Deduplication logic\n    if (currentMessage.type === 'user_message') {\n      if (lastProcessedUserMessageReceivedAtRef.current && currentMessage.receivedAt <= lastProcessedUserMessageReceivedAtRef.current) {\n        // console.log(`[SimView] User message at ${currentMessage.receivedAt} already processed or older. Skipping.`);\n        return;\n      }\n    } else if (currentMessage.type === 'assistant_message' && currentMessage.id) {\n      if (currentMessage.id === lastProcessedAssistantMessageIdRef.current) {\n        // console.log(`[SimView] Assistant message ID ${currentMessage.id} already processed. Skipping.`);\n        return;\n      }\n    }\n\n    // Log assistant_message to check for face/viseme data, specifically if 'models' are present\n    if (currentMessage.type === 'assistant_message' && currentMessage.models) {\n      console.log('[Hume EVI Message Inspector] Assistant Message with Models:', JSON.stringify(currentMessage, null, 2));\n    }\n\n    switch (currentMessage.type) {\n      case 'user_message': {\n        if (!currentMessage.message || typeof currentMessage.message.content !== 'string' || currentMessage.message.role !== 'user') {\n          console.warn('[SimView] User message content/role issue or type mismatch. Skipping. Full message:', JSON.stringify(currentMessage, null, 2));\n          break;\n        }\n        console.log(`[SimView] user_message: Role: ${currentMessage.message.role}. User text:`, currentMessage.message.content);\n        const newUserChatMessage: ChatMessage = {\n          id: uuidv4(),\n          text: currentMessage.message.content,\n          sender: 'user',\n          timestamp: currentMessage.receivedAt,\n        };\n        setState((prev: SimulationViewState) => ({ ...prev, messages: [...prev.messages, newUserChatMessage] }));\n        lastProcessedUserMessageReceivedAtRef.current = currentMessage.receivedAt;\n        break;\n      }\n      case 'assistant_message': {\n        console.log('[SimView] ENTERING assistant_message case block. Message ID:', currentMessage.id);\n        if (currentMessage.models?.prosody?.scores) {\n          console.log('[SimView] DETECTED PROSODY SCORES:', JSON.stringify(currentMessage.models.prosody.scores, null, 2));\n        } else {\n          console.log('[SimView] Assistant Message, but NO prosody.scores. Models:', JSON.stringify(currentMessage.models, null, 2));\n        }\n\n        if (!currentMessage.message || typeof currentMessage.message.content !== 'string' || currentMessage.message.role !== 'assistant') {\n          console.warn('[SimView] Assistant message content/role issue or type mismatch. Skipping. Full message:', JSON.stringify(currentMessage, null, 2));\n          break;\n        }\n        console.log(`[SimView] assistant_message: Role: ${currentMessage.message.role}. Assistant text:`, currentMessage.message.content);\n        const newAssistantChatMessage: ChatMessage = {\n          id: currentMessage.id || uuidv4(),\n          text: currentMessage.message.content,\n          sender: 'bot', // Changed 'assistant' to 'bot'\n          timestamp: currentMessage.receivedAt,\n          // emotion: getTopEmotion(currentMessage.models?.prosody?.scores)\n        };\n        let newEmotions: Emotion[] = [{ name: 'neutral', score: 1 }]; // Default\n        let emotionsSource = 'default (neutral)';\n\n        // According to Hume EVI docs and previous logs, prosody scores are the primary source for general emotion.\n        // The 'predictions' field under prosody or face might contain more detailed expression data if configured,\n        // but 'scores' under 'prosody' is for overall emotional tone of speech.\n        if (currentMessage.models?.prosody?.scores && Array.isArray(currentMessage.models.prosody.scores) && currentMessage.models.prosody.scores.length > 0) {\n          // Ensure the scores match the Emotion type structure (name, score)\n          const potentialEmotions = currentMessage.models.prosody.scores.filter(\n            (s: any) => typeof s.name === 'string' && typeof s.score === 'number'\n          );\n          if (potentialEmotions.length > 0) {\n            newEmotions = potentialEmotions;\n            emotionsSource = 'prosody.scores';\n          } else {\n            console.warn('[SimView] Hume prosody.scores present but items do not match Emotion structure:', JSON.stringify(currentMessage.models.prosody.scores));\n            emotionsSource = 'prosody.scores (invalid structure)';\n          }\n        } else {\n          console.log('[SimView] No prosody.scores found in Hume message. Using default neutral emotion.');\n        }\n        console.log(`[SimView] Extracted emotions from Hume message (source: ${emotionsSource}):`, JSON.stringify(newEmotions));\n\n        setState((prev: SimulationViewState) => {\n          const prosodyScoresForBlendshapes = newEmotions.reduce((acc: Record<string, number>, emotion: any) => {\n            acc[emotion.name] = emotion.score;\n            return acc;\n          }, {});\n          \n          console.log('[SimView assistant_message] newEmotions feeding into blendshapes:', JSON.stringify(newEmotions));\n          console.log('[SimView assistant_message] prosodyScoresForBlendshapes for prosodyToBlendshapes:', JSON.stringify(prosodyScoresForBlendshapes));\n          \n          const newProsodyBlendshapes = prosodyToBlendshapes(prosodyScoresForBlendshapes);\n          \n          console.log(`[SimView assistant_message] newProsodyBlendshapes from prosodyToBlendshapes (count: ${Object.keys(newProsodyBlendshapes).length}):`, JSON.stringify(newProsodyBlendshapes));\n\n          return {\n            ...prev,\n            messages: [...prev.messages, newAssistantChatMessage],\n            currentDetectedEmotions: newEmotions, // newEmotions is from the outer scope, reflecting Hume's detected emotion\n            isSpeaking: props.isHumeVoicePlaying,\n            statusMessage: props.isHumeVoicePlaying ? 'Bot Speaking (Hume EVI)' : (prev.statusMessage.includes('Azure') ? prev.statusMessage : 'Bot Idle'),\n            prosodyDrivenBlendshapes: newProsodyBlendshapes, // Use the dynamically calculated blendshapes\n          };\n        });\n\n        if (currentMessage.message.content) {\n          // console.log('[SimView] Assistant message content present. Azure viseme processing currently disabled for MVP.');\n          // processAndPlayAzureSpeech(currentMessage.message.content); // MVP Strategy: Disabled Azure visemes\n        }\n        if (currentMessage.id) {\n          lastProcessedAssistantMessageIdRef.current = currentMessage.id;\n        }\n        break;\n      }\n      case 'assistant_end': {\n        console.log('[SimView] assistant_end message received:', currentMessage);\n        // Perform any necessary cleanup or state changes for assistant ending speech.\n        break;\n      }\n      case 'error': { // This is for WebSocketError\n        console.error('[SimView] WebSocketError message received:', JSON.stringify(currentMessage, null, 2));\n        setState((prev: SimulationViewState) => ({ ...prev, statusMessage: `Hume EVI Error: ${currentMessage.message}`}));\n        break;\n      }\n      case 'chat_metadata': {\n        console.log('[SimView] chat_metadata message received:', currentMessage);\n        break;\n      }\n      case 'user_interruption': {\n        console.log('[SimView] user_interruption message received:', currentMessage);\n        if (azureAudioRef.current) {\n          azureAudioRef.current.pause();\n          azureAudioRef.current.src = '';\n        }\n        stopVisemeAnimation();\n        setState((prev: SimulationViewState) => ({ ...prev, isSpeaking: false, isAzureAudioActive: false, statusMessage: 'User interrupted bot.' }));\n        break;\n      }\n      case 'tool_call': {\n        console.log('[SimView] tool_call message received:', currentMessage);\n        break;\n      }\n      case 'tool_response': {\n        console.log('[SimView] tool_response message received:', currentMessage);\n        break;\n      }\n      case 'tool_error': {\n        console.log('[SimView] tool_error message received:', currentMessage);\n        break;\n      }\n      default: {\n        // const _exhaustiveCheck: never = currentMessage; // For exhaustive type checking\n        console.log('[SimView] Received unhandled or unexpected message type:', (currentMessage as any).type, 'Full message:', JSON.stringify(currentMessage, null, 2));\n        break;\n      }\n    }\n    // Cleanup function for this useEffect\n    return () => {\n      if (speakingDebounceTimerRef.current) {\n        clearTimeout(speakingDebounceTimerRef.current);\n        speakingDebounceTimerRef.current = null;\n      }\n    };\n  }, [props.humeMessages, processAndPlayAzureSpeech, props.isHumeVoicePlaying]);\n\n  // The useEffect that synchronized isSpeaking with props.isHumeVoicePlaying has been removed\n  // to prevent 'Maximum update depth exceeded' error. \n  // isSpeaking and statusMessage will be derived or handled by Azure's lifecycle directly.\n\n  // Effect to manage isSpeaking state based on Hume's voice activity\n  useEffect(() => {\n    if (props.isHumeVoicePlaying) {\n      // Hume started speaking\n      if (speakingDebounceTimerRef.current) {\n        clearTimeout(speakingDebounceTimerRef.current);\n        speakingDebounceTimerRef.current = null;\n      }\n      // Check if we are not already in 'isSpeaking' state from Hume to avoid redundant calls if handleHumeSpeakingStarted is complex\n      // However, handleHumeSpeakingStarted itself should be idempotent or handle this.\n      handleHumeSpeakingStarted(); \n    } else {\n      // Hume stopped speaking\n      // Only try to stop if we were previously speaking (stateRef.current.isSpeaking)\n      // and Azure is not playing (isAzurePlayingRef.current is false, which is always the case in MVP strategy)\n      if (stateRef.current.isSpeaking) { \n        if (speakingDebounceTimerRef.current) {\n          clearTimeout(speakingDebounceTimerRef.current);\n        }\n        speakingDebounceTimerRef.current = setTimeout(() => {\n          console.log('[SimView useEffect HumeVoicePlaying] Debounced: Hume stopped, calling handleHumeSpeakingStopped.');\n          handleHumeSpeakingStopped();\n        }, DEBOUNCE_DURATION);\n      }\n    }\n\n    // Cleanup timer on component unmount or if dependencies change causing effect re-run before timer fires\n    return () => {\n      if (speakingDebounceTimerRef.current) {\n        clearTimeout(speakingDebounceTimerRef.current);\n        speakingDebounceTimerRef.current = null;\n      }\n    };\n  }, [props.isHumeVoicePlaying, handleHumeSpeakingStarted, handleHumeSpeakingStopped, stateRef, speakingDebounceTimerRef]);\n\n  const handleInputChange = useCallback((event: React.ChangeEvent<HTMLInputElement>) => {\n    setState(prev => ({ ...prev, inputValue: event.target.value }));\n  }, [setState]);\n\n  const handleSendMessage = useCallback(async (event?: React.FormEvent<HTMLFormElement>) => {\n    if (event) {\n      event.preventDefault();\n    }\n    const text = stateRef.current.inputValue.trim();\n    if (!text) return;\n\n    const newUserMessage = {\n      id: uuidv4(),\n      text: text,\n      sender: 'user' as const,\n      timestamp: new Date(),\n    };\n\n    setState(prev => ({\n      ...prev,\n      messages: [...prev.messages, newUserMessage],\n      inputText: '', \n    }));\n\n    console.log('[SimView] Sending user message to Hume EVI:', text);\n    if (props.sendUserInputToVoice) {\n        props.sendUserInputToVoice(text); // Corrected: sendUserInput expects a string\n    } else {\n        console.warn('[SimView] sendUserInput is not available. Message not sent to Hume.');\n    }\n    \n    // if (props.simulationId && currentUser?.uid) {\n    //   try {\n    //     await saveChatMessageToFirebase(props.simulationId, currentUser.uid, newUserMessage);\n    //     console.log('[SimView] User message saved to Firebase.');\n    //   } catch (error) {\n    //     console.error('[SimView] Error saving user message to Firebase:', error);\n    //   }\n    // }\n  }, [setState, props.sendUserInputToVoice, props.simulationId, currentUser, stateRef, customSessionIdRef /*, saveChatMessageToFirebase */]);\n\n  const handleAvatarEmotionDetected = useCallback((emotionData: any) => {\n    console.log('[SimView] Avatar Emotion Detected:', emotionData);\n  }, []);\n\n  const handleAvatarError = useCallback((error: Error) => { // Changed 'any' to 'Error'\n    console.error(\"[SimView] Avatar Error:\", error);\n    // Log the full error object for more details\n    console.error(\"[SimView] Full avatar error object:\", error); \n    const errorMessage = error instanceof Error ? error.message : String(error);\n    setState(prev => ({ ...prev, statusMessage: `Avatar error: ${errorMessage}` }));\n  }, [setState]);\n\n  const handleAvatarLoad = useCallback(() => {\n    console.log('[SimView] Avatar Loaded successfully.');\n    setState(prev => ({ ...prev, statusMessage: 'Avatar loaded.' }));\n  }, [setState]);\n\n  const toggleCamera = useCallback(() => {\n    setState(prev => ({ ...prev, isCameraEnabled: !prev.isCameraEnabled })); // This was correct, assuming isCameraEnabled is in state\n  }, [setState]);\n\n  const toggleMic = useCallback(() => {\n    setState((prev: SimulationViewState) => {\n      console.log(`[SimView toggleMic] Clicked. Current voiceReadyState: ${getVoiceReadyStateName(props.voiceReadyState)}, current isMicMuted (before toggle): ${prev.isMicMuted}`);\n      const newMicMutedState = !prev.isMicMuted;\n      if (newMicMutedState) { // Mic is being muted\n        console.log('[SimView toggleMic] Mic muted. Disconnecting Hume Voice.');\n        props.disconnectVoice();\n      } else { // Mic is being unmuted\n        console.log(`[SimView toggleMic] Mic unmuted. Attempting to connect Hume Voice.`);\n        console.log(`[SimView toggleMic] Current state values - humeAccessToken: ${stateRef.current.humeAccessToken ? 'SET' : 'NOT SET'}, humeConfigId: ${stateRef.current.humeConfigId || 'NOT SET (uses default from Provider)'}`);\n        \n        props.connectVoice()\n          .then(() => {\n            console.log('[SimView toggleMic] Hume Voice connect() promise resolved successfully.');\n            // setState(s => ({...s, statusMessage: 'Mic active, connected to Hume.'})); // Status will be updated by readyState changes\n          })\n          .catch(error => {\n            console.error('[SimView toggleMic] Error connecting Hume Voice:', error);\n            setState((s: SimulationViewState) => ({...s, statusMessage: `Error connecting mic: ${(error as Error).message}`}));\n          });\n      }\n      // Update mic state for UI and internal logic\n      return { ...prev, isMicMuted: newMicMutedState, isMicOn: !newMicMutedState, statusMessage: newMicMutedState ? 'Mic muted.' : 'Mic unmuted, attempting connection...' };\n    });\n  }, [setState, props.connectVoice, props.disconnectVoice, stateRef, props.voiceReadyState]);\n\n  const toggleChat = useCallback(() => {\n    setState((prev: SimulationViewState) => ({ ...prev, showChat: !prev.showChat })); // This was correct, assuming showChat is in state\n  }, [setState]);\n\n  const toggleSound = useCallback(() => {\n    setState((prev: SimulationViewState) => {\n      const newIsSoundOn = !prev.isSoundOn;\n      if (azureAudioRef.current) {\n        azureAudioRef.current.muted = !newIsSoundOn;\n      }\n      return { ...prev, isSoundOn: newIsSoundOn, statusMessage: newIsSoundOn ? 'Sound unmuted.' : 'Sound muted.' };\n    });\n  }, [setState, azureAudioRef]);\n\n  const forceStopAllAudio = useCallback(() => {\n    console.log('[SimViewInternal] forceStopAllAudio called.');\n    // Stop Azure TTS Audio & Visemes\n    if (azureAudioRef.current && !azureAudioRef.current.paused) {\n      azureAudioRef.current.pause();\n      if (azureAudioRef.current.src && azureAudioRef.current.src.startsWith('blob:')) {\n        URL.revokeObjectURL(azureAudioRef.current.src);\n      }\n      azureAudioRef.current.src = '';\n    }\n    stopVisemeAnimation(); // This also sets isSpeaking to false\n\n    setState((prev: SimulationViewState) => ({\n      ...prev,\n      // isHumeVoicePlaying: false, // This state is more related to the Hume connection itself\n      isAzureAudioActive: false,\n      // isSpeaking is handled by stopVisemeAnimation\n      statusMessage: 'Local audio and speech visualization stopped.',\n    }));\n    // If stopping the Hume connection itself is desired from this button,\n    // SimulationViewInternal would need a prop callback from SimulationView to trigger voice.disconnect().\n  }, [azureAudioRef, stopVisemeAnimation, setState]);\n\n  useImperativeHandle(ref, () => ({\n    forceStopHumeSpeaking: () => {\n      console.log('[SimViewInternal forceStopHumeSpeaking] Imperatively called.');\n      if (azureAudioRef.current && !azureAudioRef.current.paused) {\n        azureAudioRef.current.pause();\n        if (azureAudioRef.current.src && azureAudioRef.current.src.startsWith('blob:')) {\n          URL.revokeObjectURL(azureAudioRef.current.src);\n        }\n        azureAudioRef.current.src = '';\n        // Consider azureAudioRef.current.load(); if issues persist after src reset\n      }\n      stopVisemeAnimation(); // This resets blendshapes and sets isSpeaking to false\n      setState((prevState: SimulationViewState) => ({\n        ...prevState,\n        // isSpeaking: false, // Handled by stopVisemeAnimation\n        isAzureAudioActive: false,\n        statusMessage: prevState.isAzureAudioActive ? 'Azure speech stopped via external call.' : 'Hume speech stop requested externally.',\n      }));\n      // Note: If Hume EVI has its own audio output that bypasses azureAudioRef (e.g. direct Web Audio API usage by useVoice),\n      // additional logic might be needed here to stop that. The current implementation assumes\n      // isHumeVoicePlaying reflects this and that the parent (SimulationView) handles disconnects/reconnects.\n    }\n  }), [azureAudioRef, stopVisemeAnimation, setState]);\n\n  // --- Derived state logic for isSpeaking and statusMessage ---\n  let derivedIsSpeaking = state.isSpeaking;\n  let derivedStatusMessage = state.statusMessage;\n\n  // Prioritize Azure's state if it's active\n  if (state.isAzureAudioActive) {\n    // Azure is active, its lifecycle methods (e.g., processAndPlayAzureSpeech) directly set state.isSpeaking and state.statusMessage.\n    // So, derivedIsSpeaking and derivedStatusMessage are already correctly reflecting Azure's state from state.isSpeaking and state.statusMessage.\n  } else if (props.isHumeVoicePlaying) {\n    // Hume is playing, and Azure is NOT active.\n    derivedIsSpeaking = true;\n    derivedStatusMessage = 'Bot Speaking (Hume EVI)';\n  } else {\n    // Neither Hume nor Azure is actively playing/speaking.\n    derivedIsSpeaking = false;\n    derivedStatusMessage = 'Idle';\n  }\n  // --- End of derived state logic ---\n\n  console.log('[SimViewInternal BEFORE RETURN] isHumeVoicePlaying:', props.isHumeVoicePlaying, 'Current state.isSpeaking:', state.isSpeaking, 'DerivedIsSpeaking:', derivedIsSpeaking, 'DerivedStatusMessage:', derivedStatusMessage);\n\n  return (\n    <div className=\"simulation-view\" style={{ display: 'flex', height: '100vh', flexDirection: 'column', background: '#282c34' }}>\n      {/* Header/Status Bar */}\n      <div className=\"status-bar\" style={{ padding: '10px', background: '#20232a', color: 'white', textAlign: 'center', fontSize: '0.9em', flexShrink: 0 }}>\n        Status: {derivedStatusMessage} | Hume: {(typeof props.voiceReadyState === 'number' && VoiceReadyState[props.voiceReadyState]) ? VoiceReadyState[props.voiceReadyState] : String(props.voiceReadyState)} | Speaking: {derivedIsSpeaking ? \"Yes\" : \"No\"} | Hume Playing: {props.isHumeVoicePlaying ? \"Yes\" : \"No\"}\n      </div>\n\n      {/* Test Mode Toggle */}\n      <TestModeToggle\n        onMockMessage={(message) => {\n          console.log('🧪 Mock message received:', message);\n          // Simulate processing the mock message like a real Hume message\n          if (message.prosody) {\n            // Convert array format [{name, score, timestamp}] to Record<string, number>\n            const prosodyRecord = message.prosody.reduce((acc: Record<string, number>, emotion: any) => {\n              acc[emotion.name] = emotion.score;\n              return acc;\n            }, {});\n            const prosodyBlendshapes = prosodyToBlendshapes(prosodyRecord);\n            setState(prev => ({ ...prev, prosodyDrivenBlendshapes: prosodyBlendshapes }));\n          }\n          if (message.timeline) {\n            // Simulate viseme processing\n            const mockVisemes = { jawOpen: 0.4, mouthFunnel: 0.3 };\n            setState(prev => ({ ...prev, currentVisemeShapes: mockVisemes }));\n          }\n          if (message.type === 'audio_output') {\n            setState(prev => ({ ...prev, isSpeaking: true }));\n            // Simulate audio ending after 2 seconds\n            setTimeout(() => {\n              setState(prev => ({ ...prev, isSpeaking: false, currentVisemeShapes: {} }));\n            }, 2000);\n          }\n        }}\n        onMockProsody={(emotions) => {\n          console.log('🧪 Mock prosody received:', emotions);\n          // Convert array format to Record format\n          const prosodyRecord = emotions.reduce((acc: Record<string, number>, emotion: any) => {\n            acc[emotion.name] = emotion.score;\n            return acc;\n          }, {});\n          const prosodyBlendshapes = prosodyToBlendshapes(prosodyRecord);\n          setState(prev => ({ ...prev, prosodyDrivenBlendshapes: prosodyBlendshapes }));\n        }}\n        onMockVisemes={(visemes) => {\n          console.log('🧪 Mock visemes received:', visemes);\n          setState(prev => ({ ...prev, currentVisemeShapes: visemes }));\n        }}\n      />\n\n      {/* Main Content Area (Avatar + Chat) */}\n      <div style={{ display: 'flex', flexGrow: 1, position: 'relative', overflow: 'hidden' }}>\n        {/* Avatar Display Area */}\n        <div className=\"avatar-container\" style={{ flexGrow: 1, display: 'flex', justifyContent: 'center', alignItems: 'center', position: 'relative', background: '#333740' }}>\n          {(() => { console.log('[SimViewInternal RENDER] Passing DERIVED isSpeaking to EmotionDrivenAvatar:', derivedIsSpeaking); return null; })()}\n          <EmotionDrivenAvatar\n            directBlendshapes={{ ...state.manualBlendshapes, ...state.prosodyDrivenBlendshapes }} // Combine manual and prosody blendshapes\n            key={state.avatarUrl} // Add stable key to prevent remounts\n            ref={avatarGroupRef} // Pass the ref\n            avatarUrl={state.avatarUrl} // Use state.avatarUrl\n            isSpeaking={derivedIsSpeaking} // Pass DERIVED speaking state\n            visemeData={state.currentVisemeShapes} // Attempting to use visemeData to resolve TS error\n            currentEmotion={state.currentEmotion} // Pass current emotion\n            idleShapes={idleBlendShapes} // Pass idle shapes for blinking/restingolve TS error\n            detectedEmotions={state.currentEmotion ? [{ name: state.currentEmotion, score: 1.0 }] : []} // Pass current emotion\n            cameraEnabled={state.isCameraEnabled} // Pass camera enabled state\n            talkAnimationPaths={ALL_TALK_ANIMATION_GLBS} \n            idleAnimationPaths={ALL_IDLE_ANIMATION_GLBS} \n            onError={handleAvatarErrorCb}\n            onLoad={handleAvatarLoadCb}\n          />\n          <button \n            onClick={() => generateAndAnimateVisemesFromText(\"Hello world, this is a test.\", state.azureVoiceName || \"en-US-JennyNeural\")}\n            style={{ ...buttonStyle, position: 'absolute', top: '10px', left: '10px', zIndex: 10 }}\n          >\n            Test Visemes\n          </button>\n        </div>\n\n        {/* Chat UI Area */} \n        {state.showChat && (\n          <div className=\"chat-ui\" style={{\n            width: '350px',\n            minWidth: '300px',\n            background: 'rgba(30, 33, 40, 0.95)',\n            color: 'white',\n            display: 'flex',\n            flexDirection: 'column',\n            padding: '15px',\n            borderLeft: '1px solid #4f5461',\n            boxSizing: 'border-box',\n            position: 'absolute',\n            right: 0,\n            top: 0,\n            bottom: 0,\n            zIndex: 5,\n            transition: 'transform 0.3s ease-in-out',\n            transform: 'translateX(0%)'\n          }}>\n            <div style={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center', marginBottom: '15px', flexShrink: 0 }}>\n              <h3 style={{ margin: 0, fontSize: '1.2em' }}>Conversation</h3>\n              <button onClick={toggleChat} style={{ background: 'none', border: 'none', color: '#aaa', cursor: 'pointer', fontSize: '1.5em', padding: '5px' }}>\n                X\n              </button>\n            </div>\n            <div className=\"messages-area\" style={{ flexGrow: 1, overflowY: 'auto', marginBottom: '15px', paddingRight: '10px' }}>\n              {state.messages.map((msg) => (\n                <div key={msg.id} className={`message ${msg.sender}`} style={{\n                  marginBottom: '12px',\n                  padding: '10px 15px',\n                  borderRadius: '18px',\n                  maxWidth: '85%',\n                  alignSelf: msg.sender === 'user' ? 'flex-end' : 'flex-start',\n                  background: msg.sender === 'user' ? '#007bff' : '#495057',\n                  color: 'white',\n                  textAlign: msg.sender === 'user' ? 'right' : 'left',\n                  marginLeft: msg.sender === 'bot' ? '0' : 'auto',\n                  marginRight: msg.sender === 'user' ? '0' : 'auto',\n                  wordWrap: 'break-word',\n                  fontSize: '0.95em'\n                }}>\n                  {msg.text}\n                  {msg.emotion && <em style={{fontSize: '0.8em', display: 'block', opacity: 0.7, marginTop: '4px'}}>({msg.emotion})</em>}\n                </div>\n              ))}\n              <div ref={messagesEndRef} />\n            </div>\n            <audio\n              ref={azureAudioRef}\n              onEnded={handleAzureAudioEnded}\n              onError={handleAzureAudioError}\n              style={{ display: 'none' }}\n            />\n            <form onSubmit={handleSendMessage} style={{ display: 'flex', marginTop: 'auto', paddingTop: '10px', borderTop: '1px solid #4f5461', flexShrink: 0 }}>\n              <input\n                type=\"text\"\n                value={state.inputValue}\n                onChange={handleInputChange}\n                placeholder=\"Type your message...\"\n                style={{\n                  flexGrow: 1,\n                  padding: '12px 18px',\n                  border: '1px solid #555',\n                  borderRadius: '25px',\n                  marginRight: '10px',\n                  outline: 'none',\n                  color: '#e0e0e0',\n                  background: '#3a3f47',\n                  fontSize: '1em'\n                }}\n              />\n              <button\n                type=\"submit\"\n                disabled={!state.inputValue.trim()}\n                style={{\n                  background: state.inputValue.trim() ? '#007bff' : '#555',\n                  color: 'white',\n                  border: 'none',\n                  borderRadius: '50%',\n                  width: '48px',\n                  height: '48px',\n                  display: 'flex',\n                  alignItems: 'center',\n                  justifyContent: 'center',\n                  cursor: state.inputValue.trim() ? 'pointer' : 'not-allowed',\n                  opacity: state.inputValue.trim() ? 1 : 0.6,\n                  transition: 'background-color 0.2s ease'\n                }}\n              >\n                &gt;\n              </button>\n            </form>\n          </div>\n        )}\n        \n        {!state.showChat && (\n          <button\n            onClick={toggleChat}\n            style={{\n              position: 'absolute',\n              bottom: '30px',\n              right: '30px',\n              background: '#007bff',\n              color: 'white',\n              border: 'none',\n              borderRadius: '50%',\n              width: '60px',\n              height: '60px',\n              display: 'flex',\n              alignItems: 'center',\n              justifyContent: 'center',\n              cursor: 'pointer',\n              boxShadow: '0 4px 12px rgba(0,0,0,0.3)',\n              zIndex: 10,\n              transition: 'transform 0.2s ease'\n            }}\n            onMouseOver={(e) => e.currentTarget.style.transform = 'scale(1.1)'}\n            onMouseOut={(e) => e.currentTarget.style.transform = 'scale(1)'}\n          >\n            Chat\n          </button>\n        )}\n      </div>\n\n      {/* Controls Bar at the bottom */}\n      <div className=\"controls-bar\" style={{\n        display: 'flex',\n        justifyContent: 'center',\n        alignItems: 'center',\n        padding: '10px',\n        background: '#20232a',\n        flexShrink: 0,\n        gap: '10px'\n      }}>\n        <button onClick={toggleMic} style={buttonStyle} title={state.isMicMuted ? \"Unmute Microphone\" : \"Mute Microphone\"}>\n          {state.isMicMuted ? <FaMicrophoneSlash /> : <FaMicrophone />} {state.isMicMuted ? \"Mic Off\" : \"Mic On\"}\n        </button>\n        <button onClick={toggleSound} style={buttonStyle} title={state.isSoundOn ? \"Mute Sound\" : \"Unmute Sound\"}>\n          {state.isSoundOn ? <FaVolumeUp /> : <FaVolumeMute />} {state.isSoundOn ? \"Sound On\" : \"Sound Off\"}\n        </button>\n        <button onClick={forceStopAllAudio} style={{...buttonStyle, background: '#dc3545'}} title=\"Force stop all audio and speech\">\n          <FaStopCircle /> Stop All Audio\n        </button>\n      </div>\n    </div>\n  );\n});\n\nexport const SimulationView: React.FC<SimulationViewProps> = (props) => {\n  const [humeAudioIndicatorMessage, setHumeAudioIndicatorMessage] = useState<string>('Hume Audio: Idle');\n  const internalCompRef = useRef<SimulationViewInternalHandle>(null);\n  const humeApiKey = React.useMemo(() => {\n    const key = process.env.REACT_APP_HUME_API_KEY;\n    if (!key) {\n      console.error(\"CRITICAL: REACT_APP_HUME_API_KEY is not set. Hume EVI will not work.\");\n    }\n    return key || '';\n  }, []);\n\n  const humeConfigId = process.env.REACT_APP_HUME_CONFIG_ID || '405fe2ff-0cf5-4ff9-abf9-fc09f4625ed8';\n\n  const {\n    messages: humeMessages,\n    sendUserInput: sendUserInputToVoice,\n    readyState: voiceReadyState,\n    isPlaying: isHumeVoicePlaying,\n    disconnect: disconnectVoice,\n    connect: connectVoice,\n    status,\n  } = useVoice();\n\n  console.log(\n    '[SimulationView useVoice] isHumeVoicePlaying:', isHumeVoicePlaying, \n    'status:', status\n  );\n\n  const handleVoiceOpen = useCallback(() => {\n    console.log('[Hume EVI] Connection opened via VoiceProvider.');\n  }, []);\n\n  const voiceAuth = useMemo(() => ({\n    type: 'apiKey' as const,\n    value: humeApiKey,\n  }), [humeApiKey]);\n\n  const handleVoiceMessage = useCallback((message: JSONMessage) => {\n    const messageType = message.type;\n    // Log the type of every message to give an overview without flooding with full objects\n    console.log(`[Hume EVI] Message received - Type: ${messageType}`);\n\n    switch (messageType) {\n      case 'assistant_message':\n        console.log('[Hume EVI] Assistant Message details:', message); // Log the full assistant_message object\n        if (message.models) {\n          const modelsAsAny = message.models as any; // Cast to any for easier access to dynamic model structure\n          if (modelsAsAny.face && modelsAsAny.face.predictions) {\n            // Log only if face predictions are present\n            console.log('[Hume EVI] Face model predictions:', modelsAsAny.face.predictions);\n          }\n          // Optional: Log if models exist but face/predictions are missing for debugging model structure\n          // else {\n          //   console.log('[Hume EVI] message.models.face or .predictions not found. message.models structure:', message.models);\n          // }\n        }\n        break;\n      case 'user_interruption':\n        console.log('[Hume EVI] User Interruption details:', message);\n        break;\n      case 'tool_call':\n        console.log('[Hume EVI] Tool Call details:', message);\n        break;\n      case 'tool_response':\n        console.log('[Hume EVI] Tool Response details:', message);\n        break;\n      case 'tool_error':\n        console.error('[Hume EVI] Tool Error details:', message);\n        break;\n      // Add cases for other specific message types if they become important for debugging\n      default:\n        // For any other message types, only their type is logged by the initial console.log.\n        // This significantly reduces noise from less critical or very frequent messages.\n        // If deeper debugging for other types is needed, uncomment the line below:\n        // console.log(`[Hume EVI] Other message type '${messageType}', content:`, message);\n        break;\n    }\n  }, []);\n\n  const handleVoiceClose = useCallback((e: any) => {\n    console.log('[Hume EVI] Connection closed via VoiceProvider. Code:', e.code, 'Reason:', e.reason, 'WasClean:', e.wasClean);\n    internalCompRef.current?.forceStopHumeSpeaking();\n  }, []);\n\n  const handleVoiceError = useCallback((error: any) => {\n    let logMessage = `[Hume EVI] VoiceProvider error - Type: ${error.type}, Code: ${error.code}, Message: ${error.message}`;\n    console.error(logMessage, error);\n  }, []);\n\n  if (!humeApiKey) {\n     console.warn(\"[SimulationView] Hume API Key is not available. VoiceProvider will likely fail to connect.\");\n     // Optionally, render a message to the user or a disabled state for the component\n     // return <div>Hume API Key is missing. Please configure REACT_APP_HUME_API_KEY in your environment.</div>;\n  }\n\n  console.log(`[SimulationView] Attempting to connect VoiceProvider with API Key: \"${humeApiKey}\" and Config ID: \"${humeConfigId}\"`);\n  console.log(`[SimulationView] voiceAuth object being passed:`, voiceAuth);\n\n  return (\n    <>\n      <div style={{ padding: '8px', backgroundColor: '#ff69b4', color: 'white', textAlign: 'center', fontWeight: 'bold', fontSize: '1.1em', zIndex: 9999, position: 'relative' }}>\n        HUME AUDIO EVENT STATUS: {humeAudioIndicatorMessage}\n      </div>\n      <VoiceProvider\n      onAudioStart={() => {\n        console.log('[Hume EVI VoiceProvider] onAudioStart triggered');\n        setHumeAudioIndicatorMessage('Hume Audio: STARTED');\n      }}\n      onAudioEnd={() => {\n        console.log('[Hume EVI VoiceProvider] onAudioEnd triggered');\n        setHumeAudioIndicatorMessage('Hume Audio: ENDED');\n      }}\n      auth={voiceAuth}\n      configId={humeConfigId}\n      debug={true}\n      onOpen={handleVoiceOpen}\n      onMessage={handleVoiceMessage}\n      onClose={handleVoiceClose}\n      onError={handleVoiceError}\n    >\n      <SimulationViewInternal\n          ref={internalCompRef}\n          simulationId={props.simulationId} // Pass down from parent props\n          avatarModelUrl={props.avatarModelUrl} // Pass down from parent props\n          humeMessages={humeMessages}\n          sendUserInputToVoice={sendUserInputToVoice}\n          voiceReadyState={voiceReadyState}\n          isHumeVoicePlaying={isHumeVoicePlaying}\n          disconnectVoice={disconnectVoice}\n          connectVoice={connectVoice}\n          // lastHumeVoiceMessage is removed as it's not provided by useVoice()\n          humeConfigId={humeConfigId} // Ensure humeConfigId is available in this scope\n        />\n    </VoiceProvider>\n    </>\n  );\n};\n","import { ARKitBlendshapeNamesList, type BlendshapeKey, type BlendShapeMap } from '../types/blendshapes';\n\nconst PROSODY_BLENDSHAPE_AMPLIFICATION_FACTOR = 7.0; // Increased for more dramatic expressions\n\n// Map each prosody emotion to blendshape contributions (values are multipliers for score)\nconst PROSODY_EMOTION_TO_BLENDSHAPE: Record<string, Partial<BlendShapeMap>> = {\n  joy: {\n    mouthSmileLeft: 1.2,\n    mouthSmileRight: 1.2,\n    cheekSquintLeft: 0.8,\n    cheekSquintRight: 0.8,\n    eyeWideLeft: 0.3,\n    eyeWideRight: 0.3,\n    browOuterUpLeft: 1.0, // Strong eyebrow raise for joy\n    browOuterUpRight: 1.0,\n    cheekPuff: 0.2, // Slight cheek puff for happiness\n  },\n  sadness: {\n    mouthFrownLeft: 1.2,\n    mouthFrownRight: 1.2,\n    browInnerUp: 1.0, // Strong inner brow raise (sad puppy eyes)\n    browDownLeft: 0.6,\n    browDownRight: 0.6,\n    eyeLookDownLeft: 0.4,\n    eyeLookDownRight: 0.4,\n    mouthLowerDownLeft: 0.5, // Droopy mouth corners\n    mouthLowerDownRight: 0.5,\n  },\n  anger: {\n    browDownLeft: 1.2, // Very strong brow furrow\n    browDownRight: 1.2,\n    browInnerUp: 0.3, // Slight inner brow tension\n    mouthPressLeft: 0.8,\n    mouthPressRight: 0.8,\n    eyeSquintLeft: 0.6,\n    eyeSquintRight: 0.6,\n    noseSneerLeft: 0.4, // Nostril flare for anger\n    noseSneerRight: 0.4,\n    mouthUpperUpLeft: 0.3, // Slight snarl\n    mouthUpperUpRight: 0.3,\n  },\n  surprise: {\n    jawOpen: 0.9,\n    eyeWideLeft: 1.2, // Very wide eyes\n    eyeWideRight: 1.2,\n    browOuterUpLeft: 1.2, // Dramatic eyebrow raise\n    browOuterUpRight: 1.2,\n    browInnerUp: 0.8, // Full brow raise\n    mouthFunnel: 0.3, // Slight \"O\" mouth shape\n  },\n  fear: {\n    mouthStretchLeft: 0.9,\n    mouthStretchRight: 0.9,\n    eyeWideLeft: 1.0,\n    eyeWideRight: 1.0,\n    browInnerUp: 1.0, // Strong worried brow\n    browOuterUpLeft: 0.7,\n    browOuterUpRight: 0.7,\n    jawOpen: 0.4, // Slight jaw drop in fear\n  },\n  disgust: {\n    noseSneerLeft: 0.8,\n    noseSneerRight: 0.8,\n    mouthUpperUpLeft: 0.7, // Upper lip curl\n    mouthUpperUpRight: 0.7,\n    eyeSquintLeft: 0.5,\n    eyeSquintRight: 0.5,\n    browDownLeft: 0.4,\n    browDownRight: 0.4,\n  },\n  contempt: {\n    mouthSmileLeft: 0.2, // Asymmetric smirk\n    mouthSmileRight: 0.7,\n    eyeSquintLeft: 0.3,\n    eyeSquintRight: 0.6,\n    browDownLeft: 0.2,\n    browDownRight: 0.5,\n  },\n  excitement: {\n    mouthSmileLeft: 1.0,\n    mouthSmileRight: 1.0,\n    eyeWideLeft: 0.8,\n    eyeWideRight: 0.8,\n    browOuterUpLeft: 0.9,\n    browOuterUpRight: 0.9,\n    cheekSquintLeft: 0.6,\n    cheekSquintRight: 0.6,\n    jawOpen: 0.3, // Slight excitement jaw drop\n  },\n  contentment: {\n    mouthSmileLeft: 0.4,\n    mouthSmileRight: 0.4,\n    eyeSquintLeft: 0.2, // Gentle squint for contentment\n    eyeSquintRight: 0.2,\n    browOuterUpLeft: 0.2,\n    browOuterUpRight: 0.2,\n  },\n  confusion: {\n    browDownLeft: 0.6,\n    browOuterUpRight: 0.8, // Asymmetric brow for confusion\n    browInnerUp: 0.4,\n    mouthPucker: 0.4,\n    eyeSquintLeft: 0.3,\n    jawOpen: 0.2,\n  },\n  calm: {\n    mouthSmileLeft: 0.15,\n    mouthSmileRight: 0.15,\n    eyeSquintLeft: 0.1, // Very subtle relaxed expression\n    eyeSquintRight: 0.1,\n  },\n  // Add more emotions that might come from Hume\n  amusement: {\n    mouthSmileLeft: 0.8,\n    mouthSmileRight: 0.8,\n    cheekSquintLeft: 0.7,\n    cheekSquintRight: 0.7,\n    browOuterUpLeft: 0.5,\n    browOuterUpRight: 0.5,\n  },\n  concentration: {\n    browDownLeft: 0.5,\n    browDownRight: 0.5,\n    browInnerUp: 0.3,\n    eyeSquintLeft: 0.4,\n    eyeSquintRight: 0.4,\n  },\n};\n\nexport const prosodyToBlendshapes = (prosodyScores: Record<string, number>): BlendShapeMap => {\n  console.log('[prosodyToBlendshapes] Received prosodyScore keys:', Object.keys(prosodyScores).join(', '));\n\n  // Start with all zeros  // Initialize blendshapes to 0\n  const blendshapes: BlendShapeMap = ARKitBlendshapeNamesList.reduce((acc: BlendShapeMap, shapeName: BlendshapeKey) => {\n    acc[shapeName] = 0;\n    return acc;\n  }, {} as BlendShapeMap);\n\n  console.log('[prosodyToBlendshapes] Processing emotions...');\n  for (const [emotion, score] of Object.entries(prosodyScores)) {\n    const weights = PROSODY_EMOTION_TO_BLENDSHAPE[emotion.toLowerCase()];\n    if (!weights) {\n      console.log(`[prosodyToBlendshapes] No weights found for emotion: ${emotion}. Skipping.`);\n      continue;\n    }\n    console.log(`[prosodyToBlendshapes] Processing emotion: ${emotion}, Score: ${score.toFixed(4)}`);\n    for (const [blend, weight] of Object.entries(weights)) {\n      const shapeKey = blend as BlendshapeKey;\n      const currentVal = blendshapes[shapeKey] || 0;\n      const addition = score * (weight as number) * PROSODY_BLENDSHAPE_AMPLIFICATION_FACTOR;\n      blendshapes[shapeKey] = Math.min(1, Math.max(0, currentVal + addition));\n      // Optional detailed logging for each blendshape change:\n      // console.log(`[prosodyToBlendshapes]   - Blendshape: ${shapeKey}, Weight: ${weight}, Addition: ${addition.toFixed(4)}, New Value: ${blendshapes[shapeKey].toFixed(4)}`);\n    }\n  }\n  console.log(`[prosodyToBlendshapes] Final combined blendshapes count: ${Object.keys(blendshapes).length}, sum of values: ${Object.values(blendshapes).reduce((s, v) => s + v, 0).toFixed(4)}`);\n  return blendshapes;\n}\n"],"names":["TestModeToggle","_ref","onMockMessage","onMockProsody","onMockVisemes","isTestMode","setIsTestMode","useState","isExpanded","setIsExpanded","mockEVI","setMockEVI","handleTestModeToggle","useCallback","console","log","stop","evi","MockHumeEVI","message","prosody","timeline","visemes","reduce","acc","item","type","value","jawOpen","mouthFunnel","mouthSmileLeft","mouthSmileRight","Math","max","start","handleTestEmotion","emotionSetKey","emotionSet","mockEmotionSets","concat","_jsxs","style","position","top","left","background","color","padding","borderRadius","zIndex","fontSize","width","maxHeight","overflowY","transition","border","children","display","alignItems","justifyContent","marginBottom","_jsx","margin","onClick","cursor","fontWeight","gridTemplateColumns","gap","Object","keys","map","emotionKey","textTransform","onMouseEnter","e","currentTarget","onMouseLeave","getEmotionEmoji","opacity","lineHeight","borderTop","paddingTop","marginTop","emotion","happy","sad","angry","surprised","scared","disgusted","confused","content","HUME_SAPI_TO_IPA","IPA_TO_AZURE_VISEME_ID_MAP","ipaSymbols","id","convertHumeTimelineToAzureVisemes","humeTimeline","audioDurationSeconds","visemeEvents","audioDurationMs","round","length","forEach","humeEvent","index","humePhonemeKey","phoneme","toUpperCase","visemeId","ipaSymbol","warn","entry","includes","getAzureVisemeIdFromIpa","undefined","audioOffsetTicks","time","push","audioOffset","isLastViseme","sort","a","b","speechConfig","synthesizeSpeechWithVisemes","async","text","voiceName","arguments","Date","now","Promise","resolve","reject","speechKey","process","speechRegion","SpeechSDK","fromSubscription","speechSynthesisOutputFormat","Audio16Khz32KBitRateMonoMp3","ssml","synthesizer","collectedVisemeData","standardVisemes","blendShapeFrames","promiseHandled","cleanup","close","closeError","error","visemeReceived","s","animation","trim","substring","animationData","JSON","parse","BlendShapes","FrameIndex","baseFrameIndex","shapeFrame","frameIdxInChunk","frameIndex","shapes","FrameRate","visemeID","speakSsmlAsync","result","reason","SynthesizingAudioCompleted","audioDuration","audioData","visemeData","Canceled","cancellation","fromResult","reasonText","errorDetails","ErrorCode","Error","initialEmptyBlendshapes","ARKitBlendshapeNamesList","shapeName","AZURE_VISEME_ID_TO_STATIC_BLENDSHAPES","mouthClose","mouthPucker","mouthShrugUpper","tongueUp","mouthLowerDownLeft","mouthLowerDownRight","mouthPressLeft","mouthPressRight","mouthShrugLower","defaultBlendShapes","key","idleBlendShapes","ALL_TALK_ANIMATION_GLBS","ALL_IDLE_ANIMATION_GLBS","buttonStyle","SimulationViewInternal","React","props","ref","humeMessages","sendUserInputToVoice","voiceReadyState","isHumeVoicePlaying","disconnectVoice","connectVoice","humeConfigId","VoiceReadyState","String","currentUser","useAuth","user","loading","userLoading","userError","useUser","simulationId","useNavigate","useParams","state","setState","avatarModelUrl","avatarUrl","initial","isMicOn","isCameraOn","isSoundOn","isChatOpen","messages","inputValue","isSpeaking","isAzureAudioActive","isHumeAudioActive","currentEmotion","statusMessage","humeVoiceName","azureVoiceName","currentVisemeShapes","manualBlendshapes","simulationData","isSending","isCameraEnabled","isMicMuted","showChat","prosodyDrivenBlendshapes","stateRef","useRef","handleEmotionData","prev","_objectSpread","name","toLowerCase","humeApiKey","sendVideoFrame","sendEmotionVideoFrame","connectionState","humeEmotionStreamConnectionState","lastError","humeEmotionStreamLastError","useHumeEmotionStream","isEmotionDetectionActive","isVideoOn","useEffect","azureAudioRef","humeAudioRef","visemeFramesRef","animationFrameIdRef","speakingDebounceTimerRef","lastProcessedUserMessageReceivedAtRef","lastProcessedAssistantMessageIdRef","isAzurePlayingRef","isHumeVoicePlayingRef","current","handleAvatarErrorCb","handleAvatarLoadCb","messagesEndRef","avatarGroupRef","customSessionIdRef","uuidv4","stopVisemeAnimation","cancelAnimationFrame","prepareVisemeFrames","frames","frame","humeVisemeEventsToAnimationKeyframes","keyframes","event","lastKeyframeTimeSeconds","animateVisemes","startTime","currentTime","currentFrames","activeFrame","i","requestAnimationFrame","startVisemeAnimation","handleHumeSpeakingStarted","audioUrl","pause","src","Audio","addEventListener","load","play","duration","isNaN","converted","handleHumeSpeakingStopped","handleAzureAudioEnded","startsWith","URL","revokeObjectURL","srcObject","handleAzureAudioError","audioElement","target","generateAndAnimateVisemesFromText","_serviceResult$viseme","paused","serviceResult","untypedSynthesizeSpeechWithVisemes","stringify","min","_serviceResult$viseme2","slice","lastFrameIdx","_serviceResult$viseme3","preparedFrames","audioBlob","Blob","createObjectURL","catch","processAndPlayAzureSpeech","_serviceResult$audioD","_serviceResult$viseme4","_serviceResult$viseme5","_serviceResult$viseme6","_serviceResult$viseme7","resolvedVoiceName","audioDataLength","byteLength","blendShapeFramesLength","standardVisemesLength","_serviceResult$viseme8","_serviceResult$viseme9","then","currentMessage","receivedAt","models","role","newUserChatMessage","sender","timestamp","_currentMessage$model","_currentMessage$model2","_currentMessage$model3","_currentMessage$model4","scores","newAssistantChatMessage","newEmotions","score","emotionsSource","Array","isArray","potentialEmotions","filter","prosodyScoresForBlendshapes","newProsodyBlendshapes","prosodyToBlendshapes","currentDetectedEmotions","clearTimeout","setTimeout","handleInputChange","handleSendMessage","preventDefault","newUserMessage","inputText","toggleMic","emotionData","errorMessage","IDLE","CONNECTING","OPEN","CLOSED","getVoiceReadyStateName","newMicMutedState","humeAccessToken","toggleChat","toggleSound","newIsSoundOn","muted","forceStopAllAudio","useImperativeHandle","forceStopHumeSpeaking","prevState","derivedIsSpeaking","derivedStatusMessage","className","height","flexDirection","textAlign","flexShrink","prosodyRecord","prosodyBlendshapes","mockVisemes","emotions","flexGrow","overflow","EmotionDrivenAvatar","directBlendshapes","idleShapes","detectedEmotions","cameraEnabled","talkAnimationPaths","idleAnimationPaths","onError","onLoad","minWidth","borderLeft","boxSizing","right","bottom","transform","paddingRight","msg","maxWidth","alignSelf","marginLeft","marginRight","wordWrap","onEnded","onSubmit","onChange","placeholder","outline","disabled","boxShadow","onMouseOver","onMouseOut","title","FaMicrophoneSlash","FaMicrophone","FaVolumeUp","FaVolumeMute","FaStopCircle","SimulationView","humeAudioIndicatorMessage","setHumeAudioIndicatorMessage","internalCompRef","sendUserInput","readyState","isPlaying","disconnect","connect","status","useVoice","handleVoiceOpen","voiceAuth","useMemo","handleVoiceMessage","messageType","modelsAsAny","face","predictions","handleVoiceClose","_internalCompRef$curr","code","wasClean","handleVoiceError","logMessage","_Fragment","backgroundColor","VoiceProvider","onAudioStart","onAudioEnd","auth","configId","debug","onOpen","onMessage","onClose","PROSODY_EMOTION_TO_BLENDSHAPE","joy","cheekSquintLeft","cheekSquintRight","eyeWideLeft","eyeWideRight","browOuterUpLeft","browOuterUpRight","cheekPuff","sadness","mouthFrownLeft","mouthFrownRight","browInnerUp","browDownLeft","browDownRight","eyeLookDownLeft","eyeLookDownRight","anger","eyeSquintLeft","eyeSquintRight","noseSneerLeft","noseSneerRight","mouthUpperUpLeft","mouthUpperUpRight","surprise","fear","mouthStretchLeft","mouthStretchRight","disgust","contempt","excitement","contentment","confusion","calm","amusement","concentration","prosodyScores","join","blendshapes","entries","weights","toFixed","blend","weight","shapeKey","currentVal","addition","values","v"],"sourceRoot":""}