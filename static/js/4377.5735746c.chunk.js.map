{"version":3,"file":"static/js/4377.5735746c.chunk.js","mappings":"2HA2BO,MAAMA,EAAuBA,CAClCC,EACAC,EACAC,KAKA,MAAMC,GAAQC,EAAAA,EAAAA,QAAyB,MACjCC,GAAYD,EAAAA,EAAAA,SAAO,GACnBE,GAAoBF,EAAAA,EAAAA,QAAO,GAC3BG,GAAmBH,EAAAA,EAAAA,WAEnB,yBAAEI,GAA2B,EAAK,UAAEC,GAAY,GAAUP,GAAU,CAAC,GACpEQ,EAAiBC,IAAsBC,EAAAA,EAAAA,UAA0B,iBACjEC,EAAWC,IAAgBF,EAAAA,EAAAA,UAAwB,OAG1DG,EAAAA,EAAAA,YAAU,KACRV,EAAUW,SAAU,EACb,KACLX,EAAUW,SAAU,EAEhBT,EAAiBS,UACnBC,aAAaV,EAAiBS,SAC9BT,EAAiBS,aAAUE,MAG9B,KAEHH,EAAAA,EAAAA,YAAU,KAER,IAAKf,IAAWQ,IAA6BC,EAkB3C,OAjBIN,EAAMa,UACRG,QAAQC,IAAI,mEAEZjB,EAAMa,QAAQK,OAAS,KACvBlB,EAAMa,QAAQM,UAAY,KAC1BnB,EAAMa,QAAQO,QAAU,KACxBpB,EAAMa,QAAQQ,QAAU,KACxBrB,EAAMa,QAAQS,QACdtB,EAAMa,QAAU,KAEhBV,EAAkBU,QAAU,EACxBT,EAAiBS,UACjBC,aAAaV,EAAiBS,SAC9BT,EAAiBS,aAAUE,SAGjCP,EAAmB,gBAOrB,GAAIR,EAAMa,UAAYb,EAAMa,QAAQU,aAAeC,UAAUC,MAAQzB,EAAMa,QAAQU,aAAeC,UAAUE,YAE1G,YADAV,QAAQC,IAAI,yEAIdD,QAAQC,IAAI,+EACZT,EAAmB,cACnBG,EAAa,MAEb,MAAMgB,EAAK,6CAAAC,OAAgDC,mBAAmBhC,IACxEiC,EAAK,IAAIN,UAAUG,GACzBG,EAAGC,WAAa,cAChB/B,EAAMa,QAAUiB,EAEhBd,QAAQC,IAAI,iDAAkDU,GAE9DG,EAAGZ,OAAS,KACV,IAAKhB,EAAUW,SAAWb,EAAMa,UAAYiB,EAAI,OAChDd,QAAQC,IAAI,sCACZd,EAAkBU,QAAU,EAC5BL,EAAmB,aACnBG,EAAa,MAEb,MACMqB,EAAiB,CACrBC,OAAQ,CAAEC,KAAM,CAAC,EAAGC,QAAS,CAAC,GAC9BC,KAHyB,YAK3B,IACEpB,QAAQC,IAAI,yCAA0CoB,KAAKC,UAAUN,EAAe,KAAK,IACzFF,EAAGS,KAAKF,KAAKC,UAAUN,GACzB,CAAE,MAAOQ,GACPxB,QAAQyB,MAAM,+CAAgDD,GAC9D7B,EAAa6B,aAAeE,MAAQF,EAAIG,QAAUC,OAAOJ,GAE3D,GAGFV,EAAGX,UAAa0B,IACd,GAAK3C,EAAUW,SAAWb,EAAMa,UAAYiB,EAC5C,IAAK,IAADgB,EAAAC,EAAAC,EACF,MAAMZ,EAAOC,KAAKY,MAAMJ,EAAMT,MAE9B,GAAIA,EAAKK,MAIP,OAHAzB,QAAQyB,MAAM,gCAAiCL,EAAKK,OACpDjC,EAAmB,cACnBG,EAAa,mBAADiB,OAAoBQ,EAAKK,MAAME,SAAWP,EAAKK,QAI7D,GAAoB,QAApBK,EAAIV,EAAKc,mBAAW,IAAAJ,GAAK,QAALC,EAAhBD,EAAmB,UAAE,IAAAC,GAAU,QAAVC,EAArBD,EAAuBI,gBAAQ,IAAAH,GAA/BA,EAAiCI,OAAQ,CAC3C,MACMC,EADWjB,EAAKc,YAAY,GAAGC,SACTG,QAAO,CAACC,EAAOC,IAAWD,EAAEE,MAAQD,EAAEC,MAAQF,EAAIC,GAAI,CAAEE,KAAM,UAAWD,MAAO,IAC5G3D,EAAc,CACZ4D,KAAML,EAAWK,KAAKC,cACtBF,MAAOJ,EAAWI,OAEtB,CACF,CAAE,MAAOhB,GACPzB,QAAQyB,MAAM,0CAA2CA,GACzD9B,EAAa8B,aAAiBC,MAAQD,EAAME,QAAU,gBACxD,GAkFF,OAlCAb,EAAGV,QAAWyB,IACP3C,EAAUW,SAAWb,EAAMa,UAAYiB,IAC5Cd,QAAQyB,MAAM,iCAAkCI,GAChDlC,EAAa,+BAKfmB,EAAGT,QAAWwB,IACP3C,EAAUW,SAAWb,EAAMa,UAAYiB,IAC5Cd,QAAQC,IAAI,yCAADW,OAA0CiB,EAAMe,KAAI,eAAAhC,OAAciB,EAAMgB,OAAM,cAAAjC,OAAaiB,EAAMiB,WAC5G9D,EAAMa,QAAU,KAEG,MAAfgC,EAAMe,MAAgC,OAAff,EAAMe,MAC/B5C,QAAQC,IAAI,6DACZT,EAAmB,gBACnBL,EAAkBU,QAAU,EACxBT,EAAiBS,UACjBC,aAAaV,EAAiBS,SAC9BT,EAAiBS,aAAUE,IAEtBb,EAAUW,SAAWR,GAA4BC,GAE1DU,QAAQC,IAAI,oEACZN,EAAa,gCAADiB,OAAiCiB,EAAMe,KAAI,KAAAhC,OAAIiB,EAAMgB,SArE5CE,MACrB,IAAK7D,EAAUW,UAAYR,IAA6BC,GAAaH,EAAkBU,SAAW,EAM9F,YALIV,EAAkBU,SAAU,IAC5BG,QAAQyB,MAAM,oDACdjC,EAAmB,SACnBG,EAAa,wCAKrBR,EAAkBU,UAClB,MAAMmD,EAAQC,KAAKC,IAAI,IAAOD,KAAKE,IAAI,EAAGhE,EAAkBU,QAAS,GAAK,KAC1EG,QAAQC,IAAI,0CAADW,OAA2CzB,EAAkBU,QAAO,WAAAe,OAAUoC,EAAK,UAC9FxD,EAAmB,gBAGfJ,EAAiBS,SACjBC,aAAaV,EAAiBS,SAGlCT,EAAiBS,QAAUuD,OAAOC,YAAW,KACrCnE,EAAUW,SAAWR,GAA4BC,IAOjDU,QAAQC,IAAI,uEAMTjB,EAAMa,SAAWb,EAAMa,QAAQU,aAAeC,UAAUC,OACzDzB,EAAMa,QAAU,KAGhBL,EAAmB,kBAI1BwD,IA4BHD,KAGAvD,EAAmB,gBACnBL,EAAkBU,QAAU,KAKzB,KACLG,QAAQC,IAAI,uDAERb,EAAiBS,UACnBC,aAAaV,EAAiBS,SAC9BT,EAAiBS,aAAUE,GAEzBf,EAAMa,UAERb,EAAMa,QAAQK,OAAS,KACvBlB,EAAMa,QAAQM,UAAY,KAC1BnB,EAAMa,QAAQO,QAAU,KACxBpB,EAAMa,QAAQQ,QAAU,KACpBrB,EAAMa,QAAQU,aAAeC,UAAUC,MAAQzB,EAAMa,QAAQU,aAAeC,UAAUE,YACtF1B,EAAMa,QAAQS,MAAM,KAExBtB,EAAMa,QAAU,MAIlBL,EAAmB,gBACnBL,EAAkBU,QAAU,KAK7B,CAAChB,EAAQQ,EAA0BC,EAAWR,IAoBjD,MAAO,CAAEwE,gBAlBcC,EAAAA,EAAAA,cAAYC,UACjC,IAAKxE,EAAMa,SAAWb,EAAMa,QAAQU,aAAeC,UAAUC,KAE3D,OAEF,MACMkB,EAAU,CACdV,OAAQ,CAAEC,KAAM,CAAC,EAAGC,QAAS,CAAC,GAC9BC,KArPN,SAA6BqC,GAC3B,IAAIC,EAAS,GACb,MAAMC,EAAQ,IAAIC,WAAWH,GACvBI,EAAMF,EAAMG,WAClB,IAAK,IAAIC,EAAI,EAAGA,EAAIF,EAAKE,IACvBL,GAAU9B,OAAOoC,aAAaL,EAAMI,IAEtC,OAAOX,OAAOa,KAAKP,EACrB,CA0OuBQ,CAAoBC,aAAiBC,WAAaD,EAAME,cAAgBF,IAK3F,IACEnF,EAAMa,QAAQ0B,KAAKF,KAAKC,UAAUK,GACpC,CAAE,MAAOH,GACPxB,QAAQyB,MAAM,2CAA4CD,GAC1D7B,EAAa6B,aAAeE,MAAQF,EAAIG,QAAUC,OAAOJ,GAC3D,IACC,IAEsBjC,kBAAiBG,a,4ICrQ5C,MAAM4E,EAAoE,CAExEC,IAAK,CAAEC,eAAgB,GAAKC,gBAAiB,GAAKC,gBAAiB,GAAKC,iBAAkB,GAAKC,cAAe,GAAKC,eAAgB,IACnIC,UAAW,CAAEN,eAAgB,GAAKC,gBAAiB,GAAKC,gBAAiB,GAAKC,iBAAkB,IAChGI,WAAY,CAAEC,YAAa,GAAKC,aAAc,GAAKC,UAAW,GAAKC,QAAS,GAAKC,YAAa,IAC9FC,IAAK,CAAEH,UAAW,GAAKC,QAAS,GAAKH,YAAa,GAAKC,aAAc,GAAKG,YAAa,IACvFE,WAAY,CAAEd,eAAgB,GAAKC,gBAAiB,GAAKc,cAAe,GAAKC,eAAgB,GAAKJ,YAAa,IAC/GK,KAAM,CAAEjB,eAAgB,GAAKC,gBAAiB,GAAKG,cAAe,GAAKC,eAAgB,GAAKa,gBAAiB,GAAKC,iBAAkB,IAGpIC,SAAU,CAAEhB,cAAe,GAAKC,eAAgB,GAAKgB,WAAY,IACjEC,YAAa,CAAEtB,eAAgB,GAAKC,gBAAiB,IACrDsB,SAAU,CAAEX,YAAa,GAAKJ,YAAa,GAAKC,aAAc,GAAKE,QAAS,KAC5Ea,SAAU,CAAEpB,cAAe,IAAMC,eAAgB,KAGjDoB,MAAO,CAAEC,aAAc,GAAKC,cAAe,GAAKC,cAAe,GAAKC,eAAgB,GAAKC,gBAAiB,GAAK1B,cAAe,GAAKC,eAAgB,IACnJ0B,KAAM,CAAEvB,YAAa,GAAKC,aAAc,GAAKC,UAAW,GAAKC,QAAS,GAAKC,YAAa,GAAKoB,iBAAkB,GAAKC,kBAAmB,IACvIC,OAAQ,CAAE1B,YAAa,EAAKC,aAAc,EAAKC,UAAW,GAAKC,QAAS,GAAKC,YAAa,GAAKuB,gBAAiB,IAChHC,SAAU,CAAEC,eAAgB,GAAKC,gBAAiB,GAAK1B,YAAa,GAAKR,cAAe,GAAKC,eAAgB,IAG7GkC,QAAS,CAAEF,eAAgB,GAAKC,gBAAiB,GAAK1B,YAAa,GAAKc,aAAc,GAAKC,cAAe,GAAKa,gBAAiB,GAAKC,iBAAkB,IACvJC,eAAgB,CAAEL,eAAgB,GAAKC,gBAAiB,GAAKZ,aAAc,GAAKC,cAAe,IAC/FgB,UAAW,CAAEvC,cAAe,GAAKC,eAAgB,GAAKqB,aAAc,GAAKC,cAAe,GAAKhB,QAAS,GAAKD,UAAW,GAAK8B,gBAAiB,GAAKC,iBAAkB,IACnKG,QAAS,CAAEJ,gBAAiB,GAAKC,iBAAkB,GAAKN,gBAAiB,GAAKxB,QAAS,KACvFkC,MAAO,CAAEnB,aAAc,GAAKC,cAAe,GAAKa,gBAAiB,GAAKC,iBAAkB,GAAKK,eAAgB,GAAKC,gBAAiB,IAGnIC,SAAU,CAAErC,QAAS,GAAKH,YAAa,GAAKC,aAAc,GAAKG,YAAa,GAAKF,UAAW,IAC5FuC,QAAS,CAAErB,cAAe,GAAKC,eAAgB,GAAKqB,iBAAkB,GAAKC,kBAAmB,GAAKzB,aAAc,GAAKC,cAAe,IACrIyB,SAAU,CAAEpD,eAAgB,GAAK6B,eAAgB,IACjDwB,cAAe,CAAEC,UAAW,GAAKd,gBAAiB,GAAKC,iBAAkB,GAAKK,eAAgB,GAAKC,gBAAiB,IACpHQ,UAAW,CAAE7B,aAAc,GAAKP,iBAAkB,GAAKqC,YAAa,IAEpEC,QAAS,CAGP,GA0BG,SAASC,EAAc/F,GAC5B,OAAKA,GAAgC,IAApBA,EAASC,OAGnBD,EAASG,QACd,CAAC6F,EAAKtI,IAAaA,EAAQ4C,QAAY,OAAH0F,QAAG,IAAHA,OAAG,EAAHA,EAAK1F,QAAS,GAAK5C,EAAUsI,GACjEhG,EAAS,IAJF,IAMX,CAGO,SAASiG,EAAyBjG,GACvC,MAAMkG,EAAmB,CAAC,EAMc,CAAC,eAAgB,gBAAiB,cAAe,kBAAmB,mBAAoB,YAAa,kBAAmB,mBAAoB,eAAgB,gBAAiB,kBAAmB,mBAAoB,gBAAiB,iBAAkB,iBAAkB,kBAAmB,gBAAiB,iBAAkB,gBAAiB,iBAAkB,cAAe,eAAgB,aAAc,UAAW,UAAW,WAAY,aAAc,kBAAmB,mBAAoB,iBAAkB,kBAAmB,cAAe,YAAa,qBAAsB,sBAAuB,iBAAkB,kBAAmB,cAAe,aAAc,iBAAkB,iBAAkB,kBAAmB,kBAAmB,iBAAkB,kBAAmB,mBAAoB,oBAAqB,mBAAoB,oBAAqB,gBAAiB,iBAAkB,aACn5BC,SAAQC,GAASF,EAAiBE,GAAS,IAG1D,MAAMlG,EAAa6F,EAAc/F,GAEjC,GAAIE,EAAY,CAEd,MAAMmG,EA9CV,SAAwCC,GACtC,IAAKA,IAAYA,EAAQ/F,KAAM,MAAO,CAAC,EAGvC,MAAMgG,EAAapE,EAA0BmE,EAAQ/F,MACrD,IAAKgG,EAEH,MAAO,CAAC,EAGV,MAAMC,EAAYF,EAAQhG,MAE1B,OAAOmG,OAAOC,YACZD,OAAOE,QAAQJ,GAAYK,KAAIC,IAAA,IAAEC,EAAKC,GAAMF,EAAA,MAAK,CAC/CC,EACCC,EAAmBP,MAG1B,CA4BkCQ,CAA+B9G,GAE7D,IAAK,MAAO4G,EAAKC,KAAUN,OAAOE,QAAQN,GACxCH,EAAiBY,GAAwBC,CAE7C,CAMA,OAAOb,CACT,C,0BC5FA,MAAMe,EAAoC,CACxC,2CACA,2CACA,2CACA,2CACA,2CACA,2CACA,2CACA,2CACA,2CACA,4CAGIC,EAAiC,CACrC,sCACA,sCACA,iDACA,iDACA,iDACA,iDACA,iDACA,iDACA,iDACA,iDACA,iDACA,kDAYF,SAASC,EAAsBC,IAE7B3J,EAAAA,EAAAA,YAAU,KACJ2J,GAAkBA,EAAenH,OAAS,GAC5CoH,EAAAA,EAAQC,QAAQF,KAEjB,CAACA,IAEJ,MAAMG,GAAcF,EAAAA,EAAAA,GAAQD,GAAkB,IAyB9C,OAvB4BI,EAAAA,EAAAA,UAAQ,KAClC,IAAKJ,GAA4C,IAA1BA,EAAenH,SAAiBsH,GAAsC,IAAvBA,EAAYtH,OAChF,MAAO,CAAEwH,MAAO,GAAIC,MAAO,IAG7B,MAAMC,EAAkC,GAClCC,EAAqB,GAc3B,OAZuBC,MAAMC,QAAQP,GAAeA,EAAc,CAACA,IAEpDpB,SAAQ,CAAC4B,EAAMnG,KACvBmG,GAASA,EAAKC,WAInBD,EAAKC,WAAW7B,SAAS8B,IACvBN,EAASO,KAAKD,GACdL,EAASM,KAAKD,EAAK1H,SALnB1C,QAAQsK,KAAK,gDAAD1J,OAAiDmD,EAAC,YAAAnD,OAAW2I,EAAexF,GAAE,0CAQvF,CAAE6F,MAAOE,EAAUD,MAAO,IAAI,IAAIU,IAAIR,OAC5C,CAACL,EAAaH,GAGnB,CAyGA,MAwBMiB,EAAmCA,CACvCC,EACAC,KACwB,IAADC,EAAAC,EAAAC,EACvB7K,QAAQC,IAAI,2CAA4CoB,KAAKC,UAAUmJ,EAAMK,mBAC7E9K,QAAQC,IAAI,qCAAsCwK,EAAMM,YACxD/K,QAAQC,IAAI,qCAAsCoB,KAAKC,UAAUmJ,EAAMO,aACvEhL,QAAQC,IAAI,4CAA6CoB,KAAKC,UAAUmJ,EAAMQ,oBAC5E,MAAM,WACJC,EAAU,UACVC,EAAS,oBACTC,EAAmB,QACnBC,EAAO,OACPC,EAAM,WACNP,EACAD,iBAAkBS,EAAoB,kBACtCN,EAAiB,mBACjBO,EAAkB,cAClBC,EAAa,cACbC,EAAa,cACbC,EAAa,mBACbC,EAAqBxC,EAAiC,mBACtDyC,EAAqBxC,EAA8B,WACnD2B,GACEP,EAEEqB,GAAW7M,EAAAA,EAAAA,QAAoB,OAGrC8M,EAAAA,EAAAA,qBAAoBrB,GAAK,IAAMoB,EAASjM,UAExC,MAAOmM,EAAqBC,IAA0BxM,EAAAA,EAAAA,UAAyB,OACxEyM,EAAaC,IAAkB1M,EAAAA,EAAAA,UAAiC,CAAE2M,aAAc,EAAGC,cAAe,IAEnGC,GAA0B/I,EAAAA,EAAAA,cAAakF,IAC3C,MAAM8D,GAAmBC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAQ/D,GAAO,IAAEgE,UAAWC,KAAKC,QAC1DV,EAAuBM,GACnB9B,EAAMmC,YACRnC,EAAMmC,WAAW,CAACL,MAEnB,CAAC9B,EAAMmC,cAEJ,gBAAErN,EAAe,UAAEG,EAAS,eAAE4D,IAAmB1E,EAAAA,EAAAA,GACrDsM,EACAoB,EACA,CACEjN,yBAA0BoM,EAC1BnM,UAAWmM,KAIf7L,EAAAA,EAAAA,YAAU,KACJF,GAAa2L,GAEfA,EAAQ,IAAI3J,MAAMhC,MAEnB,CAACA,EAAW2L,KAEfzL,EAAAA,EAAAA,YAAU,KACR,IAAIiN,EACAC,EAEJ,MAAMC,EAAeA,KACnBZ,EAAe,CAAEC,aAAc,EAAGC,cAAe,IACjDS,EAAyBzJ,YAAW,KAClC8I,EAAe,CAAEC,aAAc,EAAGC,cAAe,MAChD,KAGH,MAAMW,EAAiC,IAAhB/J,KAAKgK,SAAkB,IAC9CJ,EAAiBxJ,WAAW0J,EAAcC,IAItCE,EAAoC,IAAhBjK,KAAKgK,SAAkB,IAGjD,OAFAJ,EAAiBxJ,WAAW0J,EAAcG,GAEnC,KACLpN,aAAa+M,GACb/M,aAAagN,MAEd,IAEH,MAAMK,GAAoBxD,EAAAA,EAAAA,UAAQ,IAC5Bc,EAAM2C,eAED,CAAC,CAAE1K,KAAM+H,EAAM2C,eAAgB3K,MAAO,IAG3C8I,GAAwBA,EAAqBnJ,OAAS,EAEjDmJ,EAELS,EAEK,CAACA,GAGH,IACN,CAACvB,EAAM2C,eAAgB7B,EAAsBS,IAChDhM,QAAQC,IAAI,2BAA4BoB,KAAKC,UAAU6L,EAAmB,KAAM,IAChF,MAAM9K,GAAasH,EAAAA,EAAAA,UAAQ,IAAMzB,EAAciF,GAAqB,KAAK,CAACA,IAEpEE,GAAsB1D,EAAAA,EAAAA,UAAQ,KAIlC,MAAM2D,EAAclF,EAAyB/F,EAAa,CAACA,GAAc,IAEzE,OADArC,QAAQC,IAAI,mDAAoDoB,KAAKC,UAAUgM,EAAa,KAAM,IAC3FA,IACN,CAACjL,IAEEkL,GAAa5D,EAAAA,EAAAA,UAAQ,IAClB,IAAI6D,EAAAA,EAAqB,CAC9BC,sBAAuB,GACvBC,oBAAqB,GACrBC,gBAAiB,MAElB,IAEGC,GAAqBjE,EAAAA,EAAAA,UAAQ,KACjC,MAAMkE,EAA0B,CAC9BC,QAAS9C,GAAc,CAAC,EACxB7I,SAAUkL,GAAuB,CAAC,EAClCU,OAAQ9C,GAAqB,CAAC,EAC9B+C,KAAM9B,GAAe,CAAC,GAGlB+B,EAAiBV,EAAWW,QAAQL,GAG1C,GAAI9C,EAAY,EACsBnC,OAAOE,QAAQmF,GAAgBE,MAAKnF,IAAA,IAAEC,EAAKC,GAAMF,EAAA,OACnFC,EAAImF,WAAW,WAAalF,GAAS,GAAK,QAGP+E,EAAe9I,SAAW,GAAK,KAClE8I,EAAe9I,QAAU,GACzBnF,QAAQC,IAAI,mDAEhB,CAGA,OADAD,QAAQC,IAAI,8BAA+BoB,KAAKC,UAAU2M,EAAgB,KAAM,IACzEA,IACN,CAACV,EAAYvC,EAAYqC,EAAqBpC,EAAmBiB,EAAanB,KAEzEnB,MAAOyE,EAAWxE,MAAOyE,GAAkBhF,EAAsBsC,IACjEhC,MAAO2E,EAAW1E,MAAO2E,GAAkBlF,EAAsBuC,GACnE4C,GAAqB9E,EAAAA,EAAAA,UAAQ,KAC/B,MAAM+E,EAAc,IAAIC,IAMxB,MALA,IAAIN,KAAcE,GAAWjG,SAAQ8B,IAC7BA,GAAQA,EAAK1H,OAASgM,EAAYE,IAAIxE,EAAK1H,OAC3CgM,EAAYG,IAAIzE,EAAK1H,KAAM0H,MAG5BJ,MAAM8E,KAAKJ,EAAYK,YAC/B,CAACV,EAAWE,KAERS,EAAsBC,KAA2BxP,EAAAA,EAAAA,UAAS,IAC1DyP,GAAsBC,KAA2B1P,EAAAA,EAAAA,UAAS,GAE3D2P,IAAuBzF,EAAAA,EAAAA,UAAQ,IAC/ByB,IACAL,EACKuD,EAAclM,OAAS,EAAIkM,EAAcU,EAAuBV,EAAclM,QAAUsJ,GAAiB,SAE3G8C,EAAcpM,OAAS,EAAIoM,EAAcU,GAAuBV,EAAcpM,QAAUuJ,GAAiB,WAC/G,CAACZ,EAAYK,EAAqBkD,EAAeE,EAAeQ,EAAsBE,GAAsBxD,EAAeC,IAExH0D,IAAoB9L,EAAAA,EAAAA,cAAa+L,IAC/BhE,GACAA,MAEL,CAACA,IAEJ,OAAKH,GAMLnL,QAAQC,IAAI,gFACZD,QAAQC,IAAI,+CAAgDsP,GAC5DvP,QAAQC,IAAI,iDAAuD,OAALsP,QAAK,IAALA,OAAK,EAALA,EAAAA,UAC9DvP,QAAQC,IAAI,qFAA2F,OAALsP,QAAK,IAALA,IAAAA,EAAAA,QAClGvP,QAAQC,IAAI,0FAAgG,OAALsP,QAAK,IAALA,IAAAA,EAAAA,gBAEvGvP,QAAQC,IAAI,8EACZD,QAAQC,IAAI,6CAA+CmD,OAAemM,OAC1EvP,QAAQC,IAAI,+CAAqE,QAAvB0K,EAAGvH,OAAemM,aAAK,IAAA5E,OAAA,EAArBA,EAAuB6E,UACnFxP,QAAQC,IAAI,2DAAiF,QAAtB2K,EAAExH,OAAemM,aAAK,IAAA3E,IAArBA,EAAuB6E,QAC/FzP,QAAQC,IAAI,mEAAyF,QAAtB4K,EAAEzH,OAAemM,aAAK,IAAA1E,IAArBA,EAAuB6E,gBAGvG1P,QAAQC,IAAI,sEAAuEoB,KAAKC,UAAUsM,IAClG5N,QAAQC,IAAI,wEAAyEoB,KAAKC,UAAU0J,IACpGhL,QAAQC,IAAI,6DAA8DmP,KAGxEO,EAAAA,EAAAA,MAACC,EAAAA,GAAM,CACLC,OAAQ,CAAEC,SAAU,CAAC,EAAG,EAAG,GAAIC,IAAK,IACpCC,MAAO,CAAEC,YAAa,QACtBC,SAAO,EACPC,GAAI,CACFC,WAAW,EACXC,OAAO,EACPC,YAAaf,EAAAA,sBACbgB,oBAAqB,EACrBC,iBAAkBjB,EAAAA,gBAClBkB,SAAA,EAEFC,EAAAA,EAAAA,KAAA,gBAAc/H,UAAW,MACzB+H,EAAAA,EAAAA,KAAA,cAAYZ,SAAU,CAAC,EAAG,EAAG,GAAInH,UAAW,MAC5C+H,EAAAA,EAAAA,KAACC,EAAAA,EAAa,CACZC,cAAc,EACdC,WAAW,EACXC,YAAY,EACZC,YAAY,KAEdL,EAAAA,EAAAA,KAACM,EAAAA,QAAmB,CAClB7F,UAAWA,EACX8F,eAAgBxC,EAChBW,qBAAsBA,GACtB8B,cAAetD,EACfuD,aAAc,CAAC,EACfrB,SAAU,CAAC,GAAI,GAAK,GACpBsB,MAAO,EACPC,cAAehC,UAnDnBrP,QAAQsK,KAAK,mEACTe,GAASA,EAAQ,IAAI3J,MAAM,oDACxB,OAsDb8I,EAAiC8G,YAAc,mCAI/C,MAAMC,EAA+BC,EAAAA,WAA+DhH,GACpG,EAAegH,EAAAA,KAAWD,E","sources":["hooks/useHumeEmotionStream.ts","utils/emotionMappings.ts","components/EmotionDrivenAvatar.tsx"],"sourcesContent":["import { useEffect, useRef, useCallback, useState } from 'react';\n\n// Add a type for WebSocket connection state\ntype ConnectionState = 'disconnected' | 'connecting' | 'connected' | 'error' | 'reconnecting';\n\ntype EmotionData = {\n  predictions?: Array<{\n    emotions: Array<{\n      name: string;\n      score: number;\n    }>;\n  }>;\n};\n\ntype OnEmotionDataCallback = (emotion: { name: string; score: number }) => void;\n\n// Helper function to convert ArrayBuffer to Base64 string\nfunction arrayBufferToBase64(buffer: ArrayBuffer): string {\n  let binary = '';\n  const bytes = new Uint8Array(buffer);\n  const len = bytes.byteLength;\n  for (let i = 0; i < len; i++) {\n    binary += String.fromCharCode(bytes[i]);\n  }\n  return window.btoa(binary);\n}\n\nexport const useHumeEmotionStream = (\n  apiKey: string | undefined,\n  onEmotionData: OnEmotionDataCallback,\n  config?: {\n    isEmotionDetectionActive?: boolean;\n    isVideoOn?: boolean;\n  }\n) => {\n  const wsRef = useRef<WebSocket | null>(null);\n  const isMounted = useRef(true); // Tracks component mount state\n  const reconnectAttempts = useRef(0); // Tracks attempts for current connection cycle\n  const reconnectTimeout = useRef<number | undefined>(); // Stores timeout ID for clearing\n\n  const { isEmotionDetectionActive = false, isVideoOn = false } = config || {};\n  const [connectionState, setConnectionState] = useState<ConnectionState>('disconnected');\n  const [lastError, setLastError] = useState<string | null>(null);\n\n  // Effect for component mount/unmount detection\n  useEffect(() => {\n    isMounted.current = true;\n    return () => {\n      isMounted.current = false;\n      // Clear any pending reconnect timeout when the component unmounts\n      if (reconnectTimeout.current) {\n        clearTimeout(reconnectTimeout.current);\n        reconnectTimeout.current = undefined;\n      }\n    };\n  }, []);\n\n  useEffect(() => {\n    // WebSocket should only be active if API key is present AND emotion detection/video are on\n    if (!apiKey || !isEmotionDetectionActive || !isVideoOn) {\n      if (wsRef.current) {\n        console.log('[Hume Stream] Conditions not met or changed. Closing WebSocket.');\n        // Clear handlers to prevent them from firing during or after close\n        wsRef.current.onopen = null;\n        wsRef.current.onmessage = null;\n        wsRef.current.onerror = null;\n        wsRef.current.onclose = null; \n        wsRef.current.close();\n        wsRef.current = null;\n        // Reset reconnect attempts when connection is intentionally closed\n        reconnectAttempts.current = 0; \n        if (reconnectTimeout.current) {\n            clearTimeout(reconnectTimeout.current);\n            reconnectTimeout.current = undefined;\n        }\n      }\n      setConnectionState('disconnected');\n      return; // Exit early if no connection should be active\n    }\n\n    // If a WebSocket connection already exists and is open or connecting, do nothing.\n    // This check prevents re-creating a WebSocket if this effect re-runs due to other dependency changes\n    // while a connection is already being established or is active.\n    if (wsRef.current && (wsRef.current.readyState === WebSocket.OPEN || wsRef.current.readyState === WebSocket.CONNECTING)) {\n      console.log('[Hume Stream] WebSocket already open or connecting. No action needed.');\n      return;\n    }\n\n    console.log('[Hume Stream] Conditions met. Attempting to establish WebSocket connection.');\n    setConnectionState('connecting');\n    setLastError(null);\n\n    const wsUrl = `wss://api.hume.ai/v0/stream/models?apiKey=${encodeURIComponent(apiKey)}`;\n    const ws = new WebSocket(wsUrl);\n    ws.binaryType = 'arraybuffer';\n    wsRef.current = ws; // Assign new WebSocket to ref immediately\n\n    console.log('[Hume Stream] WebSocket instance created. URL:', wsUrl);\n\n    ws.onopen = () => {\n      if (!isMounted.current || wsRef.current !== ws) return; // Stale closure check\n      console.log('[Hume Stream] WebSocket connected.');\n      reconnectAttempts.current = 0; // Reset on successful connection\n      setConnectionState('connected');\n      setLastError(null);\n\n      const initialFrameBase64 = \"dGVzdA==\"; // Placeholder \"test\"\n      const initialMessage = {\n        models: { face: {}, prosody: {} },\n        data: initialFrameBase64,\n      };\n      try {\n        console.log('[Hume Stream] Sending initial message:', JSON.stringify(initialMessage,null,2));\n        ws.send(JSON.stringify(initialMessage));\n      } catch (err) {\n        console.error('[Hume Stream] Error sending initial message:', err);\n        setLastError(err instanceof Error ? err.message : String(err));\n        // Consider closing if initial send fails, or let onclose/onerror handle it\n      }\n    };\n\n    ws.onmessage = (event) => {\n      if (!isMounted.current || wsRef.current !== ws) return; // Stale closure check\n      try {\n        const data = JSON.parse(event.data as string);\n        // console.log('[Hume Stream] Received message:', data);\n        if (data.error) {\n          console.error('[Hume Stream] Hume API error:', data.error);\n          setConnectionState('error');\n          setLastError(`Hume API Error: ${data.error.message || data.error}`);\n          return;\n        }\n        // Process emotion data\n        if (data.predictions?.[0]?.emotions?.length) {\n          const emotions = data.predictions[0].emotions;\n          const topEmotion = emotions.reduce((a:any, b:any) => (a.score > b.score ? a : b), { name: 'neutral', score: 0 });\n          onEmotionData({\n            name: topEmotion.name.toLowerCase(),\n            score: topEmotion.score\n          });\n        }\n      } catch (error) {\n        console.error('[Hume Stream] Error processing message:', error);\n        setLastError(error instanceof Error ? error.message : 'Unknown error');\n      }\n    };\n\n    const attemptReconnect = () => {\n        if (!isMounted.current || !isEmotionDetectionActive || !isVideoOn || reconnectAttempts.current >= 5) {\n            if (reconnectAttempts.current >=5) {\n                console.error('[Hume Stream] Max reconnection attempts reached.');\n                setConnectionState('error');\n                setLastError('Max reconnection attempts reached.');\n            }\n            return; // Don't reconnect if component unmounted, feature disabled, or max attempts reached\n        }\n\n        reconnectAttempts.current++;\n        const delay = Math.min(1000 * Math.pow(2, reconnectAttempts.current -1 ), 30000); // Adjust delay calculation\n        console.log(`[Hume Stream] Attempting to reconnect (${reconnectAttempts.current}/5) in ${delay}ms...`);\n        setConnectionState('reconnecting');\n        \n        // Clear previous timeout before setting a new one\n        if (reconnectTimeout.current) {\n            clearTimeout(reconnectTimeout.current);\n        }\n\n        reconnectTimeout.current = window.setTimeout(() => {\n            if (isMounted.current && isEmotionDetectionActive && isVideoOn) {\n                // Explicitly call the effect's logic to re-initiate connection\n                // This is a bit of a hack; ideally, the main useEffect re-runs.\n                // For now, we'll just create a new WebSocket instance directly if conditions are still met.\n                // This requires duplicating the ws creation logic or refactoring ws creation into a separate function.\n                // To avoid immediate complexity, let's re-evaluate if this manual reconnect is even needed\n                // given the main useEffect will try to establish connection if wsRef.current is null and conditions are met.\n                console.log('[Hume Stream] Reconnect timeout: Triggering new connection attempt.');\n                // Effectively, the main useEffect should handle this by finding wsRef.current is null.\n                // So, we just need to ensure wsRef.current is null after a definitive close.\n                // The main useEffect will then take over if conditions (apiKey, isVideoOn, etc.) are still met.\n                // Forcing a re-run or a direct call to a 'createAndConnectSocket' function might be cleaner.\n                // Let's ensure wsRef.current is null and let the main effect re-evaluate.\n                if(wsRef.current && wsRef.current.readyState !== WebSocket.OPEN) {\n                  wsRef.current = null; // This will make the main useEffect re-evaluate and try to connect\n                  // Manually trigger a state change to encourage re-render of the hook's consumer, potentially re-running this effect.\n                  // This is a bit hacky. A better way would be for `connect` to be a stable function that can be called.\n                  setConnectionState('connecting'); // Force re-evaluation, not ideal\n                }\n\n            }\n        }, delay);\n    };\n\n    ws.onerror = (event) => {\n      if (!isMounted.current || wsRef.current !== ws) return; // Stale closure check\n      console.error('[Hume Stream] WebSocket error:', event);\n      setLastError('WebSocket error occurred.'); // Avoid complex object in state\n      // Don't set to 'error' state immediately, let onclose handle definitive state or attempt reconnect\n      // attemptReconnect(); // onerror often followed by onclose, let onclose manage reconnect attempt\n    };\n\n    ws.onclose = (event) => {\n      if (!isMounted.current || wsRef.current !== ws) return; // Stale closure check\n      console.log(`[Hume Stream] WebSocket closed. Code: ${event.code}, Reason: '${event.reason}', Clean: ${event.wasClean}`);\n      wsRef.current = null; // Important: clear the ref when the socket is definitively closed.\n\n      if (event.code === 1000 || event.code === 1005) { // Normal closure or no status recieved (often client-side intentional close)\n        console.log('[Hume Stream] WebSocket closed normally or intentionally.');\n        setConnectionState('disconnected');\n        reconnectAttempts.current = 0; // Reset attempts on clean close\n        if (reconnectTimeout.current) {\n            clearTimeout(reconnectTimeout.current);\n            reconnectTimeout.current = undefined;\n        }\n      } else if (isMounted.current && isEmotionDetectionActive && isVideoOn) {\n        // Abnormal closure, and we should be connected\n        console.log('[Hume Stream] WebSocket closed abnormally. Attempting reconnect.');\n        setLastError(`WebSocket closed abnormally: ${event.code} ${event.reason}`);\n        attemptReconnect();\n      } else {\n        // Closed, but we shouldn't be connected (e.g., feature toggled off during connection attempt)\n        setConnectionState('disconnected');\n        reconnectAttempts.current = 0;\n      }\n    };\n\n    // Cleanup function for this effect\n    return () => {\n      console.log('[Hume Stream] useEffect cleanup: Closing WebSocket.');\n      // Clear any pending reconnect timeout first\n      if (reconnectTimeout.current) {\n        clearTimeout(reconnectTimeout.current);\n        reconnectTimeout.current = undefined;\n      }\n      if (wsRef.current) {\n        // Remove handlers to prevent them from firing after this cleanup logic\n        wsRef.current.onopen = null;\n        wsRef.current.onmessage = null;\n        wsRef.current.onerror = null;\n        wsRef.current.onclose = null;\n        if (wsRef.current.readyState === WebSocket.OPEN || wsRef.current.readyState === WebSocket.CONNECTING) {\n            wsRef.current.close(1000); // Send a normal closure code\n        }\n        wsRef.current = null;\n      }\n       // When the effect cleans up because dependencies changed (e.g., video toggled off),\n       // or component unmounts, ensure connection state is 'disconnected'.\n      setConnectionState('disconnected'); \n      reconnectAttempts.current = 0; // Reset reconnect attempts\n    };\n  // Dependencies: apiKey ensures re-connection if it changes.\n  // isEmotionDetectionActive and isVideoOn control the active state.\n  // onEmotionData is needed to correctly set up the onmessage handler with the latest callback.\n  }, [apiKey, isEmotionDetectionActive, isVideoOn, onEmotionData]); \n\n  const sendVideoFrame = useCallback(async (frame: Blob | ArrayBuffer) => {\n    if (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) {\n      // console.warn('[Hume Stream] WebSocket not open. Cannot send video frame.');\n      return;\n    }\n    const base64Data = arrayBufferToBase64(frame instanceof Blob ? await frame.arrayBuffer() : frame);\n    const message = {\n      models: { face: {}, prosody: {} },\n      data: base64Data,\n    };\n    try {\n      wsRef.current.send(JSON.stringify(message));\n    } catch (err) {\n      console.error('[Hume Stream] Error sending video frame:', err);\n      setLastError(err instanceof Error ? err.message : String(err));\n    }\n  }, []); // No dependencies, relies on wsRef.current which is managed by useEffect\n\n  return { sendVideoFrame, connectionState, lastError };\n};\n","import { BlendShapeMap, BlendshapeKey } from '../types/blendshapes'; // Assuming BlendshapeKey is your ARKitBlendshapeName\nexport type { BlendShapeMap };\n\n// Define Emotion type if not already centrally available\ninterface Emotion {\n  name: string;\n  score: number;\n}\n\n// Expanded and TitleCased map\n// Values are target blendshape intensities when emotion score is 1.0\nconst HUME_EMOTION_TO_ARKIT_MAP: Record<string, Partial<BlendShapeMap>> = {\n  // Positive, High Arousal\n  Joy: { mouthSmileLeft: 0.8, mouthSmileRight: 0.8, cheekSquintLeft: 0.6, cheekSquintRight: 0.6, eyeSquintLeft: 0.3, eyeSquintRight: 0.3 },\n  Amusement: { mouthSmileLeft: 0.7, mouthSmileRight: 0.7, cheekSquintLeft: 0.5, cheekSquintRight: 0.5 },\n  Excitement: { eyeWideLeft: 0.4, eyeWideRight: 0.4, mouthOpen: 0.3, jawOpen: 0.2, browInnerUp: 0.3 },\n  Awe: { mouthOpen: 0.4, jawOpen: 0.3, eyeWideLeft: 0.5, eyeWideRight: 0.5, browInnerUp: 0.4 },\n  Admiration: { mouthSmileLeft: 0.3, mouthSmileRight: 0.3, eyeLookUpLeft: 0.2, eyeLookUpRight: 0.2, browInnerUp: 0.2 }, // Softer smile, upward gaze\n  Love: { mouthSmileLeft: 0.5, mouthSmileRight: 0.5, eyeSquintLeft: 0.2, eyeSquintRight: 0.2, browOuterUpLeft: 0.1, browOuterUpRight: 0.1 }, // Gentle smile\n\n  // Positive, Low Arousal\n  Calmness: { eyeSquintLeft: 0.1, eyeSquintRight: 0.1, mouthClose: 0.2 }, // Relaxed eyes, slightly closed mouth\n  Contentment: { mouthSmileLeft: 0.2, mouthSmileRight: 0.2 }, // Gentle, subtle smile\n  Interest: { browInnerUp: 0.4, eyeWideLeft: 0.2, eyeWideRight: 0.2, jawOpen: 0.15 }, // Renamed mouthOpen to mouthSlightOpen for clarity, maps to jawOpen or mouthOpen\n  Serenity: { eyeSquintLeft: 0.05, eyeSquintRight: 0.05 }, // Very subtle relaxation\n\n  // Negative, High Arousal\n  Anger: { browDownLeft: 0.9, browDownRight: 0.9, noseSneerLeft: 0.7, noseSneerRight: 0.7, mouthShrugUpper: 0.5, eyeSquintLeft: 0.6, eyeSquintRight: 0.6 },\n  Fear: { eyeWideLeft: 0.9, eyeWideRight: 0.9, mouthOpen: 0.5, jawOpen: 0.4, browInnerUp: 0.7, mouthStretchLeft: 0.3, mouthStretchRight: 0.3 },\n  Horror: { eyeWideLeft: 1.0, eyeWideRight: 1.0, mouthOpen: 0.7, jawOpen: 0.6, browInnerUp: 0.8, mouthShrugLower: 0.5 },\n  Distress: { mouthFrownLeft: 0.6, mouthFrownRight: 0.6, browInnerUp: 0.7, eyeSquintLeft: 0.4, eyeSquintRight: 0.4 },\n\n  // Negative, Low Arousal\n  Sadness: { mouthFrownLeft: 0.7, mouthFrownRight: 0.7, browInnerUp: 0.6, browDownLeft: 0.2, browDownRight: 0.2, eyeLookDownLeft: 0.3, eyeLookDownRight: 0.3 },\n  Disappointment: { mouthFrownLeft: 0.5, mouthFrownRight: 0.5, browDownLeft: 0.4, browDownRight: 0.4 },\n  Tiredness: { eyeSquintLeft: 0.5, eyeSquintRight: 0.5, browDownLeft: 0.3, browDownRight: 0.3, jawOpen: 0.1, mouthOpen: 0.1, eyeLookDownLeft: 0.2, eyeLookDownRight: 0.2 },\n  Boredom: { eyeLookDownLeft: 0.4, eyeLookDownRight: 0.4, mouthShrugLower: 0.3, jawOpen: 0.05 },\n  Guilt: { browDownLeft: 0.5, browDownRight: 0.5, eyeLookDownLeft: 0.5, eyeLookDownRight: 0.5, mouthPressLeft: 0.2, mouthPressRight: 0.2 },\n\n  // Other\n  Surprise: { jawOpen: 0.5, eyeWideLeft: 0.8, eyeWideRight: 0.8, browInnerUp: 0.7, mouthOpen: 0.4 },\n  Disgust: { noseSneerLeft: 0.8, noseSneerRight: 0.8, mouthUpperUpLeft: 0.6, mouthUpperUpRight: 0.6, browDownLeft: 0.5, browDownRight: 0.5 },\n  Contempt: { mouthSmileLeft: 0.4, noseSneerRight: 0.5 }, // Asymmetrical smile/sneer\n  Embarrassment: { cheekPuff: 0.3, eyeLookDownLeft: 0.4, eyeLookDownRight: 0.4, mouthPressLeft: 0.1, mouthPressRight: 0.1 },\n  Confusion: { browDownLeft: 0.3, browOuterUpRight: 0.3, mouthPucker: 0.2 }, // One brow down, one up (simplified)\n\n  Neutral: {\n    // Explicitly define neutral with very low values or zeros\n    // eyeBlinkLeft: 0.01, // slight ambient motion if desired\n    // eyeBlinkRight: 0.01,\n  },\n};\n\n// Renamed from original emotionToBlendshapes for clarity if needed, or keep as is.\n// This function processes a SINGLE emotion object.\nfunction getBlendshapesForSingleEmotion(emotion: Emotion): Partial<BlendShapeMap> {\n  if (!emotion || !emotion.name) return {};\n\n  // Use emotion.name directly (assuming it's TitleCase from Hume)\n  const baseShapes = HUME_EMOTION_TO_ARKIT_MAP[emotion.name];\n  if (!baseShapes) {\n    // console.log(`[emotionMappings] No direct blendshape map for emotion: ${emotion.name}`);\n    return {};\n  }\n\n  const intensity = emotion.score; // Direct score application (0-1)\n\n  return Object.fromEntries(\n    Object.entries(baseShapes).map(([key, value]) => [\n      key as BlendshapeKey, // Cast key to BlendshapeKey\n      (value as number) * intensity,\n    ])\n  ) as Partial<BlendShapeMap>;\n}\n\nexport function getTopEmotion(emotions: Emotion[]): Emotion | null {\n  if (!emotions || emotions.length === 0) {\n    return null;\n  }\n  return emotions.reduce(\n    (top, current) => (current.score > (top?.score || 0) ? current : top),\n    emotions[0] // Initialize with the first emotion to handle single-item arrays correctly\n  );\n}\n\n// This will be the main function imported by EmotionDrivenAvatar.tsx\nexport function mapEmotionsToBlendshapes(emotions: Emotion[]): BlendShapeMap {\n  const finalBlendshapes = {} as BlendShapeMap;\n\n  // Initialize all known ARKit blendshapes to 0 to ensure a clean slate\n  // This step can be skipped if TestAvatar/SimAvatar3D correctly resets influences each frame\n  // For now, let's assume it's safer to provide a full map.\n  // Consider importing ARKitBlendshapeNamesList from types/blendshapes if available\n  const allArkitShapes: BlendshapeKey[] = [\"browDownLeft\", \"browDownRight\", \"browInnerUp\", \"browOuterUpLeft\", \"browOuterUpRight\", \"cheekPuff\", \"cheekSquintLeft\", \"cheekSquintRight\", \"eyeBlinkLeft\", \"eyeBlinkRight\", \"eyeLookDownLeft\", \"eyeLookDownRight\", \"eyeLookInLeft\", \"eyeLookInRight\", \"eyeLookOutLeft\", \"eyeLookOutRight\", \"eyeLookUpLeft\", \"eyeLookUpRight\", \"eyeSquintLeft\", \"eyeSquintRight\", \"eyeWideLeft\", \"eyeWideRight\", \"jawForward\", \"jawLeft\", \"jawOpen\", \"jawRight\", \"mouthClose\", \"mouthDimpleLeft\", \"mouthDimpleRight\", \"mouthFrownLeft\", \"mouthFrownRight\", \"mouthFunnel\", \"mouthLeft\", \"mouthLowerDownLeft\", \"mouthLowerDownRight\", \"mouthPressLeft\", \"mouthPressRight\", \"mouthPucker\", \"mouthRight\", \"mouthRollLower\", \"mouthRollUpper\", \"mouthShrugLower\", \"mouthShrugUpper\", \"mouthSmileLeft\", \"mouthSmileRight\", \"mouthStretchLeft\", \"mouthStretchRight\", \"mouthUpperUpLeft\", \"mouthUpperUpRight\", \"noseSneerLeft\", \"noseSneerRight\", \"tongueOut\"]; // Add all ARKit names\n  allArkitShapes.forEach(shape => finalBlendshapes[shape] = 0);\n\n\n  const topEmotion = getTopEmotion(emotions);\n\n  if (topEmotion) {\n    // console.log(`[mapEmotionsToBlendshapes] Top emotion: ${topEmotion.name}, Score: ${topEmotion.score.toFixed(3)}`);\n    const topEmotionBlendshapes = getBlendshapesForSingleEmotion(topEmotion);\n    \n    for (const [key, value] of Object.entries(topEmotionBlendshapes)) {\n      finalBlendshapes[key as BlendshapeKey] = value;\n    }\n  } else {\n    // console.log('[mapEmotionsToBlendshapes] No top emotion found or emotions array empty.');\n    // Return the zeroed map for neutral if no emotions\n  }\n  \n  // console.log('[mapEmotionsToBlendshapes] Final combined blendshapes:', JSON.stringify(finalBlendshapes));\n  return finalBlendshapes;\n}\n\n// Example of how one might blend multiple emotions in the future (more complex):\n/*\nexport function mapEmotionsToBlendshapesAdvanced(emotions: Emotion[]): BlendShapeMap {\n  const allBlendshapes: Record<BlendshapeKey, number> = {} as Record<BlendshapeKey, number>;\n  ARKitBlendshapeNamesList.forEach(name => allBlendshapes[name] = 0); // Initialize\n\n  // Consider only top N emotions or those above a threshold\n  const significantEmotions = emotions.filter(e => e.score > 0.1).sort((a,b) => b.score - a.score).slice(0, 3);\n\n  significantEmotions.forEach(emotion => {\n    const emotionShapes = getBlendshapesForSingleEmotion(emotion);\n    for (const shapeName_str in emotionShapes) {\n      const shapeName = shapeName_str as BlendshapeKey;\n      const currentValue = allBlendshapes[shapeName] || 0;\n      const newValue = emotionShapes[shapeName]!;\n      // Simple averaging or max, could be more sophisticated (e.g., weighted by score)\n      allBlendshapes[shapeName] = Math.max(currentValue, newValue); \n    }\n  });\n  return allBlendshapes;\n}\n*/\n","import process from 'process';\nimport React, { useState, useEffect, useRef, useCallback, useMemo, useImperativeHandle, forwardRef } from 'react';\nimport { Canvas, useFrame, useThree, extend } from '@react-three/fiber';\nimport { OrbitControls, useAnimations, useGLTF } from '@react-three/drei'; // Keep useGLTF for animations if still used there\nimport { GLTFLoader, KTX2Loader, MeshoptDecoder, type GLTF } from 'three-stdlib';\n\n\nimport { DRACOLoader } from 'three/examples/jsm/loaders/DRACOLoader.js';\n\nimport * as THREE from 'three';\nimport { ARKitBlendshapeNamesList, BlendshapeKey } from '../types/blendshapes';\nimport { Group, Mesh, SkinnedMesh, Vector3Tuple, AnimationClip, LoopRepeat } from 'three';\nimport { useHumeEmotionStream } from '../hooks/useHumeEmotionStream';\nimport SimulationAvatar3D from './TestAvatar'; // Re-enable for testing with logs, now points to TestAvatar.tsx\nimport { GroupProps } from '@react-three/fiber';\n\n// We don't need ReadyPlayerMeAvatarProps here directly anymore, SimulationAvatar3D handles its own prop types.\nimport { mapEmotionsToBlendshapes, getTopEmotion } from '../utils/emotionMappings';\nimport { BlendshapeCompositor, BlendshapeInput } from '../utils/blendshapeCompositor';\n\n// Animation file lists (relative to /public directory)\nconst MASCULINE_TALKING_ANIMATION_FILES = [\n  \"/animations/M_Talking_Variations_001.glb\",\n  \"/animations/M_Talking_Variations_002.glb\",\n  \"/animations/M_Talking_Variations_003.glb\",\n  \"/animations/M_Talking_Variations_004.glb\",\n  \"/animations/M_Talking_Variations_005.glb\",\n  \"/animations/M_Talking_Variations_006.glb\",\n  \"/animations/M_Talking_Variations_007.glb\",\n  \"/animations/M_Talking_Variations_008.glb\",\n  \"/animations/M_Talking_Variations_009.glb\",\n  \"/animations/M_Talking_Variations_010.glb\",\n];\n\nconst MASCULINE_IDLE_ANIMATION_FILES = [\n  \"/animations/M_Standing_Idle_001.glb\",\n  \"/animations/M_Standing_Idle_002.glb\",\n  \"/animations/M_Standing_Idle_Variations_001.glb\",\n  \"/animations/M_Standing_Idle_Variations_002.glb\",\n  \"/animations/M_Standing_Idle_Variations_003.glb\",\n  \"/animations/M_Standing_Idle_Variations_004.glb\",\n  \"/animations/M_Standing_Idle_Variations_005.glb\",\n  \"/animations/M_Standing_Idle_Variations_006.glb\",\n  \"/animations/M_Standing_Idle_Variations_007.glb\",\n  \"/animations/M_Standing_Idle_Variations_008.glb\",\n  \"/animations/M_Standing_Idle_Variations_009.glb\",\n  \"/animations/M_Standing_Idle_Variations_010.glb\",\n];\n\n// Helper to extract filename for unique naming\nconst getUniqueAnimName = (path: string, prefix: string) => {\n  const filename = path.split('/').pop()?.replace('.glb', '');\n  return `${prefix}_${filename}`;\n};\n\n// Helper to get a filename without extension for prefixing, if needed\n// const getBaseFilename = (path: string) => path.split('/').pop()?.split('.')[0] || 'anim';\n\nfunction useExternalAnimations(animationPaths?: string[]): { clips: THREE.AnimationClip[], names: string[] } {\n  // Preload all paths. useGLTF.preload is not a hook.\n  useEffect(() => {\n    if (animationPaths && animationPaths.length > 0) {\n      useGLTF.preload(animationPaths);\n    }\n  }, [animationPaths]);\n\n  const gltfResults = useGLTF(animationPaths || []) as GLTF[]; // Cast to GLTF[] for type safety\n\n  const processedAnimations = useMemo(() => {\n    if (!animationPaths || animationPaths.length === 0 || !gltfResults || gltfResults.length === 0) {\n      return { clips: [], names: [] };\n    }\n\n    const allClips: THREE.AnimationClip[] = [];\n    const allNames: string[] = [];\n\n    const gltfsToProcess = Array.isArray(gltfResults) ? gltfResults : [gltfResults];\n\n    gltfsToProcess.forEach((gltf, i) => {\n      if (!gltf || !gltf.animations) {\n        console.warn(`[useExternalAnimations] GLTF result at index ${i} (path: ${animationPaths[i]}) is invalid or has no animations.`);\n        return;\n      }\n      gltf.animations.forEach((clip) => {\n        allClips.push(clip); \n        allNames.push(clip.name);\n      });\n    });\n    return { clips: allClips, names: [...new Set(allNames)] }; \n  }, [gltfResults, animationPaths]);\n\n  return processedAnimations;\n}\n\n// ======================== TYPES ========================\n\n// Standard ARKit blendshape names\nexport type ARKitBlendshapeName =\n  | 'browDownLeft'\n  | 'browDownRight'\n  | 'browInnerUp'\n  | 'browOuterUpLeft'\n  | 'browOuterUpRight'\n  | 'cheekPuff'\n  | 'cheekSquintLeft'\n  | 'cheekSquintRight'\n  | 'eyeBlinkLeft'\n  | 'eyeBlinkRight'\n  | 'eyeLookDownLeft'\n  | 'eyeLookDownRight'\n  | 'eyeLookInLeft'\n  | 'eyeLookInRight'\n  | 'eyeLookOutLeft'\n  | 'eyeLookOutRight'\n  | 'eyeLookUpLeft'\n  | 'eyeLookUpRight'\n  | 'eyeSquintLeft'\n  | 'eyeSquintRight'\n  | 'eyeWideLeft'\n  | 'eyeWideRight'\n  | 'jawForward'\n  | 'jawLeft'\n  | 'jawOpen'\n  | 'jawRight'\n  | 'mouthClose'\n  | 'mouthDimpleLeft'\n  | 'mouthDimpleRight'\n  | 'mouthFrownLeft'\n  | 'mouthFrownRight'\n  | 'mouthFunnel'\n  | 'mouthLeft'\n  | 'mouthLowerDownLeft'\n  | 'mouthLowerDownRight'\n  | 'mouthPressLeft'\n  | 'mouthPressRight'\n  | 'mouthPucker'\n  | 'mouthRight'\n  | 'mouthRollLower'\n  | 'mouthRollUpper'\n  | 'mouthShrugLower'\n  | 'mouthShrugUpper'\n  | 'mouthSmileLeft'\n  | 'mouthSmileRight'\n  | 'mouthStretchLeft'\n  | 'mouthStretchRight'\n  | 'mouthUpperUpLeft'\n  | 'mouthUpperUpRight'\n  | 'noseSneerLeft'\n  | 'noseSneerRight'\n  | 'tongueOut';\n\nexport type BlendShapeMap = Partial<Record<ARKitBlendshapeName, number>>;\n\n\nexport interface Emotion {\n  name: string;\n  score: number;\n  timestamp?: number; // Added timestamp\n}\n\ninterface Avatar3DProps {\n  url: string;\n  visemeShapes?: Partial<BlendShapeMap>; // For viseme-driven facial expressions\n  emotionShapes?: Partial<BlendShapeMap>; // For emotion-driven facial expressions\n  isSpeaking?: boolean; // To indicate if avatar is currently speaking (for visemes)\n  position?: Vector3Tuple;\n  scale?: number | Vector3Tuple;\n  onLoaded?: () => void;\n  onModelLoaded?: (model: Group) => void;\n  currentAnimationName?: string; // For body animation\n  additionalClips?: THREE.AnimationClip[]; // For externally loaded animations\n}\n\ninterface EmotionDrivenAvatarProps {\n  humeApiKey?: string;\n  avatarUrl: string;\n  visemeBlendshapes?: Partial<BlendShapeMap>; // Added for viseme-driven blendshapes\n  activeBodyAnimation?: string; // New prop for body animation\n  onError?: (error: Error) => void;\n  onLoad?: () => void;\n  onEmotionDetected?: (emotion: Emotion) => void;\n  isSpeaking: boolean; // Added to control avatar speaking state from parent\n  visemeData?: Record<string, number>; // Added for direct viseme data input\n  detectedEmotions?: Emotion[]; // New prop for receiving emotion data\n  directBlendshapes?: Partial<BlendShapeMap>; // For direct blendshape control\n  emotionBlendshapes?: Partial<BlendShapeMap>; // For emotion-driven blendshapes (e.g., from prosody)\n  cameraEnabled?: boolean;\n  talkAnimation?: string; // Fallback if paths are not provided\n  idleAnimation?: string; // Fallback if paths are not provided\n  talkAnimationPaths?: string[]; // Paths to GLBs for talking animations\n  idleAnimationPaths?: string[]; // Paths to GLBs for idle animations\n  onEmotions?: (emotions: Emotion[]) => void; // Added onEmotions callback\n  currentEmotion?: string; // For direct emotion string input from parent\n  idleShapes?: Partial<BlendShapeMap>; // For idle blinking or resting expression\n}\n\n// Default light properties for Avatar3D\nconst AVATAR_AMBIENT_LIGHT_INTENSITY = 0.9; // Slightly increased\nconst AVATAR_DIRECTIONAL_LIGHT_POSITION = { x: 5, y: 5, z: 5 }; // Simple object for position\nconst AVATAR_DIRECTIONAL_LIGHT_INTENSITY = 0.8; // Reduced intensity\nconst AVATAR_HEMISPHERE_SKY_COLOR = 0xffffbb;\nconst AVATAR_HEMISPHERE_GROUND_COLOR = 0x080820;\nconst AVATAR_HEMISPHERE_INTENSITY = 0.6;\n\n// Local emotion mapping helpers (ARKIT_EMOTION_MAP_MINIMAL_V2 and local mapEmotionsToBlendshapes) removed.\n// Using imported mapEmotionsToBlendshapes from ../utils/emotionMappings.ts which has its own internal map.\n\n// ======================== COMPONENTS ========================\n\n// Define the InlineBox component here, within the same file scope\nconst InlineBox = (props: GroupProps) => {\n  return (\n    <group {...props}>\n      <mesh position={[0, 0.5, 0]}> {/* Centered, assuming pivot is at feet */}\n        <boxGeometry args={[0.5, 1, 0.5]} /> {/* Approx human-like proportions */}\n        <meshStandardMaterial color=\"purple\" /> {/* Distinct color */}\n      </mesh>\n    </group>\n  );\n};\n\nconst EmotionDrivenAvatarComponentBody = (\n  props: EmotionDrivenAvatarProps,\n  ref: React.ForwardedRef<THREE.Group | null>\n): JSX.Element | null => {\n  console.log('[EDA] Props received - detectedEmotions:', JSON.stringify(props.detectedEmotions));\n  console.log('[EDA] Props received - isSpeaking:', props.isSpeaking);\n  console.log('[EDA] Props received - visemeData:', JSON.stringify(props.visemeData));\n  console.log('[EDA] Props received - directBlendshapes:', JSON.stringify(props.directBlendshapes));\n    const {\n      humeApiKey,\n      avatarUrl,\n      activeBodyAnimation,\n      onError,\n      onLoad,\n      isSpeaking,\n      detectedEmotions: propDetectedEmotions,\n      directBlendshapes,\n      emotionBlendshapes,\n      cameraEnabled,\n      talkAnimation, \n      idleAnimation, \n      talkAnimationPaths = MASCULINE_TALKING_ANIMATION_FILES,\n      idleAnimationPaths = MASCULINE_IDLE_ANIMATION_FILES,\n      visemeData, // Added to destructure from props\n    } = props;\n\n    const modelRef = useRef<THREE.Group>(null); // Local ref for SimulationAvatar3D\n    \n    // Forward the ref to the internal modelRef if the parent needs access to the THREE.Group\n    useImperativeHandle(ref, () => modelRef.current as THREE.Group);\n\n    const [latestStreamEmotion, setLatestStreamEmotion] = useState<Emotion | null>(null);\n    const [blinkShapes, setBlinkShapes] = useState<Partial<BlendShapeMap>>({ eyeBlinkLeft: 0, eyeBlinkRight: 0 });\n\n    const handleStreamEmotionData = useCallback((emotion: { name: string; score: number; }) => {\n      const newEmotion: Emotion = { ...emotion, timestamp: Date.now() };\n      setLatestStreamEmotion(newEmotion);\n      if (props.onEmotions) {\n        props.onEmotions([newEmotion]);\n      }\n    }, [props.onEmotions]);\n\n    const { connectionState, lastError, sendVideoFrame } = useHumeEmotionStream(\n      humeApiKey, // Pass apiKey directly, hook handles undefined\n      handleStreamEmotionData, // Pass the new callback\n      { // Pass config object\n        isEmotionDetectionActive: cameraEnabled,\n        isVideoOn: cameraEnabled \n      }\n    );\n\n    useEffect(() => {\n      if (lastError && onError) {\n        // lastError from useHumeEmotionStream is string | null\n        onError(new Error(lastError)); \n      }\n    }, [lastError, onError]);\n\n    useEffect(() => {\n      let blinkTimeoutId: NodeJS.Timeout;\n      let blinkDurationTimeoutId: NodeJS.Timeout;\n\n      const triggerBlink = () => {\n        setBlinkShapes({ eyeBlinkLeft: 1, eyeBlinkRight: 1 });\n        blinkDurationTimeoutId = setTimeout(() => {\n          setBlinkShapes({ eyeBlinkLeft: 0, eyeBlinkRight: 0 });\n        }, 150); // Blink duration: 150ms\n\n        // Schedule next blink randomly between 3 to 7 seconds\n        const nextBlinkDelay = Math.random() * 4000 + 3000;\n        blinkTimeoutId = setTimeout(triggerBlink, nextBlinkDelay);\n      };\n\n      // Start the first blink after a short delay\n      const initialBlinkDelay = Math.random() * 4000 + 1000; // Initial delay 1-5 seconds\n      blinkTimeoutId = setTimeout(triggerBlink, initialBlinkDelay);\n\n      return () => {\n        clearTimeout(blinkTimeoutId);\n        clearTimeout(blinkDurationTimeoutId);\n      };\n    }, []); // Empty dependency array ensures this runs once on mount and cleans up on unmount\n\n    const emotionsToProcess = useMemo(() => {\n      if (props.currentEmotion) { // Prioritize direct string from parent\n        // console.log(`[EDA] Using props.currentEmotion: ${props.currentEmotion}`);\n        return [{ name: props.currentEmotion, score: 1.0 }]; // Convert to Emotion[]\n      }\n      // Fallback to existing logic if props.currentEmotion is not provided\n      if (propDetectedEmotions && propDetectedEmotions.length > 0) {\n        // console.log('[EDA] Using props.propDetectedEmotions');\n        return propDetectedEmotions;\n      }\n      if (latestStreamEmotion) {\n        // console.log('[EDA] Using internal latestStreamEmotion');\n        return [latestStreamEmotion];\n      }\n      // console.log('[EDA] No emotion source found, returning empty array.');\n      return [];\n    }, [props.currentEmotion, propDetectedEmotions, latestStreamEmotion]);\n    console.log('[EDA] emotionsToProcess:', JSON.stringify(emotionsToProcess, null, 2));\n    const topEmotion = useMemo(() => getTopEmotion(emotionsToProcess || []), [emotionsToProcess]);\n\n    const activeEmotionShapes = useMemo(() => {\n      // Use the imported mapEmotionsToBlendshapes function\n      // It expects an array of emotions and uses its internal mapping.\n      // Pass an empty array if topEmotion is null to avoid errors and get a neutral/zeroed map.\n      const blendshapes = mapEmotionsToBlendshapes(topEmotion ? [topEmotion] : []);\n      console.log('[EDA] Blendshapes from mapEmotionsToBlendshapes:', JSON.stringify(blendshapes, null, 2));\n      return blendshapes;\n    }, [topEmotion]);\n\n    const compositor = useMemo(() => {\n      return new BlendshapeCompositor({\n        emotionMouthReduction: 0.3,  // Reduce emotion mouth shapes when visemes are active\n        emotionFaceBlending: 0.8,    // Keep emotion eye/brow shapes strong\n        smoothingFactor: 0.1         // Light smoothing for transitions\n      });\n    }, []);\n\n    const finalEmotionShapes = useMemo(() => {\n      const inputs: BlendshapeInput = {\n        visemes: visemeData || {},           // Lip sync (highest priority for mouth)\n        emotions: activeEmotionShapes || {}, // Emotional expressions\n        manual: directBlendshapes || {},     // Manual overrides (highest priority overall)\n        base: blinkShapes || {}              // Base/idle state (blinking)\n      };\n      \n      const composedShapes = compositor.compose(inputs);\n      \n      // Add fallback jaw animation if speaking but no significant mouth movement\n      if (isSpeaking) {\n        const hasSignificantMouthMovement = Object.entries(composedShapes).some(([key, value]) => \n          key.startsWith('mouth') && (value || 0) > 0.1\n        );\n        \n        if (!hasSignificantMouthMovement && (composedShapes.jawOpen || 0) < 0.1) {\n          composedShapes.jawOpen = 0.4; // Fallback jaw animation\n          console.log('[EDA] Applied fallback jaw animation for speech');\n        }\n      }\n      \n      console.log('[EDA] Composed blendshapes:', JSON.stringify(composedShapes, null, 2));\n      return composedShapes;\n    }, [compositor, visemeData, activeEmotionShapes, directBlendshapes, blinkShapes, isSpeaking]);\n\n    const { clips: talkClips, names: talkAnimNames } = useExternalAnimations(talkAnimationPaths);\n    const { clips: idleClips, names: idleAnimNames } = useExternalAnimations(idleAnimationPaths);\n    const allAdditionalClips = useMemo(() => {\n        const uniqueClips = new Map<string, THREE.AnimationClip>();\n        [...talkClips, ...idleClips].forEach(clip => {\n            if (clip && clip.name && !uniqueClips.has(clip.name)) {\n                uniqueClips.set(clip.name, clip);\n            }\n        });\n        return Array.from(uniqueClips.values());\n    }, [talkClips, idleClips]);\n\n    const [currentTalkAnimIndex, setCurrentTalkAnimIndex] = useState(0);\n    const [currentIdleAnimIndex, setCurrentIdleAnimIndex] = useState(0);\n    \n    const currentAnimationName = useMemo(() => {\n      if (activeBodyAnimation) return activeBodyAnimation;\n      if (isSpeaking) {\n        return talkAnimNames.length > 0 ? talkAnimNames[currentTalkAnimIndex % talkAnimNames.length] : talkAnimation || 'Talk_0';\n      }\n      return idleAnimNames.length > 0 ? idleAnimNames[currentIdleAnimIndex % idleAnimNames.length] : idleAnimation || 'Idle_0';\n    }, [isSpeaking, activeBodyAnimation, talkAnimNames, idleAnimNames, currentTalkAnimIndex, currentIdleAnimIndex, talkAnimation, idleAnimation]);\n\n    const handleModelLoaded = useCallback((loadedModel: THREE.Group) => {\n        if (onLoad) {\n            onLoad();\n        }\n    }, [onLoad]);\n\n    if (!avatarUrl) {\n      console.warn(\"EmotionDrivenAvatar: avatarUrl is not provided. Rendering null.\");\n      if (onError) onError(new Error(\"EmotionDrivenAvatar: avatarUrl is not provided.\"));\n      return null; \n    }\n\n    console.log(\"[EmotionDrivenAvatar] Checking IMPORTED THREE instance before Canvas render:\");\n    console.log(\"[EmotionDrivenAvatar] IMPORTED THREE object:\", THREE);\n    console.log(\"[EmotionDrivenAvatar] IMPORTED THREE.REVISION:\", THREE?.REVISION);\n    console.log(\"[EmotionDrivenAvatar] Is IMPORTED THREE.Cache available (added by threejs core)?\", !!THREE?.Cache);\n    console.log(\"[EmotionDrivenAvatar] Is IMPORTED THREE.CanvasTexture available (used by R3F Canvas)?\", !!THREE?.CanvasTexture);\n\n    console.log(\"[EmotionDrivenAvatar] Checking WINDOW.THREE instance before Canvas render:\");\n    console.log(\"[EmotionDrivenAvatar] WINDOW.THREE object:\", (window as any).THREE);\n    console.log(\"[EmotionDrivenAvatar] WINDOW.THREE.REVISION:\", (window as any).THREE?.REVISION);\n    console.log(\"[EmotionDrivenAvatar] Is WINDOW.THREE.Cache available?\", !!(window as any).THREE?.Cache);\n    console.log(\"[EmotionDrivenAvatar] Is WINDOW.THREE.CanvasTexture available?\", !!(window as any).THREE?.CanvasTexture);\n\n    // Log states being passed to SimulationAvatar3D\n    console.log('[EDA] Values passed to SimAvatar3D - finalEmotionShapes (COMPOSED):', JSON.stringify(finalEmotionShapes));\n    console.log('[EDA] Values passed to SimAvatar3D - visemeData (RAW, for reference):', JSON.stringify(visemeData));\n    console.log('[EDA] Values passed to SimAvatar3D - currentAnimationName:', currentAnimationName);\n\n    return (\n      <Canvas\n        camera={{ position: [0, 0, 2], fov: 50 }} // Values from before the bad edit\n        style={{ touchAction: 'none' }}\n        shadows\n        gl={{\n          antialias: true,\n          alpha: true,\n          toneMapping: THREE.ACESFilmicToneMapping,\n          toneMappingExposure: 1.0,\n          outputColorSpace: THREE.SRGBColorSpace,\n        }}\n      >\n        <ambientLight intensity={0.8} />\n        <pointLight position={[5, 5, 5]} intensity={0.8} />\n        <OrbitControls \n          enableRotate={false}\n          enablePan={false}\n          enableZoom={false}\n          autoRotate={false}\n        />\n        <SimulationAvatar3D // This is TestAvatar\n          avatarUrl={avatarUrl} // Pass the avatarUrl\n          animationClips={allAdditionalClips} // Pass the loaded animation clips\n          currentAnimationName={currentAnimationName} // Pass the determined current animation name\n          emotionShapes={finalEmotionShapes} // Pass the composed blendshapes (includes visemes, emotions, manual, blinks)\n          visemeShapes={{}} // Empty since finalEmotionShapes now contains composed visemes\n          position={[0, -0.8, 0]}\n          scale={1.0}\n          onModelLoaded={handleModelLoaded} // Pass onModelLoaded callback\n        />\n      </Canvas>\n    );\n}; // Closing EmotionDrivenAvatarComponentBody\nEmotionDrivenAvatarComponentBody.displayName = 'EmotionDrivenAvatarComponentBody';\n\n// Assuming EmotionDrivenAvatarProps is defined above in the file or imported.\n// The export structure from before the error:\nconst ForwardedEmotionDrivenAvatar = React.forwardRef<THREE.Group | null, EmotionDrivenAvatarProps>(EmotionDrivenAvatarComponentBody as any); // Using 'as any' to bridge potential ref type mismatches for now, this complex export needs review later.\nexport default React.memo(ForwardedEmotionDrivenAvatar);\n"],"names":["useHumeEmotionStream","apiKey","onEmotionData","config","wsRef","useRef","isMounted","reconnectAttempts","reconnectTimeout","isEmotionDetectionActive","isVideoOn","connectionState","setConnectionState","useState","lastError","setLastError","useEffect","current","clearTimeout","undefined","console","log","onopen","onmessage","onerror","onclose","close","readyState","WebSocket","OPEN","CONNECTING","wsUrl","concat","encodeURIComponent","ws","binaryType","initialMessage","models","face","prosody","data","JSON","stringify","send","err","error","Error","message","String","event","_data$predictions","_data$predictions$","_data$predictions$$em","parse","predictions","emotions","length","topEmotion","reduce","a","b","score","name","toLowerCase","code","reason","wasClean","attemptReconnect","delay","Math","min","pow","window","setTimeout","sendVideoFrame","useCallback","async","buffer","binary","bytes","Uint8Array","len","byteLength","i","fromCharCode","btoa","arrayBufferToBase64","frame","Blob","arrayBuffer","HUME_EMOTION_TO_ARKIT_MAP","Joy","mouthSmileLeft","mouthSmileRight","cheekSquintLeft","cheekSquintRight","eyeSquintLeft","eyeSquintRight","Amusement","Excitement","eyeWideLeft","eyeWideRight","mouthOpen","jawOpen","browInnerUp","Awe","Admiration","eyeLookUpLeft","eyeLookUpRight","Love","browOuterUpLeft","browOuterUpRight","Calmness","mouthClose","Contentment","Interest","Serenity","Anger","browDownLeft","browDownRight","noseSneerLeft","noseSneerRight","mouthShrugUpper","Fear","mouthStretchLeft","mouthStretchRight","Horror","mouthShrugLower","Distress","mouthFrownLeft","mouthFrownRight","Sadness","eyeLookDownLeft","eyeLookDownRight","Disappointment","Tiredness","Boredom","Guilt","mouthPressLeft","mouthPressRight","Surprise","Disgust","mouthUpperUpLeft","mouthUpperUpRight","Contempt","Embarrassment","cheekPuff","Confusion","mouthPucker","Neutral","getTopEmotion","top","mapEmotionsToBlendshapes","finalBlendshapes","forEach","shape","topEmotionBlendshapes","emotion","baseShapes","intensity","Object","fromEntries","entries","map","_ref","key","value","getBlendshapesForSingleEmotion","MASCULINE_TALKING_ANIMATION_FILES","MASCULINE_IDLE_ANIMATION_FILES","useExternalAnimations","animationPaths","useGLTF","preload","gltfResults","useMemo","clips","names","allClips","allNames","Array","isArray","gltf","animations","clip","push","warn","Set","EmotionDrivenAvatarComponentBody","props","ref","_THREE","_THREE2","_THREE3","detectedEmotions","isSpeaking","visemeData","directBlendshapes","humeApiKey","avatarUrl","activeBodyAnimation","onError","onLoad","propDetectedEmotions","emotionBlendshapes","cameraEnabled","talkAnimation","idleAnimation","talkAnimationPaths","idleAnimationPaths","modelRef","useImperativeHandle","latestStreamEmotion","setLatestStreamEmotion","blinkShapes","setBlinkShapes","eyeBlinkLeft","eyeBlinkRight","handleStreamEmotionData","newEmotion","_objectSpread","timestamp","Date","now","onEmotions","blinkTimeoutId","blinkDurationTimeoutId","triggerBlink","nextBlinkDelay","random","initialBlinkDelay","emotionsToProcess","currentEmotion","activeEmotionShapes","blendshapes","compositor","BlendshapeCompositor","emotionMouthReduction","emotionFaceBlending","smoothingFactor","finalEmotionShapes","inputs","visemes","manual","base","composedShapes","compose","some","startsWith","talkClips","talkAnimNames","idleClips","idleAnimNames","allAdditionalClips","uniqueClips","Map","has","set","from","values","currentTalkAnimIndex","setCurrentTalkAnimIndex","currentIdleAnimIndex","setCurrentIdleAnimIndex","currentAnimationName","handleModelLoaded","loadedModel","THREE","REVISION","Cache","CanvasTexture","_jsxs","Canvas","camera","position","fov","style","touchAction","shadows","gl","antialias","alpha","toneMapping","toneMappingExposure","outputColorSpace","children","_jsx","OrbitControls","enableRotate","enablePan","enableZoom","autoRotate","SimulationAvatar3D","animationClips","emotionShapes","visemeShapes","scale","onModelLoaded","displayName","ForwardedEmotionDrivenAvatar","React"],"sourceRoot":""}