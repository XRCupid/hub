{"version":3,"file":"static/js/4745.18d0ccfd.chunk.js","mappings":"oMASO,MAAMA,EAAgDC,IAItD,IAJuD,cAC5DC,EAAa,cACbC,EAAa,cACbC,GACDH,EACC,MAAOI,EAAYC,IAAiBC,EAAAA,EAAAA,WAAS,IACtCC,EAAYC,IAAiBF,EAAAA,EAAAA,WAAS,IACtCG,EAASC,IAAcJ,EAAAA,EAAAA,UAA6B,MAErDK,GAAuBC,EAAAA,EAAAA,cAAY,KACvC,GAAKR,EAoDHS,QAAQC,IAAI,oCACRL,GACFA,EAAQM,OAEVL,EAAW,MACXL,GAAc,GACdG,GAAc,OA1DC,CAEfK,QAAQC,IAAI,0DAEZ,MAAME,EAAM,IAAIC,EAAAA,IAAaC,IAY3B,GAXAL,QAAQC,IAAI,sCAA6BI,GACrCjB,GACFA,EAAciB,GAIZA,EAAQC,SAAWjB,GACrBA,EAAcgB,EAAQC,SAIpBD,EAAQE,UAAYjB,EAAe,CAErC,MAAMkB,EAAUH,EAAQE,SAASE,QAAO,CAACC,EAAUC,KACjD,GAAkB,YAAdA,EAAKC,KAEP,OAAQD,EAAKE,OACX,IAAK,KACL,IAAK,KACHH,EAAII,QAAU,EACd,MACF,IAAK,KACL,IAAK,KACHJ,EAAIK,YAAc,GAClB,MACF,IAAK,KACL,IAAK,KACHL,EAAIM,eAAiB,GACrBN,EAAIO,gBAAkB,GACtB,MACF,QACEP,EAAII,QAAUI,KAAKC,IAAIT,EAAII,SAAW,EAAG,GAG/C,OAAOJ,IACN,CAAC,GAEJpB,EAAckB,EAChB,KAGFX,EAAWM,GACXA,EAAIiB,QACJ5B,GAAc,GACdG,GAAc,EAChB,IAUC,CAACJ,EAAYK,EAASR,EAAeC,EAAeC,IAEjD+B,GAAoBtB,EAAAA,EAAAA,cAAauB,IACrC,GAAIjC,EAAe,CACjB,MAAMkC,EAAaC,EAAAA,GAAgBF,GAC/BC,IACFvB,QAAQC,IAAI,qCAADwB,OAA4BH,GAAiBC,GACxDlC,EAAckC,GAElB,IACC,CAAClC,IAEJ,OACEqC,EAAAA,EAAAA,MAAA,OAAKC,MAAO,CACVC,SAAU,QACVC,IAAK,OACLC,KAAM,OACNC,WAAY,kBACZC,MAAO,QACPC,QAAS,OACTC,aAAc,MACdC,OAAQ,IACRC,SAAU,OACVC,MAAO3C,EAAa,QAAU,QAC9B4C,UAAW5C,EAAa,OAAS,OACjC6C,UAAW7C,EAAa,OAAS,SACjC8C,WAAY,gBACZC,OAAQ,mCACRC,SAAA,EACAhB,EAAAA,EAAAA,MAAA,OAAKC,MAAO,CAAEgB,QAAS,OAAQC,WAAY,SAAUC,eAAgB,gBAAiBC,aAAc,OAAQJ,SAAA,EAC1GK,EAAAA,EAAAA,KAAA,MAAIpB,MAAO,CAAEqB,OAAQ,IAAKZ,SAAU,QAASM,SAAC,2BAC7CnD,IACCwD,EAAAA,EAAAA,KAAA,UACEE,QAASA,IAAMtD,GAAeD,GAC9BiC,MAAO,CACLI,WAAY,cACZU,OAAQ,kCACRT,MAAO,QACPC,QAAS,UACTC,aAAc,MACdgB,OAAQ,UACRd,SAAU,QACVM,SAEDhD,EAAa,SAAM,eAK1BqD,EAAAA,EAAAA,KAAA,UACEE,QAASnD,EACT6B,MAAO,CACLI,WAAYxC,EAAa,UAAY,UACrCyC,MAAO,QACPS,OAAQ,OACRR,QAAS,WACTC,aAAc,MACdgB,OAAQ,UACRb,MAAO,OACPD,SAAU,OACVU,aAAcvD,GAAcG,EAAa,OAAS,KAClDgD,SAEDnD,EAAa,oBAAY,4BAG3BA,GAAcG,IACbgC,EAAAA,EAAAA,MAAA,OAAAgB,SAAA,EACEK,EAAAA,EAAAA,KAAA,KAAGpB,MAAO,CAAEqB,OAAQ,QAASZ,SAAU,OAAQe,WAAY,QAAST,SAAC,iCAGrEK,EAAAA,EAAAA,KAAA,OAAKpB,MAAO,CAAEgB,QAAS,OAAQS,oBAAqB,UAAWC,IAAK,MAAOP,aAAc,OAAQJ,SAC9FY,OAAOC,KAAK/B,EAAAA,IAAiBgC,KAAIC,IAChC/B,EAAAA,EAAAA,MAAA,UAEEuB,QAASA,IAAM5B,EAAkBoC,GACjC9B,MAAO,CACLI,WAAY,OACZC,MAAO,QACPS,OAAQ,kCACRR,QAAS,UACTC,aAAc,MACdgB,OAAQ,UACRd,SAAU,OACVsB,cAAe,aACflB,WAAY,mBAEdmB,aAAeC,GAAMA,EAAEC,cAAclC,MAAMI,WAAa,OACxD+B,aAAeF,GAAMA,EAAEC,cAAclC,MAAMI,WAAa,OAAOW,SAAA,CAE9DqB,EAAgBN,GAAY,IAAEA,IAhB1BA,QAqBX/B,EAAAA,EAAAA,MAAA,OAAKC,MAAO,CAAES,SAAU,MAAO4B,QAAS,GAAKC,WAAY,MAAOC,UAAW,kCAAmCC,WAAY,OAAQzB,SAAA,EAChIK,EAAAA,EAAAA,KAAA,UAAAL,SAAQ,4BAAqBK,EAAAA,EAAAA,KAAA,SAAK,WAChCA,EAAAA,EAAAA,KAAA,UAAAL,SAAQ,SAAa,6BAAyBK,EAAAA,EAAAA,KAAA,SAAK,WACnDA,EAAAA,EAAAA,KAAA,UAAAL,SAAQ,WAAe,qCAAiCK,EAAAA,EAAAA,KAAA,SAAK,WAC7DA,EAAAA,EAAAA,KAAA,UAAAL,SAAQ,SAAa,2BAAuBK,EAAAA,EAAAA,KAAA,SAAK,WACjDA,EAAAA,EAAAA,KAAA,UAAAL,SAAQ,cAAkB,+BAA2BK,EAAAA,EAAAA,KAAA,SAAK,WAC1DA,EAAAA,EAAAA,KAAA,UAAAL,SAAQ,UAAc,8BAA0BK,EAAAA,EAAAA,KAAA,SAAK,WACrDA,EAAAA,EAAAA,KAAA,UAAAL,SAAQ,aAAiB,+BAKhCnD,IAAeG,IACdqD,EAAAA,EAAAA,KAAA,OAAKpB,MAAO,CAAES,SAAU,OAAQ4B,QAAS,GAAKI,UAAW,OAAQ1B,SAAC,wCAQ1E,SAASqB,EAAgBM,GAWvB,MAVyC,CACvCC,MAAO,eACPC,IAAK,eACLC,MAAO,eACPC,UAAW,eACXC,OAAQ,eACRC,UAAW,eACXC,SAAU,eACVC,QAAS,gBAEKR,IAAY,cAC9B,C,6QC5KA,MAAMS,EAAkD,CACpD,GAAM,SACN,GAAM,OACN,GAAM,SACN,GAAM,SACN,GAAM,UACN,GAAM,UACN,EAAM,IACN,GAAM,UACN,EAAM,IACN,GAAM,OACN,GAAM,SACN,GAAM,SACN,GAAM,UACN,EAAM,IACN,EAAM,IACN,GAAM,IACN,GAAM,SACN,GAAM,IACN,GAAM,UACN,EAAM,IACN,EAAM,IACN,EAAM,IACN,EAAM,IACN,GAAM,SACN,GAAM,UACN,GAAM,eACN,EAAM,IACN,EAAM,SACN,EAAM,IACN,GAAM,SACN,EAAM,IACN,GAAM,SACN,GAAM,SACN,GAAM,IACN,EAAM,IACN,EAAM,IACN,EAAM,IACN,EAAM,IACN,GAAM,SACN,IAAO,UACP,IAAO,UACP,GAAM,UAKJC,EAA0E,CAC5E,CAAEC,WAAY,CAAC,WAAYC,GAAI,GAC/B,CAAED,WAAY,CAAC,OAAK,SAAK,UAAMC,GAAI,GACnC,CAAED,WAAY,CAAC,UAAMC,GAAI,GACzB,CAAED,WAAY,CAAC,UAAMC,GAAI,GACzB,CAAED,WAAY,CAAC,SAAK,UAAMC,GAAI,GAC9B,CAAED,WAAY,CAAC,UAAMC,GAAI,GACzB,CAAED,WAAY,CAAC,IAAK,IAAK,UAAMC,GAAI,GACnC,CAAED,WAAY,CAAC,IAAK,KAAMC,GAAI,GAC9B,CAAED,WAAY,CAAC,KAAMC,GAAI,GACzB,CAAED,WAAY,CAAC,WAAOC,GAAI,GAC1B,CAAED,WAAY,CAAC,gBAAOC,GAAI,IAC1B,CAAED,WAAY,CAAC,WAAOC,GAAI,IAC1B,CAAED,WAAY,CAAC,KAAMC,GAAI,IACzB,CAAED,WAAY,CAAC,UAAMC,GAAI,IACzB,CAAED,WAAY,CAAC,KAAMC,GAAI,IACzB,CAAED,WAAY,CAAC,IAAK,KAAMC,GAAI,IAC9B,CAAED,WAAY,CAAC,SAAK,UAAM,UAAM,UAAMC,GAAI,IAC1C,CAAED,WAAY,CAAC,QAAMC,GAAI,IACzB,CAAED,WAAY,CAAC,IAAK,KAAMC,GAAI,IAC9B,CAAED,WAAY,CAAC,IAAK,IAAK,IAAK,UAAMC,GAAI,IACxC,CAAED,WAAY,CAAC,IAAK,IAAK,UAAMC,GAAI,IACnC,CAAED,WAAY,CAAC,IAAK,IAAK,KAAMC,GAAI,KAwChC,SAASC,EACZC,EACAC,GAEA,MAAMC,EAA8B,GAC9BC,EAAkBpE,KAAKqE,MAA6B,IAAvBH,GAEnC,OAAKD,GAAwC,IAAxBA,EAAaK,QAKlCL,EAAaM,SAAQ,CAACC,EAAWC,KAC7B,MAAMC,EAAiBF,EAAUG,QAAQC,cAKnCC,EAvDd,SAAiCC,GAC7B,GAAkB,OAAdA,EAEA,OADAhG,QAAQiG,KAAK,6FACN,EAEX,GAAkB,YAAdD,EAAyB,OAAO,EAEpC,IAAK,MAAME,KAASnB,EAChB,GAAImB,EAAMlB,WAAWmB,SAASH,GAC1B,OAAOE,EAAMjB,GAKrB,MAAkB,YAAde,EAA2B,EACb,YAAdA,EAA2B,GAE/BhG,QAAQiG,KAAK,4CAADxE,OAA6CuE,EAAS,yCAC3D,EACX,CAoCyBI,MAJsCC,IAArCvB,EAAiBc,GACbd,EAAiBc,GACjB,MAGhBU,EAAmBpF,KAAKqE,MAAuB,IAAjBG,EAAUa,MAE9ClB,EAAamB,KAAK,CACdC,YAAaH,EACbP,SAAUA,EACVW,aAAcf,IAAUR,EAAaK,OAAS,OAKtDH,EAAasB,MAAK,CAACC,EAAGC,IAAMD,EAAEH,YAAcI,EAAEJ,cAM9CzG,QAAQC,IAAI,+DAAgEoF,GAC5ErF,QAAQC,IAAI,0CAA2CqF,GAEhD,CAAED,eAAcC,qBA9BnBtF,QAAQiG,KAAK,6EACN,CAAEZ,eAAcC,mBA8B/B,CASA,IAAIwB,EAEG,MAeMC,EAA8BC,eAEzCC,GAEkC,IADlCC,EAAiBC,UAAA3B,OAAA,QAAAa,IAAAc,UAAA,GAAAA,UAAA,GAAG,oBAGpB,OADAnH,QAAQC,IAAI,kEAADwB,OAAmEwF,EAAI,eAAAxF,OAAcyF,EAAS,kBAAAzF,OAAiB2F,KAAKC,QACxH,IAAIC,SAAQ,CAACC,EAASC,KAC3BxH,QAAQC,IAAI,gFAADwB,OAAiFwF,EAAI,kBAAAxF,OAAiB2F,KAAKC,QAEtH,MAAMI,EAAYC,IAAAA,IAAYC,2BACxBC,EAAeF,IAAAA,IAAYG,8BAEjC,IAAKJ,IAAcG,EAIjB,OAHA5H,QAAQ8H,MAAM,4DACd9H,QAAQC,IAAI,qDAADwB,OAAsDwF,EAAI,kBAAAxF,OAAiB2F,KAAKC,aAC3FG,EAAO,IAAIO,MAAM,+CAInB,IACEjB,EAAekB,EAAAA,aAAuBC,iBAAiBR,EAAWG,GAClE5H,QAAQC,IAAI,uEAAwE6G,GACpFA,EAAaoB,4BAA8BF,EAAAA,4BAAsCG,4BAGjF,MAAMC,EAAI,mKAAA3G,OAESyF,EAAS,yEAAAzF,OAEpBwF,EAAI,0CAIZ,IAAIoB,EAAc,IAAIL,EAAAA,kBAA4BlB,EAAc,MAChE9G,QAAQC,IAAI,uFAAwFmI,GAEpG,IAAIE,EAAkC,CAAEC,gBAAiB,GAAIC,iBAAkB,IAC3EC,GAAiB,EAErB,MAAMC,EAAUA,KACd,GAAIL,EAAa,CACf,IACEA,EAAYM,OACd,CAAE,MAAOC,GACP5I,QAAQ8H,MAAM,wDAAyDc,EACzE,CACAP,EAAc,IAChB,GAGFA,EAAYQ,eAAiB,CAACC,EAAGlF,KAE/B,GADA5D,QAAQC,IAAI,qDAADwB,OAAsDmC,EAAE6C,YAAc,IAAK,kBAAAhF,OAAiBmC,EAAEmC,WACrGnC,EAAEmF,WAAoC,KAAvBnF,EAAEmF,UAAUC,OAAe,CAC5ChJ,QAAQC,IAAI,uDAADwB,OAAwDmC,EAAEmF,UAAUvD,SAC/ExF,QAAQC,IAAI,kEAADwB,OAAmEmC,EAAEmF,UAAUE,UAAU,EAAG,OAGvG,IACE,MAAMC,EAAgBC,KAAKC,MAAMxF,EAAEmF,WACnC,GAAIG,EAAcG,kBAA4ChD,IAA7B6C,EAAcI,WAA0B,CACvE,MAAMC,EAAiBL,EAAcI,WACrCJ,EAAcG,YAAY5D,SAAQ,CAAC+D,EAAsBC,KACvDnB,EAAoBE,iBAAiBhC,KAAK,CACxCkD,WAAYH,EAAiBE,EAC7BE,OAAQH,EACR/C,YAAc7C,EAAE6C,YAAc,IAAUgD,GAAmB,KAAQP,EAAcU,WAAa,SAGpG,MACEtB,EAAoBC,gBAAgB/B,KAAK,CACvCqD,SAAUjG,EAAEmC,SACZU,YAAa7C,EAAE6C,YAAc,KAGnC,CAAE,MAAOqB,GACP9H,QAAQ8H,MAAM,kEAAmEA,EAAO,+BAAgClE,EAAEmF,WAC1HT,EAAoBC,gBAAgB/B,KAAK,CACvCqD,SAAUjG,EAAEmC,SACZU,YAAa7C,EAAE6C,YAAc,KAEjC,CACF,MACEzG,QAAQC,IAAI,sEACZqI,EAAoBC,gBAAgB/B,KAAK,CACvCqD,SAAUjG,EAAEmC,SACZU,YAAa7C,EAAE6C,YAAc,OAKnC4B,EAAYyB,eACV1B,GACA2B,IACE,GAAItB,EACFC,QADF,CAIA,GADAD,GAAiB,EACbsB,EAAOC,SAAWhC,EAAAA,aAAuBiC,2BAA4B,CACvE3B,EAAoBE,iBAAiB7B,MAAK,CAACC,EAAGC,IAAMD,EAAE8C,WAAa7C,EAAE6C,aACrEpB,EAAoBC,gBAAgB5B,MAAK,CAACC,EAAEC,IAAMD,EAAEH,YAAcI,EAAEJ,cACpEzG,QAAQC,IAAI,+CAAgD8J,EAAOG,eACnE,MAAM5E,EAAkByE,EAAOG,cAAgB,IAC/ClK,QAAQC,IAAI,+DAADwB,OAAgEsI,EAAOG,cAAa,qBAAAzI,OAAoB6D,EAAe,OAClItF,QAAQC,IAAI,gDAADwB,OAAiDwF,EAAI,wBAAAxF,OAAuB6D,EAAe,iBAAA7D,OAAgB2F,KAAKC,QAC3HE,EAAQ,CAAE4C,UAAWJ,EAAOI,UAAWC,WAAY9B,EAAqBhD,gBAAiBA,GAC3F,MAAO,GAAIyE,EAAOC,SAAWhC,EAAAA,aAAuBqC,SAAU,CAC5D,MAAMC,EAAetC,EAAAA,oBAA8BuC,WAAWR,GACxDS,OAAqCnE,IAAxBiE,EAAaN,OAAuBhC,EAAAA,mBAA6BsC,EAAaN,QAAU,gBAC3GhK,QAAQ8H,MAAM,sDAADrG,OAAuD+I,EAAU,eAAA/I,OAAc6I,EAAaG,aAAY,iBAAAhJ,OAAgB6I,EAAaI,YAClJ1K,QAAQC,IAAI,gDAADwB,OAAiDwF,EAAI,eAAAxF,OAAc+I,EAAU,iBAAA/I,OAAgB2F,KAAKC,QAC7GG,EAAO,uBAAD/F,OAAwB+I,EAAU,eAAA/I,OAAc6I,EAAaG,aAAY,iBAAAhJ,OAAgB6I,EAAaI,WAC9G,MACE1K,QAAQ8H,MAAM,oDAADrG,OAAqDsI,EAAOC,OAAM,eAAAvI,OAAcsI,EAAOU,eACpGzK,QAAQC,IAAI,qDAADwB,OAAsDwF,EAAI,eAAAxF,OAAcsI,EAAOC,OAAM,iBAAAvI,OAAgB2F,KAAKC,QACrHG,EAAO,IAAIO,MAAM,qBAADtG,OAAsBsI,EAAOU,cAAgB,mBAE/D/B,GArBA,KAuBFZ,IACMW,IAGJA,GAAiB,EACjBzI,QAAQC,IAAI,sDAADwB,OAAuDwF,EAAI,cAAAxF,OAAaqG,EAAK,iBAAArG,OAAgB2F,KAAKC,QAC7GG,EAAOM,IAJLY,MAQR,CAAE,MAAOZ,GACP9H,QAAQ8H,MAAM,+DAAgEA,GAC9E9H,QAAQC,IAAI,mDAADwB,OAAoDwF,EAAI,cAAAxF,OAAaqG,EAAK,iBAAArG,OAAgB2F,KAAKC,QAC1GG,EAAOM,EACT,IAEJ,E,eC/PA,MAyBM6C,EAAyCC,EAAAA,EAAyBnK,QAAO,CAACC,EAAKmK,KACnFnK,EAAImK,GAAa,EACVnK,IACN,CAAC,GAGEoK,EAAgF,CACpF,EAAG,CAAEhK,QAAS,EAAGiK,WAAY,EAAGC,YAAa,EAAGjK,YAAa,EAAGC,eAAgB,EAAGC,gBAAiB,GACpG,EAAG,CAAEH,QAAS,GAAKmK,gBAAiB,IACpC,EAAG,CAAEnK,QAAS,GAAKC,YAAa,IAChC,EAAG,CAAED,QAAS,GAAKC,YAAa,GAAKiK,YAAa,IAClD,EAAG,CAAElK,QAAS,GAAKE,eAAgB,GAAKC,gBAAiB,IACzD,EAAG,CAAEH,QAAS,IAAMmK,gBAAiB,GAAKC,SAAU,IACpD,EAAG,CAAEpK,QAAS,GAAKE,eAAgB,GAAKC,gBAAiB,IACzD,EAAG,CAAEH,QAAS,IAAMkK,YAAa,GAAKjK,YAAa,IACnD,EAAG,CAAED,QAAS,GAAKkK,YAAa,GAAKjK,YAAa,IAClD,EAAG,CAAED,QAAS,GAAKC,YAAa,GAAKiK,YAAa,IAClD,GAAI,CAAElK,QAAS,GAAKC,YAAa,GAAKC,eAAgB,IACtD,GAAI,CAAEF,QAAS,GAAKE,eAAgB,GAAKC,gBAAiB,IAC1D,GAAI,CAAEH,QAAS,GAAKmK,gBAAiB,KACrC,GAAI,CAAEnK,QAAS,GAAKkK,YAAa,GAAKE,SAAU,IAChD,GAAI,CAAEpK,QAAS,IAAMoK,SAAU,GAAKlK,eAAgB,IACpD,GAAI,CAAEF,QAAS,IAAMiK,WAAY,GAAK/J,eAAgB,GAAKC,gBAAiB,IAC5E,GAAI,CAAEH,QAAS,IAAMkK,YAAa,GAAKjK,YAAa,IACpD,GAAI,CAAED,QAAS,GAAKoK,SAAU,GAAKC,mBAAoB,GAAKC,oBAAqB,IACjF,GAAI,CAAEtK,QAAS,IAAMqK,mBAAoB,GAAKC,oBAAqB,GAAKC,eAAgB,GAAKC,gBAAiB,IAC9G,GAAI,CAAExK,QAAS,GAAKoK,SAAU,GAAKH,WAAY,IAC/C,GAAI,CAAEjK,QAAS,GAAKyK,gBAAiB,GAAKL,SAAU,IACpD,GAAI,CAAEpK,QAAS,EAAGiK,WAAY,EAAGC,YAAa,MAI1CQ,EAAoCZ,EAAAA,EAAyBnK,QAAO,CAACC,EAAoB+K,KAC7F/K,EAAI+K,GAAO,EACJ/K,IACN,CAAC,GAGEgL,EAA0Cd,EAAAA,EAAyBnK,QAAO,CAACC,EAA6B+K,KAChG,iBAARA,GAAkC,kBAARA,IAE5B/K,EAAI+K,GAAO,GAEN/K,IACN,CAAC,GAuGJ,MAAMiL,EAA0B,CAC9B,2CACA,2CACA,2CACA,2CACA,2CACA,2CACA,2CACA,2CACA,2CACA,2CACA,wBAGIC,EAA0B,CAC9B,sCACA,sCACA,iDACA,iDACA,iDACA,iDACA,iDACA,iDACA,iDACA,iDACA,iDACA,iDACA,wBAMIC,EAAmC,CACvC9J,WAAY,kBACZU,OAAQ,kBACRP,aAAc,MACdD,QAAS,WACTD,MAAO,QACPkB,OAAQ,UACRd,SAAU,QACVY,OAAQ,OAGJ8I,EAAyBC,EAAAA,YAG7B,CAACC,EAAOC,KACR,MAAM,aACJC,EAAY,qBACZC,EAAoB,gBACpBC,EAAe,mBACfC,EAAkB,gBAClBC,EAAe,aACfC,EAAY,aACZC,GACER,EAEJhM,QAAQC,IAAI,+DAADwB,OAAgEuK,EAAMK,qBAEjFrM,QAAQC,IAAI,gEAAiE+L,EAAMK,mBAAoB,mBAAsD,kBAA1BL,EAAMI,iBAAgCK,EAAAA,GAAgBT,EAAMI,iBAAoBK,EAAAA,GAAgBT,EAAMI,iBAAmBM,OAAOV,EAAMI,kBAEzQ,MAAM,YAAEO,IAAgBC,EAAAA,EAAAA,MAClB,KAAEC,EAAMC,QAASC,EAAajF,MAAOkF,IAAcC,EAAAA,EAAAA,OAEnD,aAAEC,KADSC,EAAAA,EAAAA,OACQC,EAAAA,EAAAA,OA6BlBC,EAAOC,IAPDtB,EAAMuB,iBAAsB,OAAJV,QAAI,IAAJA,GAAAA,EAAMW,YAOjB/N,EAAAA,EAAAA,WAA8B,KACtD,MAAMgO,EAAU,CACdC,SAAS,EACTC,YAAY,EACZC,WAAW,EACXC,YAAY,EACZC,SAAU,GACVC,WAAY,GACZC,YAAY,EACZC,oBAAoB,EACpBC,mBAAmB,EACnBC,eAAgB,UAChBC,cAAe,kBACfC,cAAe,OACfC,eAAgB,kBAChBC,oBAAqB7C,EACrB8C,kBAAmBhD,EACnBiD,eAAgB,KAChB3G,MAAO,KACP4G,WAAW,EACXlB,UAAWxB,EAAMuB,iBAAsB,OAAJV,QAAI,IAAJA,OAAI,EAAJA,EAAMW,YAAa,WACtDmB,iBAAiB,EACjBC,YAAY,EACZC,UAAU,EACVC,yBAA0BnE,GAG5B,OADA3K,QAAQC,IAAI,sEAADwB,OAAuEgM,EAAQD,UAAS,kCAAA/L,OAAiCuK,EAAMuB,eAAc,uBAAA9L,OAA0B,OAAJoL,QAAI,IAAJA,OAAI,EAAJA,EAAMW,YAC7KC,MAIHsB,GAAWC,EAAAA,EAAAA,QAAO3B,GAGlB4B,GAAoBlP,EAAAA,EAAAA,cAAasE,IAErCiJ,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAEf,eAAgB9J,EAAQ+K,KAAKC,oBACzD,CAAC/B,IAGEgC,EAAa5H,oDAGjB6H,eAAgBC,EAChBC,gBAAiBC,EACjBC,UAAWC,IACTC,EAAAA,EAAAA,GACFP,EACAL,EACA,CACEa,yBAA0CzC,EAAMM,WAChDoC,UAAW1C,EAAMM,cAIrBqC,EAAAA,EAAAA,YAAU,KACRhQ,QAAQC,IAAI,gDAAiDyP,EAAkC,cAAeE,GAC1GA,GACFtC,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAEd,cAAc,yBAAD3M,OAA2BmO,SAEtE,CAACF,EAAkCE,IAGtC,MAAMK,GAAgBjB,EAAAA,EAAAA,QAAgC,MAChDkB,GAAelB,EAAAA,EAAAA,QAAgC,MAC/CmB,GAAkBnB,EAAAA,EAAAA,QAA4B,IAC9CoB,GAAsBpB,EAAAA,EAAAA,QAAsB,MAC5CqB,GAA2BrB,EAAAA,EAAAA,QAA8B,MACzDsB,GAAwCtB,EAAAA,EAAAA,QAAoB,MAC5DuB,GAAqCvB,EAAAA,EAAAA,QAAsB,MAC3DwB,GAAoBxB,EAAAA,EAAAA,SAAgB,GAEpCyB,IADuBzB,EAAAA,EAAAA,QAAqC,OACpCA,EAAAA,EAAAA,QAAOhD,EAAMK,sBAG3C2D,EAAAA,EAAAA,YAAU,KACRjB,EAAS2B,QAAUrD,IAClB,CAACA,KAEJ2C,EAAAA,EAAAA,YAAU,KAGRhQ,QAAQC,IAAI,8DAADwB,OAA+D4L,EAAMG,UAAS,wCAAA/L,OAAuCuK,EAAMuB,eAAc,+BAAA9L,OAAkC,OAAJoL,QAAI,IAAJA,OAAI,EAAJA,EAAMW,cACvL,CAACH,EAAMG,UAAWxB,EAAMuB,eAAgBV,KAE3CmD,EAAAA,EAAAA,YAAU,KACRhQ,QAAQC,IAAI,8EAADwB,OAA+EuK,EAAMuB,eAAc,+BAAA9L,OAA8B4L,EAAMG,YAC3I,KAELxN,QAAQC,IAAI,sFAADwB,OAAuFuK,EAAMuB,eAAc,kCAAA9L,OAAiCsN,EAAS2B,QAAQlD,eAGzK,IAEH,MAAMmD,IAAsB5Q,EAAAA,EAAAA,cAAa+H,IACvCwF,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAEd,cAAc,iBAAD3M,OAAmBqG,EAAMzH,eAClE,IAEGuQ,IAAqB7Q,EAAAA,EAAAA,cAAY,KACrCuN,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAEd,cAAe,sBAC3C,KAGH4B,EAAAA,EAAAA,YAAU,KACRS,EAAsBC,QAAU1E,EAAMK,mBACtCrM,QAAQC,IAAI,mFAADwB,OAAoFgP,EAAsBC,QAAO,8BAAAjP,OAA6BuK,EAAMK,mBAAkB,QAChL,CAACL,EAAMK,qBAGV,MAAMwE,IAAiB7B,EAAAA,EAAAA,QAA8B,MAC/C8B,IAAiB9B,EAAAA,EAAAA,QAA2B,MAC5C+B,IAAqB/B,EAAAA,EAAAA,SAAegC,EAAAA,EAAAA,MAOpCC,IAAsBlR,EAAAA,EAAAA,cAAY,KAClCqQ,EAAoBM,UACtBQ,qBAAqBd,EAAoBM,SACzCN,EAAoBM,QAAU,MAGhCpD,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAEX,oBAAqB7C,EAAiBsC,YAAY,MAC/EhO,QAAQC,IAAI,wHACX,CAACqN,EAAU8C,EAAqB5E,IAE7B2F,IAAsBpR,EAAAA,EAAAA,cAAaqK,IAEvC,GADApK,QAAQC,IAAI,mEAAoEmK,IAC3EA,IAAeA,EAAW5B,kBAA2D,IAAvC4B,EAAW5B,iBAAiBhD,OAE7E,OADAxF,QAAQiG,KAAK,yEACN,GAGT,MAAMmL,EAA8BhH,EAAW5B,iBAAiBhF,KAAI6N,IAClE,MAAM1H,EAAiC,CAAC,EAIxC,OAHAiB,EAAAA,EAAyBnF,SAAQ,CAAC2J,EAAqBzJ,KACrDgE,EAAOyF,GAAQiC,EAAM1H,OAAOhE,MAEvB,CACLY,KAAM8K,EAAM5K,YACZkD,OAAQA,MAIZ,OADA3J,QAAQC,IAAI,0CAADwB,OAA2C2P,EAAO5L,OAAM,gBAC5D4L,IACN,IAEGE,IAAuCvR,EAAAA,EAAAA,cAAY,CACvDsF,EACAC,KAGA,GADAtF,QAAQC,IAAI,8DAADwB,OAA+D4D,EAAaG,OAAM,8BAAA/D,OAA6B6D,EAAe,QACpID,GAAwC,IAAxBA,EAAaG,OAChC,OAAIF,EAAkB,GAEpBtF,QAAQC,IAAI,+FACL,CACL,CAAEsG,KAAM,EAAGoD,OAAQmB,EAAsC,IAAMU,GAC/D,CAAEjF,KAAMjB,EAAkB,IAAMqE,OAAQmB,EAAsC,IAAMU,MAGxFxL,QAAQC,IAAI,+GACL,IAGT,MAAMsR,EAAiClM,EAAa7B,KAAIgO,IAAK,CAC3DjL,KAAMiL,EAAM/K,YAAc,IAC1BkD,OAAQmB,EAAsC0G,EAAMzL,WAAayF,MAI7DiG,EAA0BF,EAAU/L,OAAS,EAAI+L,EAAUA,EAAU/L,OAAS,GAAGe,KAAO,EACxFnB,EAAuBE,EAAkB,IAa/C,OAXyB,IAArBiM,EAAU/L,QAAgBiM,EAA0BrM,KAE3B,IAArBmM,EAAU/L,QAAgBJ,EAAuB,GACjDmM,EAAU/K,KAAK,CAAED,KAAM,EAAGoD,OAAQmB,EAAsC,IAAMU,IAElF+F,EAAU/K,KAAK,CACXD,KAAMnB,EACNuE,OAAQmB,EAAsC,IAAMU,KAG5DxL,QAAQC,IAAI,4DAADwB,OAA6D8P,EAAU/L,OAAM,gBACjF+L,IACN,CAAC/F,IAEEkG,IAAiB3R,EAAAA,EAAAA,cAAa4R,IAClC,MAAMC,GAAexK,KAAKC,MAAQsK,GAAa,IACzCE,EAAgB1B,EAAgBO,QAEtC,IAAKmB,GAA0C,IAAzBA,EAAcrM,OAGlC,OAFAxF,QAAQC,IAAI,0EACZgR,KAIF,IAAIa,EAAc,KAClB,IAAK,IAAIC,EAAI,EAAGA,EAAIF,EAAcrM,QAC5BoM,GAAeC,EAAcE,GAAGxL,KADIwL,IAEtCD,EAAcD,EAAcE,GAM5BD,IACFxE,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAEX,oBAAqBuD,EAAYnI,WAC9D3J,QAAQC,IAAI,mEAAoE6R,EAAYnI,SAI9F,GAAIiI,GADkBC,EAAcrM,OAAS,EAAIqM,EAAcA,EAAcrM,OAAS,GAAGe,KAAO,GAC9D,KAASuL,GAAeF,EAAc,IAAOC,EAAcrM,OAAS,EAGpG,OAFAxF,QAAQC,IAAI,6FACZgR,KAIFb,EAAoBM,QAAUsB,uBAAsB,IAAMN,GAAeC,OACxE,CAACrE,EAAU2D,GAAqBd,EAAiBC,IAE9C6B,IAAuBlS,EAAAA,EAAAA,cAAY,KAEvC,GADAC,QAAQC,IAAI,sDAAuDkQ,EAAgBO,QAAQlL,QACvF2K,EAAgBO,SAAWP,EAAgBO,QAAQlL,OAAS,EAAG,CACjExF,QAAQC,IAAI,wEAAyEkQ,EAAgBO,SACrG,MAAMiB,EAAYvK,KAAKC,MACnB+I,EAAoBM,SACtBQ,qBAAqBd,EAAoBM,SAE3CgB,GAAeC,EACjB,MACE3R,QAAQC,IAAI,+DACZgR,OAED,CAACS,GAAgBT,GAAqBb,EAAqBD,IAiExD+B,KA/D4BnS,EAAAA,EAAAA,cAAYiH,MAAOmL,EAAkB5R,KACrEP,QAAQC,IAAI,2EACZqN,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAEjB,oBAAoB,EAAOC,mBAAmB,MAGvE+B,EAAcS,UAChBT,EAAcS,QAAQ0B,QACtBnC,EAAcS,QAAQ2B,IAAM,IAE9BpB,KAEKf,EAAaQ,UAChBR,EAAaQ,QAAU,IAAI4B,MAC3BpC,EAAaQ,QAAQ6B,iBAAiB,SAAS,KAC7CvS,QAAQC,IAAI,uCACZgR,KACA3D,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAElB,YAAY,EAAOE,mBAAmB,SAErEgC,EAAaQ,QAAQ6B,iBAAiB,SAAU3O,IAC9C5D,QAAQ8H,MAAM,4CAA6ClE,GAC3DqN,KACA3D,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAElB,YAAY,EAAOE,mBAAmB,UAIvEgC,EAAaQ,QAAQ2B,IAAMF,EAC3BjC,EAAaQ,QAAQ8B,OAErB,UACQtC,EAAaQ,QAAQ+B,OAC3BzS,QAAQC,IAAI,yCACZqN,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAElB,YAAY,MAGzC,MAAM5I,EAAuB8K,EAAaQ,QAAQgC,UAC9CC,MAAMvN,IAAkD,IAAzBA,IACjCpF,QAAQiG,KAAK,yHAKfjG,QAAQC,IAAI,4DAADwB,OAA6D2D,EAAoB,MAG5F,MAAMwN,EAAY1N,EAAkC3E,EAAU6E,GAGxDmM,EAAYD,GAAqCsB,EAAUvN,aAAcuN,EAAUtN,iBAErFiM,GAAaA,EAAU/L,OAAS,GAClC2K,EAAgBO,QAAUa,EAC1BU,MAEAjS,QAAQC,IAAI,uGAGhB,CAAE,MAAO6H,GACP9H,QAAQ8H,MAAM,sFAAuFA,GACrGmJ,KACA3D,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAElB,YAAY,EAAOE,mBAAmB,KACrE,IACC,CAAC+C,GAAqBgB,GAAsBX,GAAsC9F,EAAoBkG,MAEvE3R,EAAAA,EAAAA,cAAaM,IAC7CL,QAAQC,IAAI,2EACZqN,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAElB,YAAY,EAAMI,cAAe,+BAC7D,CAACd,KAEEuF,IAA4B9S,EAAAA,EAAAA,cAAY,KAC5CC,QAAQC,IAAI,+CACP8O,EAAS2B,QAAQzC,mBAKpBjO,QAAQC,IAAI,0HAJZD,QAAQC,IAAI,iHACZgR,KACA3D,GAAU4B,IAAyBC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAWD,GAAI,IAAElB,YAAY,EAAOI,cAAe,cAIvF,CAAC6C,GAAqB3D,EAAUyB,IAE7B+D,IAAwB/S,EAAAA,EAAAA,cAAY,KACxCC,QAAQC,IAAI,2CACZD,QAAQC,IAAI,6EACZuQ,EAAkBE,SAAU,EAC5BO,KACA3D,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAEjB,oBAAoB,MAC7CgC,EAAcS,SAAWT,EAAcS,QAAQ2B,IAAIU,WAAW,WAChEC,IAAIC,gBAAgBhD,EAAcS,QAAQ2B,KAC1CpC,EAAcS,QAAQ2B,IAAM,GAC5BpC,EAAcS,QAAQwC,UAAY,KAClClT,QAAQC,IAAI,oFAEb,CAACgR,GAAqB3D,EAAU2C,IAE7BkD,IAAwBpT,EAAAA,EAAAA,cAAayR,IACzC,MAAM4B,EAAe5B,EAAM6B,OAC3BrT,QAAQ8H,MAAM,8DAA+DsL,EAAatL,OAC1FmJ,KACA3D,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAEjB,oBAAoB,EAAOG,cAAe,iCACnE6B,EAAcS,SAAWT,EAAcS,QAAQ2B,IAAIU,WAAW,WAChEC,IAAIC,gBAAgBhD,EAAcS,QAAQ2B,KAC1CpC,EAAcS,QAAQ2B,IAAM,GAC5BpC,EAAcS,QAAQwC,UAAY,KAClClT,QAAQC,IAAI,gGAEb,CAACgR,GAAqB3D,EAAU2C,IAG7BqD,IAAoCvT,EAAAA,EAAAA,cAAYiH,MAAOC,EAAcC,KAEzE,GADAlH,QAAQC,IAAI,gFAADwB,OAAiFwF,EAAI,cAAAxF,OAAayF,IACxG+I,EAAcS,QAInB,IAAK,IAAD6C,EACFjG,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAEd,cAAe,qBAAsBJ,YAAY,EAAMC,oBAAoB,MACnGgC,EAAcS,QAAQ8C,SACzBvD,EAAcS,QAAQ0B,QACtBnC,EAAcS,QAAQkB,YAAc,GAEtC,MAAM6B,QAAsBC,EAAmCzM,EAAMC,GAAa6H,EAAS2B,QAAQpC,gBAInG,GADAtO,QAAQC,IAAI,qFAAsFkJ,KAAKwK,UAAUF,EAAe,KAAM,IACrH,OAAbA,QAAa,IAAbA,GAAyB,QAAZF,EAAbE,EAAerJ,kBAAU,IAAAmJ,GAAzBA,EAA2B/K,kBAAoBiL,EAAcrJ,WAAW5B,iBAAiBhD,OAAS,EAAG,CACvGxF,QAAQC,IAAI,4FACZ,IAAK,IAAI8R,EAAI,EAAGA,EAAI7Q,KAAK0S,IAAI,EAAGH,EAAcrJ,WAAW5B,iBAAiBhD,QAASuM,IAAK,CAAC,IAAD8B,EACtF7T,QAAQC,IAAI,WAADwB,OAAYsQ,EAAC,kBAAAtQ,OAAiBgS,EAAcrJ,WAAW5B,iBAAiBuJ,GAAGtL,YAAW,aAAAhF,OAAY0H,KAAKwK,UAA6D,QAApDE,EAACJ,EAAcrJ,WAAW5B,iBAAiBuJ,GAAGpI,cAAM,IAAAkK,OAAA,EAAnDA,EAAqDC,MAAM,EAAE,IAAG,OAC9L,CACA,MAAMC,EAAeN,EAAcrJ,WAAW5B,iBAAiBhD,OAAS,EAChD,IAADwO,EAAvB,GAAID,GAAgB,EAChB/T,QAAQC,IAAI,gBAADwB,OAAiBsS,EAAY,kBAAAtS,OAAiBgS,EAAcrJ,WAAW5B,iBAAiBuL,GAActN,YAAW,aAAAhF,OAAY0H,KAAKwK,UAAwE,QAA/DK,EAACP,EAAcrJ,WAAW5B,iBAAiBuL,GAAcpK,cAAM,IAAAqK,OAAA,EAA9DA,EAAgEF,MAAM,EAAE,IAAG,OAExO,MACE9T,QAAQC,IAAI,6HASd,GAPIwT,GAAiB,oBAAqBA,EACxCzT,QAAQC,IAAI,6EAA8EwT,EAAcnO,iBAExGtF,QAAQC,IAAI,+FAIVwT,GAAiBA,EAActJ,WAAasJ,EAAcrJ,WAAY,CACxE,MAAM6J,EAAiB9C,GAAoBsC,EAAcrJ,YACrD6J,GAAkBA,EAAezO,OAAS,GAC5C2K,EAAgBO,QAAUuD,EAC1BhC,MAEAhB,KAEF,MAAMiD,EAAY,IAAIC,KAAK,CAACV,EAActJ,WAAY,CAAEvJ,KAAM,eACxDuR,EAAWa,IAAIoB,gBAAgBF,GACrCjE,EAAcS,QAAQwC,UAAY,KAClCjD,EAAcS,QAAQ2B,IAAMF,EAC5BlC,EAAcS,QAAQ+B,OAAO4B,OAAMzQ,IACjC5D,QAAQ8H,MAAM,wEAAyElE,GACvFkP,OAEJ,MACE9S,QAAQ8H,MAAM,yFACdgL,IAEJ,CAAE,MAAOhL,GACP9H,QAAQ8H,MAAM,sEAAuEA,GACrFgL,IACF,MAvDE9S,QAAQ8H,MAAM,gFAwDf,CAAC4L,EAAoCvC,GAAqBc,GAAsBhB,GAAqB6B,GAAuBxF,EAAUyB,EAAUkB,EAAeE,IAE5JmE,IAA4BvU,EAAAA,EAAAA,cAAYiH,UAG9C,GAFEhH,QAAQC,IAAI,oDAAqDgH,IAE9DgJ,EAAcS,QAGjB,OAFA1Q,QAAQ8H,MAAM,oGACdgL,KAIFxF,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAElB,YAAY,EAAMC,oBAAoB,EAAMG,cAAe,8BAElF6B,EAAcS,UAAYT,EAAcS,QAAQ8C,SAClDxT,QAAQiG,KAAK,kHACbgK,EAAcS,QAAQ0B,QACtBnC,EAAcS,QAAQkB,YAAc,GAGtC,IAAK,IAAD2C,EAAAC,EAAAC,EAAAC,EAAAC,EACF3U,QAAQC,IAAI,2EAA4EoN,EAAMiB,gBAC9F,MAAMsG,EAAoBvH,EAAMiB,eAE1BmF,QAAsBC,EAAmCzM,EAAM2N,GAGrE,GAFA5U,QAAQC,IAAI,+DAAgEkJ,KAAKwK,UAAUF,EAAgB,CAAEoB,gBAAwC,QAAzBN,EAAEd,EAActJ,iBAAS,IAAAoK,OAAA,EAAvBA,EAAyBO,WAAYC,uBAAgD,QAA1BP,EAAEf,EAAcrJ,kBAAU,IAAAoK,GAAkB,QAAlBC,EAAxBD,EAA0BhM,wBAAgB,IAAAiM,OAAlB,EAAxBA,EAA4CjP,OAAQwP,sBAA+C,QAA1BN,EAAEjB,EAAcrJ,kBAAU,IAAAsK,GAAiB,QAAjBC,EAAxBD,EAA0BnM,uBAAe,IAAAoM,OAAjB,EAAxBA,EAA2CnP,QAAW,OAExTiO,GAAiBA,EAActJ,WAAasJ,EAAcrJ,WAAY,CAAC,IAAD6K,EAAAC,EACxElV,QAAQC,IAAI,8FAAwI,QAA3CgV,EAAExB,EAAcrJ,WAAW5B,wBAAgB,IAAAyM,OAAA,EAAzCA,EAA2CzP,OAAQ,yBAAkE,QAA1C0P,EAAEzB,EAAcrJ,WAAW7B,uBAAe,IAAA2M,OAAA,EAAxCA,EAA0C1P,OAAQ,sCAC1O,MAAMyO,EAAiB9C,GAAoBsC,EAAcrJ,YACzDpK,QAAQC,IAAI,sDAAuDgU,GAC/DA,GAAkBA,EAAezO,OAAS,GAC5C2K,EAAgBO,QAAUuD,EAC1BjU,QAAQC,IAAI,2EAA4EkQ,EAAgBO,QAAQlL,UAEhH2K,EAAgBO,QAAU,GAC1B1Q,QAAQC,IAAI,qHAEd,MAAMiU,EAAY,IAAIC,KAAK,CAACV,EAActJ,WAAY,CAAEvJ,KAAM,eACxDuR,EAAWa,IAAIoB,gBAAgBF,GACrCjE,EAAcS,QAAQwC,UAAY,KAClCjD,EAAcS,QAAQ2B,IAAMF,EAC5BlC,EAAcS,QAAQ+B,OAAO0C,MAAK,KAChCnV,QAAQC,IAAI,4EACRkQ,EAAgBO,SAAWP,EAAgBO,QAAQlL,OAAS,GAC9DxF,QAAQC,IAAI,8FACZgS,OAEAjS,QAAQC,IAAI,kHACZgR,SAEDoD,OAAMzQ,IACP5D,QAAQ8H,MAAM,iEAAkElE,GAChFkP,QAEF9S,QAAQC,IAAI,gHACZuQ,EAAkBE,SAAU,CAC9B,MACE1Q,QAAQ8H,MAAM,uGAAwG2L,GACtHX,IAEJ,CAAE,MAAOhL,GACP9H,QAAQ8H,MAAM,wEAAyEA,GACvFgL,IACF,IACC,CAACY,EAAoCzD,EAAe4C,GAA2B1B,GAAqBc,GAAsBhB,GAAqBR,EAAuBqC,GAAuB3C,KAIhMH,EAAAA,EAAAA,YAAU,KACR,GAAkC,IAA9BhE,EAAME,aAAa1G,OACrB,OAGF,MAAM4P,EAAiBpJ,EAAME,aAAaF,EAAME,aAAa1G,OAAS,GACtE,GAAK4P,EAAL,CAGA,GAA4B,iBAAxBA,EAAexU,MACjB,GAAI0P,EAAsCI,SAAW0E,EAAeC,YAAc/E,EAAsCI,QAEtH,YAEG,GAA4B,sBAAxB0E,EAAexU,MAAgCwU,EAAenQ,IACnEmQ,EAAenQ,KAAOsL,EAAmCG,QAE3D,OASJ,OAJ4B,sBAAxB0E,EAAexU,MAAgCwU,EAAeE,QAChEtV,QAAQC,IAAI,8DAA+DkJ,KAAKwK,UAAUyB,EAAgB,KAAM,IAG1GA,EAAexU,MACrB,IAAK,eAAgB,CACnB,IAAKwU,EAAe/U,SAAqD,kBAAnC+U,EAAe/U,QAAQwE,SAAwD,SAAhCuQ,EAAe/U,QAAQkV,KAAiB,CAC3HvV,QAAQiG,KAAK,sFAAuFkD,KAAKwK,UAAUyB,EAAgB,KAAM,IACzI,KACF,CACApV,QAAQC,IAAI,iCAADwB,OAAkC2T,EAAe/U,QAAQkV,KAAI,gBAAgBH,EAAe/U,QAAQwE,SAC/G,MAAM2Q,EAAkC,CACtCvQ,IAAI+L,EAAAA,EAAAA,KACJ/J,KAAMmO,EAAe/U,QAAQwE,QAC7B4Q,OAAQ,OACRC,UAAWN,EAAeC,YAE5B/H,GAAU4B,IAAyBC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAWD,GAAI,IAAEpB,SAAU,IAAIoB,EAAKpB,SAAU0H,OACjFlF,EAAsCI,QAAU0E,EAAeC,WAC/D,KACF,CACA,IAAK,oBAAqB,CAAC,IAADM,EAAAC,EAAAC,EAAAC,EAQxB,GAPA9V,QAAQC,IAAI,+DAAgEmV,EAAenQ,IAClE,QAAzB0Q,EAAIP,EAAeE,cAAM,IAAAK,GAAS,QAATC,EAArBD,EAAuBrV,eAAO,IAAAsV,GAA9BA,EAAgCG,OAClC/V,QAAQC,IAAI,qCAAsCkJ,KAAKwK,UAAUyB,EAAeE,OAAOhV,QAAQyV,OAAQ,KAAM,IAE7G/V,QAAQC,IAAI,8DAA+DkJ,KAAKwK,UAAUyB,EAAeE,OAAQ,KAAM,KAGpHF,EAAe/U,SAAqD,kBAAnC+U,EAAe/U,QAAQwE,SAAwD,cAAhCuQ,EAAe/U,QAAQkV,KAAsB,CAChIvV,QAAQiG,KAAK,2FAA4FkD,KAAKwK,UAAUyB,EAAgB,KAAM,IAC9I,KACF,CACApV,QAAQC,IAAI,sCAADwB,OAAuC2T,EAAe/U,QAAQkV,KAAI,qBAAqBH,EAAe/U,QAAQwE,SACzH,MAAMmR,EAAuC,CAC3C/Q,GAAImQ,EAAenQ,KAAM+L,EAAAA,EAAAA,KACzB/J,KAAMmO,EAAe/U,QAAQwE,QAC7B4Q,OAAQ,MACRC,UAAWN,EAAeC,YAG5B,IAAIY,EAAyB,CAAC,CAAE7G,KAAM,UAAW8G,MAAO,IACpDC,EAAiB,oBAKrB,GAAyB,QAArBN,EAAAT,EAAeE,cAAM,IAAAO,GAAS,QAATC,EAArBD,EAAuBvV,eAAO,IAAAwV,GAA9BA,EAAgCC,QAAUK,MAAMC,QAAQjB,EAAeE,OAAOhV,QAAQyV,SAAWX,EAAeE,OAAOhV,QAAQyV,OAAOvQ,OAAS,EAAG,CAEpJ,MAAM8Q,EAAoBlB,EAAeE,OAAOhV,QAAQyV,OAAOQ,QAC5DzN,GAA6B,kBAAXA,EAAEsG,MAAwC,kBAAZtG,EAAEoN,QAEjDI,EAAkB9Q,OAAS,GAC7ByQ,EAAcK,EACdH,EAAiB,mBAEjBnW,QAAQiG,KAAK,kFAAmFkD,KAAKwK,UAAUyB,EAAeE,OAAOhV,QAAQyV,SAC7II,EAAiB,qCAErB,MACEnW,QAAQC,IAAI,qFAEdD,QAAQC,IAAI,2DAADwB,OAA4D0U,EAAc,MAAMhN,KAAKwK,UAAUsC,IAE1G3I,GAAU4B,IACR,MAAMsH,EAA8BP,EAAYxV,QAAO,CAACC,EAA6B2D,KACnF3D,EAAI2D,EAAQ+K,MAAQ/K,EAAQ6R,MACrBxV,IACN,CAAC,GAEJV,QAAQC,IAAI,oEAAqEkJ,KAAKwK,UAAUsC,IAChGjW,QAAQC,IAAI,oFAAqFkJ,KAAKwK,UAAU6C,IAEhH,MAAMC,GAAwBC,EAAAA,EAAAA,GAAqBF,GAInD,OAFAxW,QAAQC,IAAI,uFAADwB,OAAwF6B,OAAOC,KAAKkT,GAAuBjR,OAAM,MAAM2D,KAAKwK,UAAU8C,KAEjKtH,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GACKD,GAAI,IACPpB,SAAU,IAAIoB,EAAKpB,SAAUkI,GAC7BW,wBAAyBV,EACzBjI,WAAYhC,EAAMK,mBAClB+B,cAAepC,EAAMK,mBAAqB,0BAA6B6C,EAAKd,cAAcjI,SAAS,SAAW+I,EAAKd,cAAgB,WACnIU,yBAA0B2H,OAI1BrB,EAAe/U,QAAQwE,QAIvBuQ,EAAenQ,KACjBsL,EAAmCG,QAAU0E,EAAenQ,IAE9D,KACF,CACA,IAAK,gBACHjF,QAAQC,IAAI,4CAA6CmV,GAEzD,MAEF,IAAK,QACHpV,QAAQ8H,MAAM,6CAA8CqB,KAAKwK,UAAUyB,EAAgB,KAAM,IACjG9H,GAAU4B,IAAyBC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAWD,GAAI,IAAEd,cAAc,mBAAD3M,OAAqB2T,EAAe/U,aACrG,MAEF,IAAK,gBACHL,QAAQC,IAAI,4CAA6CmV,GACzD,MAEF,IAAK,oBACHpV,QAAQC,IAAI,gDAAiDmV,GACzDnF,EAAcS,UAChBT,EAAcS,QAAQ0B,QACtBnC,EAAcS,QAAQ2B,IAAM,IAE9BpB,KACA3D,GAAU4B,IAAyBC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAWD,GAAI,IAAElB,YAAY,EAAOC,oBAAoB,EAAOG,cAAe,4BACjH,MAEF,IAAK,YACHpO,QAAQC,IAAI,wCAAyCmV,GACrD,MAEF,IAAK,gBACHpV,QAAQC,IAAI,4CAA6CmV,GACzD,MAEF,IAAK,aACHpV,QAAQC,IAAI,yCAA0CmV,GACtD,MAEF,QAEEpV,QAAQC,IAAI,2DAA6DmV,EAAuBxU,KAAM,gBAAiBuI,KAAKwK,UAAUyB,EAAgB,KAAM,IAKhK,MAAO,KACD/E,EAAyBK,UAC3BkG,aAAavG,EAAyBK,SACtCL,EAAyBK,QAAU,MA9JZ,IAiK1B,CAAC1E,EAAME,aAAcoI,GAA2BtI,EAAMK,sBAOzD2D,EAAAA,EAAAA,YAAU,KACJhE,EAAMK,oBAEJgE,EAAyBK,UAC3BkG,aAAavG,EAAyBK,SACtCL,EAAyBK,QAAU,MAIrCwB,MAKInD,EAAS2B,QAAQ1C,aACfqC,EAAyBK,SAC3BkG,aAAavG,EAAyBK,SAExCL,EAAyBK,QAAUmG,YAAW,KAC5C7W,QAAQC,IAAI,oGACZ4S,OA3tBgB,MAiuBf,KACDxC,EAAyBK,UAC3BkG,aAAavG,EAAyBK,SACtCL,EAAyBK,QAAU,SAGtC,CAAC1E,EAAMK,mBAAoB6F,GAA2BW,GAA2B9D,EAAUsB,IAE9F,MAAMyG,IAAoB/W,EAAAA,EAAAA,cAAayR,IACrClE,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAEnB,WAAYyD,EAAM6B,OAAOxS,YACrD,CAACyM,IAEEyJ,IAAoBhX,EAAAA,EAAAA,cAAYiH,UAChCwK,GACFA,EAAMwF,iBAER,MAAM/P,EAAO8H,EAAS2B,QAAQ3C,WAAW/E,OACzC,IAAK/B,EAAM,OAEX,MAAMgQ,EAAiB,CACrBhS,IAAI+L,EAAAA,EAAAA,KACJ/J,KAAMA,EACNwO,OAAQ,OACRC,UAAW,IAAItO,MAGjBkG,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GACRD,GAAI,IACPpB,SAAU,IAAIoB,EAAKpB,SAAUmJ,GAC7BC,UAAW,OAGblX,QAAQC,IAAI,8CAA+CgH,GACvD+E,EAAMG,qBACNH,EAAMG,qBAAqBlF,GAE3BjH,QAAQiG,KAAK,yEAWhB,CAACqH,EAAUtB,EAAMG,qBAAsBH,EAAMkB,aAAcP,EAAaoC,EAAUgC,KAuB/EoG,KArB8BpX,EAAAA,EAAAA,cAAaqX,IAC/CpX,QAAQC,IAAI,qCAAsCmX,KACjD,KAEuBrX,EAAAA,EAAAA,cAAa+H,IACrC9H,QAAQ8H,MAAM,0BAA2BA,GAEzC9H,QAAQ8H,MAAM,sCAAuCA,GACrD,MAAMuP,EAAevP,aAAiBC,MAAQD,EAAMzH,QAAUqM,OAAO5E,GACrEwF,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAEd,cAAc,iBAAD3M,OAAmB4V,SAC5D,CAAC/J,KAEqBvN,EAAAA,EAAAA,cAAY,KACnCC,QAAQC,IAAI,yCACZqN,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAEd,cAAe,uBAC3C,CAACd,KAEiBvN,EAAAA,EAAAA,cAAY,KAC/BuN,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAEP,iBAAkBO,EAAKP,sBACnD,CAACrB,KAEcvN,EAAAA,EAAAA,cAAY,KAC5BuN,GAAU4B,IACRlP,QAAQC,IAAI,yDAADwB,OA74BjB,SAAgC4L,GAC9B,OAAQA,GACN,KAAKZ,EAAAA,GAAgB6K,KAAM,MAAO,OAClC,KAAK7K,EAAAA,GAAgB8K,WAAY,MAAO,aACxC,KAAK9K,EAAAA,GAAgB+K,KAAM,MAAO,OAElC,KAAK/K,EAAAA,GAAgBgL,OAAQ,MAAO,SACpC,QAKE,OAFAzX,QAAQiG,KAAK,0EAADxE,OAA2E4L,IAEjF,kBAAN5L,OAAyBiL,OAAOW,GAAM,KAE5C,CA+3B2EqK,CAAuB1L,EAAMI,iBAAgB,0CAAA3K,OAAyCyN,EAAKN,aAChK,MAAM+I,GAAoBzI,EAAKN,WAmB/B,OAlBI+I,GACF3X,QAAQC,IAAI,4DACZ+L,EAAMM,oBAENtM,QAAQC,IAAI,sEACZD,QAAQC,IAAI,+DAADwB,OAAgEsN,EAAS2B,QAAQkH,gBAAkB,MAAQ,UAAS,oBAAAnW,OAAmBsN,EAAS2B,QAAQlE,cAAgB,yCAEnLR,EAAMO,eACH4I,MAAK,KACJnV,QAAQC,IAAI,8EAGboU,OAAMvM,IACL9H,QAAQ8H,MAAM,mDAAoDA,GAClEwF,GAAUxE,IAAsBqG,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUrG,GAAC,IAAEsF,cAAc,yBAAD3M,OAA4BqG,EAAgBzH,kBAI5G8O,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAYD,GAAI,IAAEN,WAAY+I,EAAkBjK,SAAUiK,EAAkBvJ,cAAeuJ,EAAmB,aAAe,+CAE9H,CAACrK,EAAUtB,EAAMO,aAAcP,EAAMM,gBAAiByC,EAAU/C,EAAMI,mBAEnEyL,IAAa9X,EAAAA,EAAAA,cAAY,KAC7BuN,GAAU4B,IAAyBC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAWD,GAAI,IAAEL,UAAWK,EAAKL,eACnE,CAACvB,IAEEwK,IAAc/X,EAAAA,EAAAA,cAAY,KAC9BuN,GAAU4B,IACR,MAAM6I,GAAgB7I,EAAKtB,UAI3B,OAHIqC,EAAcS,UAChBT,EAAcS,QAAQsH,OAASD,IAEjC5I,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAYD,GAAI,IAAEtB,UAAWmK,EAAc3J,cAAe2J,EAAe,iBAAmB,sBAE7F,CAACzK,EAAU2C,IAERgI,IAAoBlY,EAAAA,EAAAA,cAAY,KACpCC,QAAQC,IAAI,+CAERgQ,EAAcS,UAAYT,EAAcS,QAAQ8C,SAClDvD,EAAcS,QAAQ0B,QAClBnC,EAAcS,QAAQ2B,KAAOpC,EAAcS,QAAQ2B,IAAIU,WAAW,UACpEC,IAAIC,gBAAgBhD,EAAcS,QAAQ2B,KAE5CpC,EAAcS,QAAQ2B,IAAM,IAE9BpB,KAEA3D,GAAU4B,IAAyBC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAC9BD,GAAI,IAEPjB,oBAAoB,EAEpBG,cAAe,sDAIhB,CAAC6B,EAAegB,GAAqB3D,KAExC4K,EAAAA,EAAAA,qBAAoBjM,GAAK,MACvBkM,sBAAuBA,KACrBnY,QAAQC,IAAI,gEACRgQ,EAAcS,UAAYT,EAAcS,QAAQ8C,SAClDvD,EAAcS,QAAQ0B,QAClBnC,EAAcS,QAAQ2B,KAAOpC,EAAcS,QAAQ2B,IAAIU,WAAW,UACpEC,IAAIC,gBAAgBhD,EAAcS,QAAQ2B,KAE5CpC,EAAcS,QAAQ2B,IAAM,IAG9BpB,KACA3D,GAAU8K,IAA8BjJ,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GACnCiJ,GAAS,IAEZnK,oBAAoB,EACpBG,cAAegK,EAAUnK,mBAAqB,0CAA4C,iDAM5F,CAACgC,EAAegB,GAAqB3D,IAGzC,IAAI+K,GAAoBhL,EAAMW,WAC1BsK,GAAuBjL,EAAMe,cAmBjC,OAhBIf,EAAMY,qBAGCjC,EAAMK,oBAEfgM,IAAoB,EACpBC,GAAuB,4BAGvBD,IAAoB,EACpBC,GAAuB,SAIzBtY,QAAQC,IAAI,sDAAuD+L,EAAMK,mBAAoB,4BAA6BgB,EAAMW,WAAY,qBAAsBqK,GAAmB,wBAAyBC,KAG5M5W,EAAAA,EAAAA,MAAA,OAAK6W,UAAU,kBAAkB5W,MAAO,CAAEgB,QAAS,OAAQ6V,OAAQ,QAASC,cAAe,SAAU1W,WAAY,WAAYW,SAAA,EAE3HhB,EAAAA,EAAAA,MAAA,OAAK6W,UAAU,aAAa5W,MAAO,CAAEM,QAAS,OAAQF,WAAY,UAAWC,MAAO,QAAS0W,UAAW,SAAUtW,SAAU,QAASuW,WAAY,GAAIjW,SAAA,CAAC,WAC3I4V,GAAqB,YAA4C,kBAA1BtM,EAAMI,iBAAgCK,EAAAA,GAAgBT,EAAMI,iBAAoBK,EAAAA,GAAgBT,EAAMI,iBAAmBM,OAAOV,EAAMI,iBAAiB,gBAAciM,GAAoB,MAAQ,KAAK,oBAAkBrM,EAAMK,mBAAqB,MAAQ,SAI7StJ,EAAAA,EAAAA,KAAC7D,EAAAA,eAAc,CACbE,cAAgBiB,IAGd,GAFAL,QAAQC,IAAI,sCAA6BI,GAErCA,EAAQC,QAAS,CAEnB,MAAMsY,EAAgBvY,EAAQC,QAAQG,QAAO,CAACC,EAA6B2D,KACzE3D,EAAI2D,EAAQ+K,MAAQ/K,EAAQ6R,MACrBxV,IACN,CAAC,GACEmY,GAAqBnC,EAAAA,EAAAA,GAAqBkC,GAChDtL,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAEJ,yBAA0B+J,KACzD,CACA,GAAIxY,EAAQE,SAAU,CAEpB,MAAMuY,EAAc,CAAEhY,QAAS,GAAKC,YAAa,IACjDuM,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAEX,oBAAqBuK,KACpD,CACqB,iBAAjBzY,EAAQO,OACV0M,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAElB,YAAY,MAEzC6I,YAAW,KACTvJ,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAElB,YAAY,EAAOO,oBAAqB,CAAC,QACrE,OAGPlP,cAAgB0Z,IACd/Y,QAAQC,IAAI,sCAA6B8Y,GAEzC,MAAMH,EAAgBG,EAAStY,QAAO,CAACC,EAA6B2D,KAClE3D,EAAI2D,EAAQ+K,MAAQ/K,EAAQ6R,MACrBxV,IACN,CAAC,GACEmY,GAAqBnC,EAAAA,EAAAA,GAAqBkC,GAChDtL,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAEJ,yBAA0B+J,OAEzDvZ,cAAgBkB,IACdR,QAAQC,IAAI,sCAA6BO,GACzC8M,GAAS4B,IAAIC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAUD,GAAI,IAAEX,oBAAqB/N,UAKtDkB,EAAAA,EAAAA,MAAA,OAAKC,MAAO,CAAEgB,QAAS,OAAQqW,SAAU,EAAGpX,SAAU,WAAYqX,SAAU,UAAWvW,SAAA,EAErFhB,EAAAA,EAAAA,MAAA,OAAK6W,UAAU,mBAAmB5W,MAAO,CAAEqX,SAAU,EAAGrW,QAAS,OAAQE,eAAgB,SAAUD,WAAY,SAAUhB,SAAU,WAAYG,WAAY,WAAYW,SAAA,EAC3J1C,QAAQC,IAAI,8EAA+EoY,IAA2B,OAChItV,EAAAA,EAAAA,KAACmW,EAAAA,QAAmB,CAClBC,mBAAiBhK,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAO9B,EAAMmB,mBAAsBnB,EAAMyB,0BAE1D7C,IAAK6E,GACLtD,UAAWH,EAAMG,UACjBQ,WAAYqK,GACZjO,WAAYiD,EAAMkB,oBAClBJ,eAAgBd,EAAMc,eACtBiL,WAAY1N,EACZ2N,iBAAkBhM,EAAMc,eAAiB,CAAC,CAAEiB,KAAM/B,EAAMc,eAAgB+H,MAAO,IAAS,GACxFoD,cAAejM,EAAMsB,gBACrB4K,mBAAoB5N,EACpB6N,mBAAoB5N,EACpB6N,QAAS9I,GACT+I,OAAQ9I,IAZHvD,EAAMG,YAcbzK,EAAAA,EAAAA,KAAA,UACEE,QAASA,IAAMqQ,GAAkC,+BAAgCjG,EAAMiB,gBAAkB,qBACzG3M,OAAKwN,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAOtD,GAAW,IAAEjK,SAAU,WAAYC,IAAK,OAAQC,KAAM,OAAQK,OAAQ,KAAKO,SACxF,oBAMF2K,EAAMwB,WACLnN,EAAAA,EAAAA,MAAA,OAAK6W,UAAU,UAAU5W,MAAO,CAC9BU,MAAO,QACPsX,SAAU,QACV5X,WAAY,yBACZC,MAAO,QACPW,QAAS,OACT8V,cAAe,SACfxW,QAAS,OACT2X,WAAY,oBACZC,UAAW,aACXjY,SAAU,WACVkY,MAAO,EACPjY,IAAK,EACLkY,OAAQ,EACR5X,OAAQ,EACRK,WAAY,6BACZwX,UAAW,kBACXtX,SAAA,EACAhB,EAAAA,EAAAA,MAAA,OAAKC,MAAO,CAAEgB,QAAS,OAAQE,eAAgB,gBAAiBD,WAAY,SAAUE,aAAc,OAAQ6V,WAAY,GAAIjW,SAAA,EAC1HK,EAAAA,EAAAA,KAAA,MAAIpB,MAAO,CAAEqB,OAAQ,EAAGZ,SAAU,SAAUM,SAAC,kBAC7CK,EAAAA,EAAAA,KAAA,UAAQE,QAAS4U,GAAYlW,MAAO,CAAEI,WAAY,OAAQU,OAAQ,OAAQT,MAAO,OAAQkB,OAAQ,UAAWd,SAAU,QAASH,QAAS,OAAQS,SAAC,UAInJhB,EAAAA,EAAAA,MAAA,OAAK6W,UAAU,gBAAgB5W,MAAO,CAAEqX,SAAU,EAAGzW,UAAW,OAAQO,aAAc,OAAQmX,aAAc,QAASvX,SAAA,CAClH2K,EAAMS,SAAStK,KAAK0W,IACnBxY,EAAAA,EAAAA,MAAA,OAAkB6W,UAAS,WAAA9W,OAAayY,EAAIzE,QAAU9T,MAAO,CAC3DmB,aAAc,OACdb,QAAS,YACTC,aAAc,OACdiY,SAAU,MACVC,UAA0B,SAAfF,EAAIzE,OAAoB,WAAa,aAChD1T,WAA2B,SAAfmY,EAAIzE,OAAoB,UAAY,UAChDzT,MAAO,QACP0W,UAA0B,SAAfwB,EAAIzE,OAAoB,QAAU,OAC7C4E,WAA2B,QAAfH,EAAIzE,OAAmB,IAAM,OACzC6E,YAA4B,SAAfJ,EAAIzE,OAAoB,IAAM,OAC3C8E,SAAU,aACVnY,SAAU,UACVM,SAAA,CACCwX,EAAIjT,KACJiT,EAAI7V,UAAW3C,EAAAA,EAAAA,MAAA,MAAIC,MAAO,CAACS,SAAU,QAASO,QAAS,QAASqB,QAAS,GAAKI,UAAW,OAAO1B,SAAA,CAAC,IAAEwX,EAAI7V,QAAQ,SAfxG6V,EAAIjV,OAkBhBlC,EAAAA,EAAAA,KAAA,OAAKkJ,IAAK4E,SAEZ9N,EAAAA,EAAAA,KAAA,SACEkJ,IAAKgE,EACLuK,QAAS1H,GACT2G,QAAStG,GACTxR,MAAO,CAAEgB,QAAS,WAEpBjB,EAAAA,EAAAA,MAAA,QAAM+Y,SAAU1D,GAAmBpV,MAAO,CAAEgB,QAAS,OAAQyB,UAAW,OAAQD,WAAY,OAAQD,UAAW,oBAAqByU,WAAY,GAAIjW,SAAA,EAClJK,EAAAA,EAAAA,KAAA,SACEnC,KAAK,OACLC,MAAOwM,EAAMU,WACb2M,SAAU5D,GACV6D,YAAY,uBACZhZ,MAAO,CACLqX,SAAU,EACV/W,QAAS,YACTQ,OAAQ,iBACRP,aAAc,OACdoY,YAAa,OACbM,QAAS,OACT5Y,MAAO,UACPD,WAAY,UACZK,SAAU,UAGdW,EAAAA,EAAAA,KAAA,UACEnC,KAAK,SACLia,UAAWxN,EAAMU,WAAW/E,OAC5BrH,MAAO,CACLI,WAAYsL,EAAMU,WAAW/E,OAAS,UAAY,OAClDhH,MAAO,QACPS,OAAQ,OACRP,aAAc,MACdG,MAAO,OACPmW,OAAQ,OACR7V,QAAS,OACTC,WAAY,SACZC,eAAgB,SAChBK,OAAQmK,EAAMU,WAAW/E,OAAS,UAAY,cAC9ChF,QAASqJ,EAAMU,WAAW/E,OAAS,EAAI,GACvCxG,WAAY,8BACZE,SACH,aAOL2K,EAAMwB,WACN9L,EAAAA,EAAAA,KAAA,UACEE,QAAS4U,GACTlW,MAAO,CACLC,SAAU,WACVmY,OAAQ,OACRD,MAAO,OACP/X,WAAY,UACZC,MAAO,QACPS,OAAQ,OACRP,aAAc,MACdG,MAAO,OACPmW,OAAQ,OACR7V,QAAS,OACTC,WAAY,SACZC,eAAgB,SAChBK,OAAQ,UACR4X,UAAW,6BACX3Y,OAAQ,GACRK,WAAY,uBAEduY,YAAcnX,GAAMA,EAAEC,cAAclC,MAAMqY,UAAY,aACtDgB,WAAapX,GAAMA,EAAEC,cAAclC,MAAMqY,UAAY,WAAWtX,SACjE,aAOLhB,EAAAA,EAAAA,MAAA,OAAK6W,UAAU,eAAe5W,MAAO,CACnCgB,QAAS,OACTE,eAAgB,SAChBD,WAAY,SACZX,QAAS,OACTF,WAAY,UACZ4W,WAAY,EACZtV,IAAK,QACLX,SAAA,EACAhB,EAAAA,EAAAA,MAAA,UAAQuB,QAASkU,GAAWxV,MAAOkK,EAAaoP,MAAO5N,EAAMuB,WAAa,oBAAsB,kBAAkBlM,SAAA,CAC/G2K,EAAMuB,YAAa7L,EAAAA,EAAAA,KAACmY,EAAAA,IAAiB,KAAMnY,EAAAA,EAAAA,KAACoY,EAAAA,IAAY,IAAI,IAAE9N,EAAMuB,WAAa,UAAY,aAEhGlN,EAAAA,EAAAA,MAAA,UAAQuB,QAAS6U,GAAanW,MAAOkK,EAAaoP,MAAO5N,EAAMO,UAAY,aAAe,eAAelL,SAAA,CACtG2K,EAAMO,WAAY7K,EAAAA,EAAAA,KAACqY,EAAAA,IAAU,KAAMrY,EAAAA,EAAAA,KAACsY,EAAAA,IAAY,IAAI,IAAEhO,EAAMO,UAAY,WAAa,gBAExFlM,EAAAA,EAAAA,MAAA,UAAQuB,QAASgV,GAAmBtW,OAAKwN,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAMtD,GAAW,IAAE9J,WAAY,YAAYkZ,MAAM,kCAAiCvY,SAAA,EACzHK,EAAAA,EAAAA,KAACuY,EAAAA,IAAY,IAAG,8BAObC,EAAiDvP,IAC5D,MAAOwP,EAA2BC,IAAgChc,EAAAA,EAAAA,UAAiB,oBAC7Eic,GAAkB1M,EAAAA,EAAAA,QAAqC,MACvDM,EAAavD,EAAAA,SAAc,KAC/B,MAAMN,EAAM/D,mDAIZ,OAAO+D,GAAO,KACb,IAEGe,EAAe9E,wCAGnBoG,SAAU5B,EACVyP,cAAexP,EACfyP,WAAYxP,EACZyP,UAAWxP,EACXyP,WAAYxP,EACZyP,QAASxP,EAAY,OACrByP,IACEC,EAAAA,EAAAA,MAEJjc,QAAQC,IACN,gDAAiDoM,EACjD,UAAW2P,GAGb,MAAME,GAAkBnc,EAAAA,EAAAA,cAAY,KAClCC,QAAQC,IAAI,qDACX,IAEGkc,GAAYC,EAAAA,EAAAA,UAAQ,MACxBxb,KAAM,SACNC,MAAOyO,KACL,CAACA,IAEC+M,GAAqBtc,EAAAA,EAAAA,cAAaM,IACtC,MAAMic,EAAcjc,EAAQO,KAI5B,OAFAZ,QAAQC,IAAI,uCAADwB,OAAwC6a,IAE3CA,GACN,IAAK,oBAEH,GADAtc,QAAQC,IAAI,wCAAyCI,GACjDA,EAAQiV,OAAQ,CAClB,MAAMiH,EAAclc,EAAQiV,OACxBiH,EAAYC,MAAQD,EAAYC,KAAKC,aAEvCzc,QAAQC,IAAI,qCAAsCsc,EAAYC,KAAKC,YAMvE,CACA,MACF,IAAK,oBACHzc,QAAQC,IAAI,wCAAyCI,GACrD,MACF,IAAK,YACHL,QAAQC,IAAI,gCAAiCI,GAC7C,MACF,IAAK,gBACHL,QAAQC,IAAI,oCAAqCI,GACjD,MACF,IAAK,aACHL,QAAQ8H,MAAM,iCAAkCzH,MAUnD,IAEGqc,GAAmB3c,EAAAA,EAAAA,cAAa6D,IAAY,IAAD+Y,EAC/C3c,QAAQC,IAAI,wDAAyD2D,EAAEgZ,KAAM,UAAWhZ,EAAEoG,OAAQ,YAAapG,EAAEiZ,UAC1F,QAAvBF,EAAAjB,EAAgBhL,eAAO,IAAAiM,GAAvBA,EAAyBxE,0BACxB,IAEG2E,GAAmB/c,EAAAA,EAAAA,cAAa+H,IACpC,IAAIiV,EAAU,0CAAAtb,OAA6CqG,EAAMlH,KAAI,YAAAa,OAAWqG,EAAM8U,KAAI,eAAAnb,OAAcqG,EAAMzH,SAC9GL,QAAQ8H,MAAMiV,EAAYjV,KACzB,IAWH,OATKwH,GACFtP,QAAQiG,KAAK,8FAKhBjG,QAAQC,IAAI,uEAADwB,OAAwE6N,EAAU,sBAAA7N,OAAqB+K,EAAY,MAC9HxM,QAAQC,IAAI,kDAAmDkc,IAG7Dza,EAAAA,EAAAA,MAAAsb,EAAAA,SAAA,CAAAta,SAAA,EACEhB,EAAAA,EAAAA,MAAA,OAAKC,MAAO,CAAEM,QAAS,MAAOgb,gBAAiB,UAAWjb,MAAO,QAAS0W,UAAW,SAAUvV,WAAY,OAAQf,SAAU,QAASD,OAAQ,KAAMP,SAAU,YAAac,SAAA,CAAC,4BAChJ8Y,MAE5BzY,EAAAA,EAAAA,KAACma,EAAAA,GAAa,CACdC,aAAcA,KACZnd,QAAQC,IAAI,mDACZwb,EAA6B,wBAE/B2B,WAAYA,KACVpd,QAAQC,IAAI,iDACZwb,EAA6B,sBAE/B4B,KAAMlB,EACNmB,SAAU9Q,EACV+Q,OAAO,EACPC,OAAQtB,EACRuB,UAAWpB,EACXqB,QAAShB,EACTjD,QAASqD,EAAiBpa,UAE1BK,EAAAA,EAAAA,KAAC+I,EAAsB,CACnBG,IAAKyP,EACLxO,aAAclB,EAAMkB,aACpBK,eAAgBvB,EAAMuB,eACtBrB,aAAcA,EACdC,qBAAsBA,EACtBC,gBAAiBA,EACjBC,mBAAoBA,EACpBC,gBAAiBA,EACjBC,aAAcA,EAEdC,aAAcA,S,sGC1hDxB,MAGMmR,EAAwE,CAC5EC,IAAK,CACH5c,eAAgB,IAChBC,gBAAiB,IACjB4c,gBAAiB,GACjBC,iBAAkB,GAClBC,YAAa,GACbC,aAAc,GACdC,gBAAiB,EACjBC,iBAAkB,EAClBC,UAAW,IAEbC,QAAS,CACPC,eAAgB,IAChBC,gBAAiB,IACjBC,YAAa,EACbC,aAAc,GACdC,cAAe,GACfC,gBAAiB,GACjBC,iBAAkB,GAClBxT,mBAAoB,GACpBC,oBAAqB,IAEvBwT,MAAO,CACLJ,aAAc,IACdC,cAAe,IACfF,YAAa,GACblT,eAAgB,GAChBC,gBAAiB,GACjBuT,cAAe,GACfC,eAAgB,GAChBC,cAAe,GACfC,eAAgB,GAChBC,iBAAkB,GAClBC,kBAAmB,IAErBC,SAAU,CACRre,QAAS,GACTid,YAAa,IACbC,aAAc,IACdC,gBAAiB,IACjBC,iBAAkB,IAClBK,YAAa,GACbxd,YAAa,IAEfqe,KAAM,CACJC,iBAAkB,GAClBC,kBAAmB,GACnBvB,YAAa,EACbC,aAAc,EACdO,YAAa,EACbN,gBAAiB,GACjBC,iBAAkB,GAClBpd,QAAS,IAEXye,QAAS,CACPR,cAAe,GACfC,eAAgB,GAChBC,iBAAkB,GAClBC,kBAAmB,GACnBL,cAAe,GACfC,eAAgB,GAChBN,aAAc,GACdC,cAAe,IAEjBe,SAAU,CACRxe,eAAgB,GAChBC,gBAAiB,GACjB4d,cAAe,GACfC,eAAgB,GAChBN,aAAc,GACdC,cAAe,IAEjBgB,WAAY,CACVze,eAAgB,EAChBC,gBAAiB,EACjB8c,YAAa,GACbC,aAAc,GACdC,gBAAiB,GACjBC,iBAAkB,GAClBL,gBAAiB,GACjBC,iBAAkB,GAClBhd,QAAS,IAEX4e,YAAa,CACX1e,eAAgB,GAChBC,gBAAiB,GACjB4d,cAAe,GACfC,eAAgB,GAChBb,gBAAiB,GACjBC,iBAAkB,IAEpByB,UAAW,CACTnB,aAAc,GACdN,iBAAkB,GAClBK,YAAa,GACbvT,YAAa,GACb6T,cAAe,GACf/d,QAAS,IAEX8e,KAAM,CACJ5e,eAAgB,IAChBC,gBAAiB,IACjB4d,cAAe,GACfC,eAAgB,IAGlBe,UAAW,CACT7e,eAAgB,GAChBC,gBAAiB,GACjB4c,gBAAiB,GACjBC,iBAAkB,GAClBG,gBAAiB,GACjBC,iBAAkB,IAEpB4B,cAAe,CACbtB,aAAc,GACdC,cAAe,GACfF,YAAa,GACbM,cAAe,GACfC,eAAgB,KAIPpI,EAAwBqJ,IACnC/f,QAAQC,IAAI,qDAAsDqD,OAAOC,KAAKwc,GAAeC,KAAK,OAGlG,MAAMC,EAA6BrV,EAAAA,EAAyBnK,QAAO,CAACC,EAAoBmK,KACtFnK,EAAImK,GAAa,EACVnK,IACN,CAAC,GAEJV,QAAQC,IAAI,iDACZ,IAAK,MAAOoE,EAAS6R,KAAU5S,OAAO4c,QAAQH,GAAgB,CAC5D,MAAMI,EAAUxC,EAA8BtZ,EAAQgL,eACtD,GAAK8Q,EAAL,CAIAngB,QAAQC,IAAI,8CAADwB,OAA+C4C,EAAO,aAAA5C,OAAYyU,EAAMkK,QAAQ,KAC3F,IAAK,MAAOC,EAAOC,KAAWhd,OAAO4c,QAAQC,GAAU,CACrD,MAAMI,EAAWF,EACXG,EAAaP,EAAYM,IAAa,EACtCE,EAAWvK,EAASoK,EAnJgB,EAoJ1CL,EAAYM,GAAYrf,KAAK0S,IAAI,EAAG1S,KAAKC,IAAI,EAAGqf,EAAaC,GAG/D,CATA,MAFEzgB,QAAQC,IAAI,wDAADwB,OAAyD4C,EAAO,eAY/E,CAEA,OADArE,QAAQC,IAAI,4DAADwB,OAA6D6B,OAAOC,KAAK0c,GAAaza,OAAM,qBAAA/D,OAAoB6B,OAAOod,OAAOT,GAAaxf,QAAO,CAACqI,EAAG6X,IAAM7X,EAAI6X,GAAG,GAAGP,QAAQ,KAClLH,E","sources":["components/TestModeToggle.tsx","services/VisemeService.ts","components/SimulationView.tsx","utils/prosodyToBlendshapes.ts"],"sourcesContent":["import React, { useState, useCallback } from 'react';\nimport { MockHumeEVI, mockHumeProsodyData, mockEmotionSets } from '../utils/mockHumeData';\n\ninterface TestModeToggleProps {\n  onMockMessage?: (message: any) => void;\n  onMockProsody?: (emotions: any[]) => void;\n  onMockVisemes?: (visemes: any) => void;\n}\n\nexport const TestModeToggle: React.FC<TestModeToggleProps> = ({\n  onMockMessage,\n  onMockProsody,\n  onMockVisemes\n}) => {\n  const [isTestMode, setIsTestMode] = useState(false);\n  const [isExpanded, setIsExpanded] = useState(false);\n  const [mockEVI, setMockEVI] = useState<MockHumeEVI | null>(null);\n\n  const handleTestModeToggle = useCallback(() => {\n    if (!isTestMode) {\n      // Start test mode\n      console.log('üß™ Enabling Test Mode - Using Mock Hume Data');\n      \n      const evi = new MockHumeEVI((message) => {\n        console.log('üé≠ Mock message received:', message);\n        if (onMockMessage) {\n          onMockMessage(message);\n        }\n        \n        // Extract prosody for emotions\n        if (message.prosody && onMockProsody) {\n          onMockProsody(message.prosody);\n        }\n        \n        // Extract timeline for visemes\n        if (message.timeline && onMockVisemes) {\n          // Convert timeline to viseme format\n          const visemes = message.timeline.reduce((acc: any, item: any) => {\n            if (item.type === 'phoneme') {\n              // Map phonemes to basic visemes\n              switch (item.value) {\n                case 'eh':\n                case 'ae':\n                  acc.jawOpen = 0.0; // FIXED: was 0.6 - this was causing mouth stuck open\n                  break;\n                case 'ow':\n                case 'aw':\n                  acc.mouthFunnel = 0.7;\n                  break;\n                case 'iy':\n                case 'ih':\n                  acc.mouthSmileLeft = 0.3;\n                  acc.mouthSmileRight = 0.3;\n                  break;\n                default:\n                  acc.jawOpen = Math.max(acc.jawOpen || 0, 0.0); // FIXED: was 0.2 - forcing minimum jaw open\n              }\n            }\n            return acc;\n          }, {});\n          \n          onMockVisemes(visemes);\n        }\n      });\n      \n      setMockEVI(evi);\n      evi.start();\n      setIsTestMode(true);\n      setIsExpanded(true); // Auto-expand when starting test mode\n    } else {\n      // Stop test mode\n      console.log('üß™ Disabling Test Mode');\n      if (mockEVI) {\n        mockEVI.stop();\n      }\n      setMockEVI(null);\n      setIsTestMode(false);\n      setIsExpanded(false);\n    }\n  }, [isTestMode, mockEVI, onMockMessage, onMockProsody, onMockVisemes]);\n\n  const handleTestEmotion = useCallback((emotionSetKey: string) => {\n    if (onMockProsody) {\n      const emotionSet = mockEmotionSets[emotionSetKey as keyof typeof mockEmotionSets];\n      if (emotionSet) {\n        console.log(`üß™ Testing emotion set: ${emotionSetKey}`, emotionSet);\n        onMockProsody(emotionSet);\n      }\n    }\n  }, [onMockProsody]);\n\n  return (\n    <div style={{\n      position: 'fixed',\n      top: '10px',\n      left: '10px', // Moved to left side\n      background: 'rgba(0,0,0,0.9)',\n      color: 'white',\n      padding: '10px',\n      borderRadius: '8px',\n      zIndex: 1000,\n      fontSize: '12px',\n      width: isExpanded ? '280px' : '160px',\n      maxHeight: isExpanded ? '70vh' : 'auto',\n      overflowY: isExpanded ? 'auto' : 'hidden',\n      transition: 'all 0.3s ease',\n      border: '1px solid rgba(255,255,255,0.2)'\n    }}>\n      <div style={{ display: 'flex', alignItems: 'center', justifyContent: 'space-between', marginBottom: '8px' }}>\n        <h4 style={{ margin: '0', fontSize: '13px' }}>üß™ Test Mode</h4>\n        {isTestMode && (\n          <button\n            onClick={() => setIsExpanded(!isExpanded)}\n            style={{\n              background: 'transparent',\n              border: '1px solid rgba(255,255,255,0.3)',\n              color: 'white',\n              padding: '2px 6px',\n              borderRadius: '3px',\n              cursor: 'pointer',\n              fontSize: '10px'\n            }}\n          >\n            {isExpanded ? '‚ñ≤' : '‚ñº'}\n          </button>\n        )}\n      </div>\n      \n      <button\n        onClick={handleTestModeToggle}\n        style={{\n          background: isTestMode ? '#ff4444' : '#44ff44',\n          color: 'white',\n          border: 'none',\n          padding: '6px 10px',\n          borderRadius: '4px',\n          cursor: 'pointer',\n          width: '100%',\n          fontSize: '11px',\n          marginBottom: isTestMode && isExpanded ? '10px' : '0'\n        }}\n      >\n        {isTestMode ? 'üõë Stop' : '‚ñ∂Ô∏è Start Test'}\n      </button>\n\n      {isTestMode && isExpanded && (\n        <div>\n          <p style={{ margin: '5px 0', fontSize: '11px', fontWeight: 'bold' }}>\n            üé≠ Test Emotions:\n          </p>\n          <div style={{ display: 'grid', gridTemplateColumns: '1fr 1fr', gap: '4px', marginBottom: '8px' }}>\n            {Object.keys(mockEmotionSets).map(emotionKey => (\n              <button\n                key={emotionKey}\n                onClick={() => handleTestEmotion(emotionKey)}\n                style={{\n                  background: '#555',\n                  color: 'white',\n                  border: '1px solid rgba(255,255,255,0.2)',\n                  padding: '4px 6px',\n                  borderRadius: '3px',\n                  cursor: 'pointer',\n                  fontSize: '10px',\n                  textTransform: 'capitalize',\n                  transition: 'background 0.2s'\n                }}\n                onMouseEnter={(e) => e.currentTarget.style.background = '#777'}\n                onMouseLeave={(e) => e.currentTarget.style.background = '#555'}\n              >\n                {getEmotionEmoji(emotionKey)} {emotionKey}\n              </button>\n            ))}\n          </div>\n          \n          <div style={{ fontSize: '9px', opacity: 0.7, lineHeight: '1.2', borderTop: '1px solid rgba(255,255,255,0.1)', paddingTop: '6px' }}>\n            <strong>üéØ Look for:</strong><br/>\n            ‚Ä¢ <strong>Joy:</strong> Big smile + raised brows<br/>\n            ‚Ä¢ <strong>Anger:</strong> Furrowed brows + flared nostrils<br/>\n            ‚Ä¢ <strong>Sad:</strong> Frown + puppy dog eyes<br/>\n            ‚Ä¢ <strong>Surprise:</strong> Wide eyes + dramatic brows<br/>\n            ‚Ä¢ <strong>Fear:</strong> Wide eyes + worried brows<br/>\n            ‚Ä¢ <strong>Disgust:</strong> Nose sneer + lip curl\n          </div>\n        </div>\n      )}\n      \n      {isTestMode && !isExpanded && (\n        <div style={{ fontSize: '10px', opacity: 0.8, marginTop: '5px' }}>\n          Click ‚ñº to expand controls\n        </div>\n      )}\n    </div>\n  );\n};\n\nfunction getEmotionEmoji(emotion: string): string {\n  const emojiMap: Record<string, string> = {\n    happy: 'üòä',\n    sad: 'üò¢', \n    angry: 'üò†',\n    surprised: 'üò≤',\n    scared: 'üò®',\n    disgusted: 'ü§¢',\n    confused: 'üòï',\n    content: 'üòå'\n  };\n  return emojiMap[emotion] || 'üòê';\n}\n","// /Users/douglasgoldstein/XRCupid_Clone/hub/src/services/VisemeService.ts\nimport process from 'process';\nimport * as SpeechSDK from 'microsoft-cognitiveservices-speech-sdk';\n\n// Type definitions for the data structures\nexport interface BlendshapeFrame {\n  frameIndex: number; // Absolute frame index, sorted\n  shapes: number[];   // Array of 55 blendshape values\n  audioOffset: number; // In milliseconds, from the start of the audio, corresponding to this frame or start of this animation chunk\n}\n\nexport interface StandardViseme {\n  visemeID: number;\n  audioOffset: number; // In milliseconds, from the start of the audio\n}\n\nexport interface VisemeData {\n  standardVisemes: StandardViseme[];\n  blendShapeFrames: BlendshapeFrame[];\n}\n\nexport interface VisemeServiceResult {\n  audioData: ArrayBuffer;\n  visemeData: VisemeData;\n  audioDurationMs?: number; // Added to store audio duration in milliseconds\n}\n\n// Keep VisemeEvent for the raw SDK event if needed, or remove if fully superseded\n// TypeScript interfaces and enums remain the same\n// ... existing VisemeEvent, etc.\n\n// Mappings for Hume EVI to Azure Viseme Conversion\n\n// Based on Hume's likely SAPI-like output (e.g., 'AA', 'AE', 'SIL')\n// and Azure's IPA-based viseme table.\nconst HUME_SAPI_TO_IPA: Record<string, string | null> = {\n    'AA': '…ë',  // father\n    'AE': '√¶',  // cat\n    'AH': ' å',  // but\n    'AO': '…î',  // dog\n    'AW': 'a ä', // cow\n    'AY': 'a…™', // say\n    'B':  'b',\n    'CH': 't É',\n    'D':  'd',\n    'DH': '√∞',\n    'EH': '…õ',  // bed\n    'ER': '…ù',  // her\n    'EY': 'e…™', // they (diphthong, special handling for viseme ID)\n    'F':  'f',\n    'G':  'g',\n    'HH': 'h',\n    'IH': '…™',  // sit\n    'IY': 'i',  // see\n    'JH': 'd í',\n    'K':  'k',\n    'L':  'l',\n    'M':  'm',\n    'N':  'n',\n    'NG': '≈ã',  // sing\n    'OW': 'o ä', // go (diphthong, special handling for viseme ID)\n    'OY': '…î…™', // boy\n    'P':  'p',\n    'R':  '…π',  // red\n    'S':  's',\n    'SH': ' É',\n    'T':  't',\n    'TH': 'Œ∏',  // thin\n    'UH': ' ä',  // book\n    'UW': 'u',  // blue\n    'V':  'v',\n    'W':  'w',\n    'Y':  'j',  // yes\n    'Z':  'z',\n    'ZH': ' í',\n    'SIL': 'silence', // Hume 'sil'\n    'PAU': 'silence', // Hume 'pau'\n    'AX': '…ô', // Schwa, common SAPI phoneme\n    // Add any other SAPI phonemes Hume might output\n};\n\n// Based on Azure's Viseme ID to IPA mapping table\nconst IPA_TO_AZURE_VISEME_ID_MAP: Array<{ ipaSymbols: string[], id: number }> = [\n    { ipaSymbols: ['silence'], id: 0 }, // Special case for silence\n    { ipaSymbols: ['√¶', '…ô', ' å'], id: 1 },\n    { ipaSymbols: ['…ë'], id: 2 },\n    { ipaSymbols: ['…î'], id: 3 },\n    { ipaSymbols: ['…õ', ' ä'], id: 4 },\n    { ipaSymbols: ['…ù'], id: 5 },\n    { ipaSymbols: ['j', 'i', '…™'], id: 6 },\n    { ipaSymbols: ['w', 'u'], id: 7 },\n    { ipaSymbols: ['o'], id: 8 },\n    { ipaSymbols: ['a ä'], id: 9 },\n    { ipaSymbols: ['…î…™'], id: 10 },\n    { ipaSymbols: ['a…™'], id: 11 },\n    { ipaSymbols: ['h'], id: 12 },\n    { ipaSymbols: ['…π'], id: 13 },\n    { ipaSymbols: ['l'], id: 14 },\n    { ipaSymbols: ['s', 'z'], id: 15 },\n    { ipaSymbols: [' É', 't É', 'd í', ' í'], id: 16 },\n    { ipaSymbols: ['√∞'], id: 17 },\n    { ipaSymbols: ['f', 'v'], id: 18 },\n    { ipaSymbols: ['d', 't', 'n', 'Œ∏'], id: 19 },\n    { ipaSymbols: ['k', 'g', '≈ã'], id: 20 },\n    { ipaSymbols: ['p', 'b', 'm'], id: 21 },\n];\n\nfunction getAzureVisemeIdFromIpa(ipaSymbol: string | null): number {\n    if (ipaSymbol === null) { // Indicates an unmapped Hume SAPI phoneme\n        console.warn(`[VisemeConversion] Received unmapped Hume SAPI phoneme, defaulting to Viseme 0 (silence).`);\n        return 0;\n    }\n    if (ipaSymbol === 'silence') return 0;\n\n    for (const entry of IPA_TO_AZURE_VISEME_ID_MAP) {\n        if (entry.ipaSymbols.includes(ipaSymbol)) {\n            return entry.id;\n        }\n    }\n\n    // Fallback for specific diphthongs not directly in Azure's simple IPA list for a single viseme ID\n    if (ipaSymbol === 'e…™') return 4; // For SAPI 'EY' -> IPA 'e…™'. Map to viseme for '…õ' (ID 4) or '…™' (ID 6). Chosen 4.\n    if (ipaSymbol === 'o ä') return 8; // For SAPI 'OW' -> IPA 'o ä'. Map to viseme for 'o' (ID 8). Good match.\n\n    console.warn(`[VisemeConversion] Unmapped IPA symbol: '${ipaSymbol}', defaulting to Viseme 0 (silence).`);\n    return 0; // Default to silence for unmapped IPA symbols\n}\n\nexport interface HumeTimelineEvent {\n    time: number; // in seconds\n    phoneme: string; // SAPI-like phoneme string (e.g., 'AA', 'P', 'SIL')\n}\n\nexport interface ConvertedHumeVisemes {\n    visemeEvents: VisemeEvent[];\n    audioDurationMs: number;\n}\n\n/**\n * Converts a Hume EVI timeline to Azure-compatible VisemeEvents.\n * @param humeTimeline An array of HumeTimelineEvent objects.\n * @param audioDurationSeconds The total duration of the Hume audio in seconds.\n * @returns An object containing an array of VisemeEvents and the audio duration in milliseconds.\n */\nexport function convertHumeTimelineToAzureVisemes(\n    humeTimeline: HumeTimelineEvent[],\n    audioDurationSeconds: number\n): ConvertedHumeVisemes {\n    const visemeEvents: VisemeEvent[] = [];\n    const audioDurationMs = Math.round(audioDurationSeconds * 1000);\n\n    if (!humeTimeline || humeTimeline.length === 0) {\n        console.warn('[VisemeConversion] Hume timeline is empty. Returning empty viseme events.');\n        return { visemeEvents, audioDurationMs };\n    }\n\n    humeTimeline.forEach((humeEvent, index) => {\n        const humePhonemeKey = humeEvent.phoneme.toUpperCase(); // Normalize to uppercase for map lookup\n        const ipaSymbol = HUME_SAPI_TO_IPA[humePhonemeKey] !== undefined \n                            ? HUME_SAPI_TO_IPA[humePhonemeKey]\n                            : null; // null if not found\n        \n        const visemeId = getAzureVisemeIdFromIpa(ipaSymbol);\n        const audioOffsetTicks = Math.round(humeEvent.time * 10_000_000); // Convert seconds to 100-nanosecond ticks\n\n        visemeEvents.push({\n            audioOffset: audioOffsetTicks,\n            visemeId: visemeId,\n            isLastViseme: index === humeTimeline.length - 1,\n        });\n    });\n\n    // Ensure visemes are sorted by audioOffset, though Hume timeline should already be sorted.\n    visemeEvents.sort((a, b) => a.audioOffset - b.audioOffset);\n    \n    // It's possible the last viseme from Hume doesn't extend to the full audio duration.\n    // The animation might need a final 'silence' viseme at the actual audio end if not covered.\n    // For now, we rely on the provided audioDurationMs for animation timing.\n\n    console.log('[VisemeConversion] Converted Hume timeline to Azure visemes:', visemeEvents);\n    console.log('[VisemeConversion] Audio duration (ms):', audioDurationMs);\n\n    return { visemeEvents, audioDurationMs };\n}\n\nexport interface VisemeEvent {\n    audioOffset: number; // In ticks (100 nanoseconds)\n    visemeId: number;\n    isLastViseme?: boolean; // Optional: true if this is the last viseme in the sequence\n    animation?: string; // JSON string with blend shapes and frame index for SSML\n  }\n\nlet speechConfig: SpeechSDK.SpeechConfig | undefined;\n\nexport const initializeAzureSdk = (): void => {\n  console.log('[VisemeService.ts initializeAzureSdk] Called');\n  const speechKey = process.env.REACT_APP_AZURE_SPEECH_KEY;\n  const speechRegion = process.env.REACT_APP_AZURE_SPEECH_REGION;\n\n  if (!speechKey || !speechRegion) {\n    console.error('[VisemeService.ts] Azure Speech Key or Region not configured.');\n    // Consider throwing an error to make initialization failure more explicit\n    // throw new Error('Azure Speech Key or Region not configured.');\n    return;\n  }\n  // speechConfig is created on demand in synthesizeSpeechWithVisemes\n  console.log('[VisemeService.ts] Azure SDK credentials check passed.');\n};\n\nexport const synthesizeSpeechWithVisemes = async (\n  // CASCADE_DEBUG_V6\n  text: string,\n  voiceName: string = 'en-US-JennyNeural' // Default voice\n): Promise<VisemeServiceResult> => {\n  console.log(`[VisemeService.ts synthesizeSpeechWithVisemes ENTRY_V6] Text: \"${text}\", Voice: \"${voiceName}\", Timestamp: ${Date.now()}`); // CASCADE_DEBUG_V6\n  return new Promise((resolve, reject) => {\n    console.log(`[VisemeService.ts synthesizeSpeechWithVisemes PROMISE_CONSTRUCTOR_V6] Text: \"${text}\", Timestamp: ${Date.now()}`); // CASCADE_DEBUG_V6\n    // Removed duplicated V4 log and Promise constructor here\n    const speechKey = process.env.REACT_APP_AZURE_SPEECH_KEY;\n    const speechRegion = process.env.REACT_APP_AZURE_SPEECH_REGION;\n\n    if (!speechKey || !speechRegion) {\n      console.error('[VisemeService.ts] Azure Speech Key or Region not found.');\n      console.log(`[VisemeService.ts REJECT_V6_NO_KEY_REGION] Text: \"${text}\", Timestamp: ${Date.now()}`); // CASCADE_DEBUG_V6\n      reject(new Error('Azure Speech Key or Region not configured.'));\n      return;\n    }\n\n    try {\n      speechConfig = SpeechSDK.SpeechConfig.fromSubscription(speechKey, speechRegion);\n      console.log('[VisemeService.ts synthesizeSpeechWithVisemes] SpeechConfig created:', speechConfig);\n      speechConfig.speechSynthesisOutputFormat = SpeechSDK.SpeechSynthesisOutputFormat.Audio16Khz32KBitRateMonoMp3;\n      // speechConfig.speechSynthesisVoiceName = voiceName; // Set by SSML\n\n      const ssml = `\n        <speak version=\"1.0\" xmlns=\"http://www.w3.org/2001/10/synthesis\" xmlns:mstts=\"http://www.w3.org/2001/mstts\" xml:lang=\"en-US\">\n          <voice name=\"${voiceName}\">\n            <mstts:viseme type=\"FacialExpression\"/>\n            ${text}\n          </voice>\n        </speak>`;\n\n      let synthesizer = new SpeechSDK.SpeechSynthesizer(speechConfig, null);\n      console.log('[VisemeService.ts synthesizeSpeechWithVisemes] Synthesizer created. SSML to be used:', ssml);\n\n      let collectedVisemeData: VisemeData = { standardVisemes: [], blendShapeFrames: [] };\n      let promiseHandled = false;\n\n      const cleanup = () => {\n        if (synthesizer) {\n          try {\n            synthesizer.close();\n          } catch (closeError) {\n            console.error('[VisemeService.ts cleanup] Error closing synthesizer:', closeError);\n          }\n          synthesizer = null as any; // Allow reassignment to null\n        }\n      };\n\n      synthesizer.visemeReceived = (s, e: SpeechSDK.SpeechSynthesisVisemeEventArgs) => {\n        console.log(`[VisemeService.ts visemeReceived] FIRING! Offset: ${e.audioOffset / 10000}ms, VisemeId: ${e.visemeId}`);\n        if (e.animation && e.animation.trim() !== '') {\n          console.log(`[VisemeService.ts visemeReceived] Animation length: ${e.animation.length}`);\n          console.log(`[VisemeService.ts visemeReceived] Animation (first 100 chars): ${e.animation.substring(0, 100)}`);\n        // Note: There was a nested 'if (e.animation && e.animation.trim() !== '')' here which might be redundant.\n        // Keeping the outer one for now.\n          try {\n            const animationData = JSON.parse(e.animation);\n            if (animationData.BlendShapes && animationData.FrameIndex !== undefined) {\n              const baseFrameIndex = animationData.FrameIndex;\n              animationData.BlendShapes.forEach((shapeFrame: number[], frameIdxInChunk: number) => {\n                collectedVisemeData.blendShapeFrames.push({\n                  frameIndex: baseFrameIndex + frameIdxInChunk,\n                  shapes: shapeFrame,\n                  audioOffset: (e.audioOffset / 10000) + (frameIdxInChunk * (1000 / (animationData.FrameRate || 60))),\n                });\n              });\n            } else {\n              collectedVisemeData.standardVisemes.push({\n                visemeID: e.visemeId,\n                audioOffset: e.audioOffset / 10000,\n              });\n            }\n          } catch (error) {\n            console.error('[VisemeService.ts visemeReceived] Error parsing animation JSON:', error, 'Animation string (on error):', e.animation);\n            collectedVisemeData.standardVisemes.push({\n              visemeID: e.visemeId,\n              audioOffset: e.audioOffset / 10000,\n            });\n          }\n        } else {\n          console.log(`[VisemeService.ts visemeReceived] Animation is EMPTY or UNDEFINED.`);\n          collectedVisemeData.standardVisemes.push({\n            visemeID: e.visemeId,\n            audioOffset: e.audioOffset / 10000\n          });\n        }\n      };\n\n      synthesizer.speakSsmlAsync(\n        ssml,\n        result => {\n          if (promiseHandled) {\n            cleanup(); return;\n          }\n          promiseHandled = true;\n          if (result.reason === SpeechSDK.ResultReason.SynthesizingAudioCompleted) {\n            collectedVisemeData.blendShapeFrames.sort((a, b) => a.frameIndex - b.frameIndex);\n            collectedVisemeData.standardVisemes.sort((a,b) => a.audioOffset - b.audioOffset);\n            console.log('[VisemeService V6] Raw result.audioDuration:', result.audioDuration);\n            const audioDurationMs = result.audioDuration / 10000; // Azure returns duration in 100-nanosecond units (ticks)\n            console.log(`[VisemeService.ts speakSsmlAsync COMPLETED] Audio Duration: ${result.audioDuration} (100ns units) = ${audioDurationMs}ms`);\n            console.log(`[VisemeService.ts RESOLVE_V6_SUCCESS] Text: \"${text}\", audioDurationMs: ${audioDurationMs}, Timestamp: ${Date.now()}`); // CASCADE_DEBUG_V6\n            resolve({ audioData: result.audioData, visemeData: collectedVisemeData, audioDurationMs: audioDurationMs });\n          } else if (result.reason === SpeechSDK.ResultReason.Canceled) {\n            const cancellation = SpeechSDK.CancellationDetails.fromResult(result);\n            const reasonText = cancellation.reason !== undefined ? SpeechSDK.CancellationReason[cancellation.reason] : 'UnknownReason';\n            console.error(`[VisemeService.ts speakSsmlAsync CANCELED] Reason: ${reasonText}, Details: ${cancellation.errorDetails}, ErrorCode: ${cancellation.ErrorCode}`);\n            console.log(`[VisemeService.ts REJECT_V6_CANCELED] Text: \"${text}\", Reason: ${reasonText}, Timestamp: ${Date.now()}`); // CASCADE_DEBUG_V6\n            reject(`Synthesis Canceled: ${reasonText}. Details: ${cancellation.errorDetails}. ErrorCode: ${cancellation.ErrorCode}`);\n          } else {\n            console.error(`[VisemeService.ts speakSsmlAsync FAILED] Reason: ${result.reason}, Details: ${result.errorDetails}`);\n            console.log(`[VisemeService.ts REJECT_V6_FAILED_RESULT] Text: \"${text}\", Reason: ${result.reason}, Timestamp: ${Date.now()}`); // CASCADE_DEBUG_V6\n            reject(new Error(`Synthesis failed: ${result.errorDetails || 'Unknown error'}`));\n          }\n          cleanup();\n        },\n        error => {\n          if (promiseHandled) {\n            cleanup(); return;\n          }\n          promiseHandled = true;\n          console.log(`[VisemeService.ts REJECT_V6_ERROR_CALLBACK] Text: \"${text}\", Error: ${error}, Timestamp: ${Date.now()}`); // CASCADE_DEBUG_V6\n          reject(error);\n          cleanup();\n        }\n      );\n    } catch (error) {\n      console.error('[VisemeService.ts] Error initializing or starting synthesis:', error);\n      console.log(`[VisemeService.ts REJECT_V6_OUTER_CATCH] Text: \"${text}\", Error: ${error}, Timestamp: ${Date.now()}`); // CASCADE_DEBUG_V6\n      reject(error);\n    }\n  });\n};\n\n","import process from 'process';\nimport React, { useCallback, useEffect, useMemo, useRef, useState, useContext, useImperativeHandle } from 'react';\nimport { useParams, useNavigate } from 'react-router-dom';\nimport { getFirestore, Timestamp, doc, getDoc, updateDoc, arrayUnion } from 'firebase/firestore';\nimport { prosodyToBlendshapes } from '../utils/prosodyToBlendshapes';\nimport * as Hume from 'hume';\n\nimport { v4 as uuidv4 } from 'uuid';\nimport { \n  FaMicrophone, \n  FaMicrophoneSlash, \n  FaVolumeUp, \n  FaVolumeMute, \n  FaPaperPlane, \n  FaTimes, \n  FaExpand, \n  FaCompress, \n  FaCommentDots, \n  FaCog, \n  FaPlay, \n  FaPause, \n  FaRedo, \n  FaStopCircle \n} from 'react-icons/fa';\nimport { IoMdSend } from 'react-icons/io';\nimport { Canvas } from '@react-three/fiber';\nimport { OrbitControls } from '@react-three/drei';\nimport { Group, Vector3 } from 'three';\nimport * as THREE from 'three';\nimport { VoiceProvider, useVoice, VoiceReadyState, type JSONMessage, type ConnectionMessage } from '@humeai/voice-react'; // Assuming JSONMessage is exported\nimport { TestModeToggle } from './TestModeToggle';\n\nimport { useAuth } from '../contexts/AuthContext';\nimport { useUser } from '../contexts/UserContext';\nimport firebase from 'firebase/app';\nimport 'firebase/firestore';\nimport ReadyPlayerMeAvatar from './ReadyPlayerMeAvatar';\nimport EmotionDrivenAvatar, { type Emotion } from './EmotionDrivenAvatar';\n\nimport { ARKitBlendshapeNamesList, type BlendShapeMap, type BlendshapeKey } from '../types/blendshapes';\nimport { useHumeEmotionStream } from '../hooks/useHumeEmotionStream';\nimport { initializeAzureSdk, synthesizeSpeechWithVisemes as untypedSynthesizeSpeechWithVisemes, VisemeEvent, convertHumeTimelineToAzureVisemes, type HumeTimelineEvent, type ConvertedHumeVisemes } from '../services/VisemeService';\n// import { saveChatMessageToFirebase } from '../firebase/firebaseServices'; \n// import { getTopEmotion } from '../utils/emotionMappings'; // emotionToBlendshapes removed, getTopEmotion also unused due to local definition\nimport * as SpeechSDK from 'microsoft-cognitiveservices-speech-sdk'; \n\n\n// Data structures from VisemeService.js\ninterface BlendshapeFrame {\n  frameIndex: number; // Absolute frame index\n  shapes: number[];   // Array of 55 blendshape values\n  audioOffset: number; // In milliseconds, from the start of the audio\n}\n\ninterface StandardViseme {\n  visemeID: number;\n  audioOffset: number; // In milliseconds, already converted by VisemeService\n}\n\ninterface VisemeData {\n  standardVisemes: StandardViseme[];\n  blendShapeFrames: BlendshapeFrame[];\n}\n\ninterface VisemeServiceResult {\n  audioData: ArrayBuffer;\n  visemeData: VisemeData;\n}\n\n// The old AzureViseme and AzureVisemeAnimation interfaces might be removable if no longer used\n// For now, keeping them commented out or to be removed if prepareVisemeFrames fully transitions\n/*\ninterface SpeakTextWithVisemesResult {\n  audioData: ArrayBuffer; \n  visemes: AzureViseme[]; \n}\ninterface AzureVisemeAnimation {\n  frameIndex: number;\n  blendShapes: number[][];\n}\ninterface AzureViseme {\n  audioOffset: number; \n  visemeID: number;\n  animation?: AzureVisemeAnimation | null; \n}\n*/\n\n// ARKit blendshape names corresponding to Azure's 55 blendshapes\nconst ARKIT_BLENDSHAPE_NAMES_AZURE = [\n  \"browDownLeft\", \"browDownRight\", \"browInnerUp\", \"browOuterUpLeft\", \"browOuterUpRight\",\n  \"cheekPuff\", \"cheekSquintLeft\", \"cheekSquintRight\", \"eyeBlinkLeft\", \"eyeBlinkRight\",\n  \"eyeLookDownLeft\", \"eyeLookDownRight\", \"eyeLookInLeft\", \"eyeLookInRight\", \"eyeLookOutLeft\",\n  \"eyeLookOutRight\", \"eyeLookUpLeft\", \"eyeLookUpRight\", \"eyeSquintLeft\", \"eyeSquintRight\",\n  \"eyeWideLeft\", \"eyeWideRight\", \"jawForward\", \"jawLeft\", \"jawOpen\", \"jawRight\",\n  \"mouthClose\", \"mouthDimpleLeft\", \"mouthDimpleRight\", \"mouthFrownLeft\", \"mouthFrownRight\",\n  \"mouthFunnel\", \"mouthLeft\", \"mouthLowerDownLeft\", \"mouthLowerDownRight\", \"mouthPressLeft\",\n  \"mouthPressRight\", \"mouthPucker\", \"mouthRight\", \"mouthRollLower\", \"mouthRollUpper\",\n  \"mouthShrugLower\", \"mouthShrugUpper\", \"mouthSmileLeft\", \"mouthSmileRight\", \"mouthStretchLeft\",\n  \"mouthStretchRight\", \"mouthUpperUpLeft\", \"mouthUpperUpRight\", \"noseSneerLeft\", \"noseSneerRight\",\n  // Assuming the last 3 are for tongue, though Azure docs might need to be checked for exact mapping\n  \"tongueOut\", \"tongueUp\", \"tongueDown\" // Placeholder if 55th is tongue related, adjust as needed\n];\n\ninterface AnimationKeyframe {\n  time: number; // in seconds\n  shapes: Partial<BlendShapeMap>;\n}\n\nconst FRAME_RATE_HZ = 50; // Azure viseme animation frame rate\nconst FRAME_DURATION_MS = 1000 / FRAME_RATE_HZ;\nconst BLENDSHAPE_AMPLIFICATION_FACTOR = 1.5; // Adjusted for more natural amplification\n\n// Helper for initializing an empty BlendShapeMap\nconst initialEmptyBlendshapes: BlendShapeMap = ARKitBlendshapeNamesList.reduce((acc, shapeName) => {\n  acc[shapeName] = 0;\n  return acc;\n}, {} as BlendShapeMap);\n\n// Static Blendshape mapping for Azure Viseme IDs\nconst AZURE_VISEME_ID_TO_STATIC_BLENDSHAPES: Record<number, Partial<BlendShapeMap>> = {\n  0: { jawOpen: 0, mouthClose: 1, mouthPucker: 0, mouthFunnel: 0, mouthSmileLeft: 0, mouthSmileRight: 0 }, // Silence\n  1: { jawOpen: 0.2, mouthShrugUpper: 0.1 }, // √¶, …ô,  å (e.g., cat, but, about)\n  2: { jawOpen: 0.6, mouthFunnel: 0.1 },     // …ë (e.g., father)\n  3: { jawOpen: 0.4, mouthFunnel: 0.3, mouthPucker: 0.2 }, // …î (e.g., dog, caught)\n  4: { jawOpen: 0.3, mouthSmileLeft: 0.1, mouthSmileRight: 0.1 }, // …õ,  ä (e.g., bed, book)\n  5: { jawOpen: 0.25, mouthShrugUpper: 0.2, tongueUp: 0.3 }, // …ù (e.g., her)\n  6: { jawOpen: 0.1, mouthSmileLeft: 0.4, mouthSmileRight: 0.4 }, // j, i, …™ (e.g., yes, see, sit)\n  7: { jawOpen: 0.15, mouthPucker: 0.6, mouthFunnel: 0.4 },// w, u (e.g., way, blue)\n  8: { jawOpen: 0.3, mouthPucker: 0.5, mouthFunnel: 0.3 }, // o (e.g., go - o part of o ä)\n  9: { jawOpen: 0.5, mouthFunnel: 0.2, mouthPucker: 0.3 }, // a ä (e.g., cow) - starts open, moves to pucker\n  10: { jawOpen: 0.3, mouthFunnel: 0.2, mouthSmileLeft: 0.2 }, // …î…™ (e.g., boy) - starts round, moves to smile\n  11: { jawOpen: 0.5, mouthSmileLeft: 0.3, mouthSmileRight: 0.3 }, // a…™ (e.g., buy) - starts open, moves to smile\n  12: { jawOpen: 0.1, mouthShrugUpper: 0.05 }, // h (slight breath)\n  13: { jawOpen: 0.2, mouthPucker: 0.1, tongueUp: 0.4 }, // …π (e.g., red)\n  14: { jawOpen: 0.25, tongueUp: 0.5, mouthSmileLeft: 0.1 }, // l (e.g., lay)\n  15: { jawOpen: 0.05, mouthClose: 0.5, mouthSmileLeft: 0.2, mouthSmileRight: 0.2 }, // s, z (teeth close)\n  16: { jawOpen: 0.15, mouthPucker: 0.4, mouthFunnel: 0.2 }, //  É, t É, d í,  í (e.g., shy, chin, joy, vision)\n  17: { jawOpen: 0.1, tongueUp: 0.2, mouthLowerDownLeft: 0.1, mouthLowerDownRight: 0.1 }, // √∞ (e.g., they - tongue slightly visible)\n  18: { jawOpen: 0.05, mouthLowerDownLeft: 0.3, mouthLowerDownRight: 0.3, mouthPressLeft: 0.2, mouthPressRight: 0.2 }, // f, v (upper teeth on lower lip)\n  19: { jawOpen: 0.1, tongueUp: 0.6, mouthClose: 0.2 }, // d, t, n, Œ∏ (tongue tip to alveolar ridge or teeth)\n  20: { jawOpen: 0.3, mouthShrugLower: 0.1, tongueUp: 0.2 }, // k, g, ≈ã (back of tongue)\n  21: { jawOpen: 0, mouthClose: 1, mouthPucker: 0.05 }, // p, b, m (lips together)\n};\n\n// Define default blendshapes for initialization\nconst defaultBlendShapes: BlendShapeMap = ARKitBlendshapeNamesList.reduce((acc: BlendShapeMap, key: BlendshapeKey) => {\n  acc[key] = 0;\n  return acc;\n}, {} as BlendShapeMap);\n\n// Define blendshapes for idle state, allowing autonomous blinking\nconst idleBlendShapes: Partial<BlendShapeMap> = ARKitBlendshapeNamesList.reduce((acc: Partial<BlendShapeMap>, key: BlendshapeKey) => {\n  if (key !== 'eyeBlinkLeft' && key !== 'eyeBlinkRight') {\n    // Initialize non-blink shapes to 0, consistent with defaultBlendShapes for those keys\n    acc[key] = 0;\n  }\n  return acc;\n}, {} as Partial<BlendShapeMap>);\n\n// Processed animation keyframe structure\ninterface AnimationKeyframe {\n  time: number; // in seconds, relative to the start of the audio\n  shapes: Partial<BlendShapeMap>; // ARKit blendshape values\n}\n\n// Define ChatMessage structure\ninterface ChatMessage {\n  id: string;\n  text: string;\n  sender: 'user' | 'bot';\n  timestamp: Timestamp | Date; // Allow both Firebase Timestamp and JS Date\n  emotion?: string;\n}\n\n// Define Simulation data structure (placeholder, adjust as needed)\ninterface Simulation {\n  id: string;\n  name?: string;\n  description?: string;\n  avatarUrl?: string; // Assuming simulation might also define an avatar\n  // Add other fields as necessary based on your Firestore structure\n}\n\n// Interface for methods exposed by SimulationViewInternal via ref\nexport interface SimulationViewInternalHandle {\n  forceStopHumeSpeaking: () => void;\n}\n\n// State for the SimulationViewInternal component\n// Helper function to get string name for VoiceReadyState\nfunction getVoiceReadyStateName(state: VoiceReadyState): string {\n  switch (state) {\n    case VoiceReadyState.IDLE: return \"IDLE\";\n    case VoiceReadyState.CONNECTING: return \"CONNECTING\";\n    case VoiceReadyState.OPEN: return \"OPEN\";\n    // case VoiceReadyState.CLOSING: return \"CLOSING\"; // Removed as 'CLOSING' might not be a valid enum member\n    case VoiceReadyState.CLOSED: return \"CLOSED\";\n    default:\n      // This path might be reached if 'state' is a valid VoiceReadyState member not explicitly cased,\n      // or an unexpected value. It could also be a state like 'CLOSING' if that enum member doesn't exist by that name.\n      console.warn(`[getVoiceReadyStateName] Encountered unhandled or unknown state value: ${state}`);\n      // const _exhaustiveCheck: never = state; // Ensures all enum cases are handled\n      return `UNKNOWN_STATE_(${String(state)})`;\n  }\n}\n\ninterface SimulationViewState {\n  isMicOn: boolean;\n  isCameraOn: boolean;\n  isSoundOn: boolean;\n  isChatOpen: boolean;\n  messages: ChatMessage[];\n  inputValue: string;\n  isSpeaking: boolean;\n  isAzureAudioActive: boolean; // New: Tracks if Azure audio is playing\n  isHumeAudioActive: boolean; // Tracks if Hume EVI audio is playing\n  currentEmotion: string;\n  statusMessage: string;\n  humeSessionId?: string;\n  humeAccessToken?: string;\n  humeVoiceId?: string;\n  humeVoiceName?: string;\n  humeConfigId?: string;\n  azureVoiceName?: string; \n  currentVisemeShapes: Partial<BlendShapeMap>; // Use Partial to allow omitting blink shapes for idle\n  simulationId?: string; // Added to store the simulation ID from URL\n  simulationData?: Simulation | null; // Added to store fetched simulation data\n  error?: string | null; // Added for error handling\n  isSending: boolean; // Tracks if a message is currently being sent via text input\n  manualBlendshapes: Partial<BlendShapeMap>; // For direct blendshape control via sliders\n  // Re-add properties that were removed but are used by UI\n  avatarUrl: string;\n  isCameraEnabled: boolean;\n  isMicMuted: boolean;\n  showChat: boolean;\n  prosodyDrivenBlendshapes: BlendShapeMap; // For expressions from EVI voice prosody\n}\n\ninterface SimulationViewProps {\n  simulationId?: string; \n  avatarModelUrl: string; \n  // Add any other props that SimulationView might pass down\n}\n\n// Define types for props passed from SimulationView (parent) to SimulationViewInternal\n// Placeholder types are used; replace with actual types from useVoice or Hume SDK if available\ninterface SimulationViewInternalPassedProps {\n  humeMessages: (JSONMessage | ConnectionMessage)[]; // Ensured type\n  sendUserInputToVoice: (input: string, type?: 'chat' | 'text_input') => void;\n  voiceReadyState: VoiceReadyState;\n  isHumeVoicePlaying: boolean;\n  disconnectVoice: () => void;\n  connectVoice: () => Promise<void>;\n  // lastHumeVoiceMessage is removed as it's not provided by useVoice()\n  humeConfigId?: string;\n}\n\n// Combined props for SimulationViewInternal\ntype SimulationViewInternalFullProps = SimulationViewProps & SimulationViewInternalPassedProps;\n\nconst ALL_TALK_ANIMATION_GLBS = [\n  \"/animations/M_Talking_Variations_001.glb\",\n  \"/animations/M_Talking_Variations_002.glb\",\n  \"/animations/M_Talking_Variations_003.glb\",\n  \"/animations/M_Talking_Variations_004.glb\",\n  \"/animations/M_Talking_Variations_005.glb\",\n  \"/animations/M_Talking_Variations_006.glb\",\n  \"/animations/M_Talking_Variations_007.glb\",\n  \"/animations/M_Talking_Variations_008.glb\",\n  \"/animations/M_Talking_Variations_009.glb\",\n  \"/animations/M_Talking_Variations_010.glb\",\n  \"/animations/talk.glb\"\n];\n\nconst ALL_IDLE_ANIMATION_GLBS = [\n  \"/animations/M_Standing_Idle_001.glb\",\n  \"/animations/M_Standing_Idle_002.glb\",\n  \"/animations/M_Standing_Idle_Variations_001.glb\",\n  \"/animations/M_Standing_Idle_Variations_002.glb\",\n  \"/animations/M_Standing_Idle_Variations_003.glb\",\n  \"/animations/M_Standing_Idle_Variations_004.glb\",\n  \"/animations/M_Standing_Idle_Variations_005.glb\",\n  \"/animations/M_Standing_Idle_Variations_006.glb\",\n  \"/animations/M_Standing_Idle_Variations_007.glb\",\n  \"/animations/M_Standing_Idle_Variations_008.glb\",\n  \"/animations/M_Standing_Idle_Variations_009.glb\",\n  \"/animations/M_Standing_Idle_Variations_010.glb\",\n  \"/animations/idle.glb\"\n];\n\nconst DEBOUNCE_DURATION = 250; // milliseconds\n\n// Helper function for button styles (to avoid repetition)\nconst buttonStyle: React.CSSProperties = {\n  background: 'rgba(0,0,0,0.5)',\n  border: '1px solid white',\n  borderRadius: '5px',\n  padding: '8px 12px',\n  color: 'white',\n  cursor: 'pointer',\n  fontSize: '0.9em',\n  margin: '5px',\n};\n\nconst SimulationViewInternal = React.forwardRef<\n  SimulationViewInternalHandle,\n  SimulationViewInternalFullProps\n>((props, ref): React.ReactElement | null => {\n  const { \n    humeMessages, \n    sendUserInputToVoice, \n    voiceReadyState, \n    isHumeVoicePlaying, \n    disconnectVoice, \n    connectVoice, \n    humeConfigId \n  } = props;\n\n  console.log(`[SimViewInternal FUNC_START] isHumeVoicePlaying from props: ${props.isHumeVoicePlaying}`);\n\n  console.log('[SimViewInternal RENDER START] isHumeVoicePlaying from props:', props.isHumeVoicePlaying, 'voiceReadyState:', (typeof props.voiceReadyState === 'number' && VoiceReadyState[props.voiceReadyState]) ? VoiceReadyState[props.voiceReadyState] : String(props.voiceReadyState));\n\n  const { currentUser } = useAuth(); \n  const { user, loading: userLoading, error: userError } = useUser(); \n  const navigate = useNavigate();\n  const { simulationId } = useParams<{ simulationId: string }>();\n\n  const initialState: SimulationViewState = {\n    isMicOn: false,\n    isCameraOn: true, // Default to camera being on conceptually\n    isSoundOn: true,\n    isChatOpen: false,\n    messages: [],\n    inputValue: '',\n    isSpeaking: false,\n    isAzureAudioActive: false,\n    isHumeAudioActive: false, // To track Hume EVI audio state\n    currentEmotion: 'neutral',\n    statusMessage: 'Initializing...', \n    humeVoiceName: 'KARL',\n    azureVoiceName: 'en-US-AvaNeural',\n    currentVisemeShapes: idleBlendShapes, // Initialize with idle blendshapes to allow autonomous blinking\n    manualBlendshapes: defaultBlendShapes, // Initialize manual blendshapes with defaults\n    simulationData: null,\n    error: null,\n    isSending: false,\n    // Initialize re-added properties\n    avatarUrl: props.avatarModelUrl || user?.avatarUrl || '/bro.glb', // Ensure user is checked after loading\n    isCameraEnabled: true, // UI toggle for camera view\n    isMicMuted: false,     // UI toggle for mic input to Hume\n    showChat: true,        // UI toggle for chat panel\n    prosodyDrivenBlendshapes: initialEmptyBlendshapes, // Initialize with all shapes at 0\n  };\n\n  const [state, setState] = useState<SimulationViewState>(() => {\n    const initial = {\n      isMicOn: false,\n      isCameraOn: true, // Default to camera being on conceptually\n      isSoundOn: true,\n      isChatOpen: false,\n      messages: [],\n      inputValue: '',\n      isSpeaking: false,\n      isAzureAudioActive: false, // New\n      isHumeAudioActive: false, // Added missing property\n      currentEmotion: 'neutral',\n      statusMessage: 'Initializing...', \n      humeVoiceName: 'KARL',\n      azureVoiceName: 'en-US-AvaNeural',\n      currentVisemeShapes: idleBlendShapes, // Initialize with idle blendshapes to allow autonomous blinking\n      manualBlendshapes: defaultBlendShapes, // Initialize manual blendshapes with defaults\n      simulationData: null,\n      error: null,\n      isSending: false,\n      avatarUrl: props.avatarModelUrl || user?.avatarUrl || '/bro.glb', // Ensure user is checked after loading\n      isCameraEnabled: true, // UI toggle for camera view\n      isMicMuted: false,     // UI toggle for mic input to Hume\n      showChat: true,        // UI toggle for chat panel\n      prosodyDrivenBlendshapes: initialEmptyBlendshapes, // Initialize with all shapes at 0\n    };\n    console.log(`[SimViewInternal INITIAL_STATE] Calculated initialState.avatarUrl: ${initial.avatarUrl}, Using props.avatarModelUrl: ${props.avatarModelUrl}, user?.avatarUrl: ${user?.avatarUrl}`);\n    return initial;\n  });\n\n  // Refs\n  const stateRef = useRef(state);\n\n  // --- Hume Emotion Stream Integration ---\n  const handleEmotionData = useCallback((emotion: { name: string; score: number }) => {\n    // console.log('[SimView handleEmotionData] Received emotion:', emotion);\n    setState(prev => ({ ...prev, currentEmotion: emotion.name.toLowerCase() }));\n  }, [setState]);\n\n  // TODO: API Key should ideally be passed via props or context, not directly accessed from process.env here.\n  const humeApiKey = process.env.REACT_APP_HUME_API_KEY || process.env.NEXT_PUBLIC_HUME_API_KEY;\n\n  const { \n    sendVideoFrame: sendEmotionVideoFrame, // Renamed to avoid conflict if another sendVideoFrame exists\n    connectionState: humeEmotionStreamConnectionState, \n    lastError: humeEmotionStreamLastError \n  } = useHumeEmotionStream(\n    humeApiKey,\n    handleEmotionData,\n    {\n      isEmotionDetectionActive: !!humeApiKey && state.isCameraOn, // Only active if key and camera are on\n      isVideoOn: state.isCameraOn, // Tied to the main camera state\n    }\n  );\n\n  useEffect(() => {\n    console.log('[SimView HumeEmotionStream] Connection State:', humeEmotionStreamConnectionState, 'Last Error:', humeEmotionStreamLastError);\n    if (humeEmotionStreamLastError) {\n      setState(prev => ({ ...prev, statusMessage: `Emotion Stream Error: ${humeEmotionStreamLastError}` }));\n    }\n  }, [humeEmotionStreamConnectionState, humeEmotionStreamLastError]);\n  // --- End Hume Emotion Stream Integration ---\n\n  const azureAudioRef = useRef<HTMLAudioElement | null>(null);\n  const humeAudioRef = useRef<HTMLAudioElement | null>(null);\n  const visemeFramesRef = useRef<AnimationKeyframe[]>([]);\n  const animationFrameIdRef = useRef<number | null>(null);\n  const speakingDebounceTimerRef = useRef<NodeJS.Timeout | null>(null);\n  const lastProcessedUserMessageReceivedAtRef = useRef<Date | null>(null);\n  const lastProcessedAssistantMessageIdRef = useRef<string | null>(null);\n  const isAzurePlayingRef = useRef<boolean>(false); // CASCADE: Restored for tracking Azure playback state \n  const audioBufferSourceRef = useRef<AudioBufferSourceNode | null>(null);\n  const isHumeVoicePlayingRef = useRef(props.isHumeVoicePlaying);\n\n  // Effect to keep stateRef updated with the latest state\n  useEffect(() => {\n    stateRef.current = state;\n  }, [state]);\n\n  useEffect(() => {\n    // This log helps track the state.avatarUrl specifically when it's established or changes.\n    // Note: user object might not be immediately available from useUser(), so initial user?.avatarUrl could be undefined.\n    console.log(`[SimViewInternal AVATAR_URL_STATE] state.avatarUrl is now: ${state.avatarUrl}. Initial props.avatarModelUrl was: ${props.avatarModelUrl}. Current user?.avatarUrl: ${user?.avatarUrl}`);\n  }, [state.avatarUrl, props.avatarModelUrl, user]); // Log when these key values change\n\n  useEffect(() => {\n    console.log(`[SimViewInternal MOUNT] Component did mount. Current props.avatarModelUrl: ${props.avatarModelUrl}, Current state.avatarUrl: ${state.avatarUrl}`);\n    return () => {\n      // stateRef.current can be used here to get the state at the time of unmount if direct state access is stale\n      console.log(`[SimViewInternal UNMOUNT] Component will unmount. props.avatarModelUrl at unmount: ${props.avatarModelUrl}, state.avatarUrl at unmount: ${stateRef.current.avatarUrl}`);\n    };\n  // eslint-disable-next-line react-hooks/exhaustive-deps\n  }, []); // Empty dependency array for mount/unmount effect\n\n  const handleAvatarErrorCb = useCallback((error: Error) => {\n    setState(prev => ({ ...prev, statusMessage: `Avatar Error: ${error.message}` }));\n  }, []); // setState is stable, so empty deps or [setState] is fine.\n\n  const handleAvatarLoadCb = useCallback(() => {\n    setState(prev => ({ ...prev, statusMessage: 'Avatar Loaded' }));\n  }, []); // setState is stable\n\n  // Effect to keep isHumeVoicePlayingRef updated\n  useEffect(() => {\n    isHumeVoicePlayingRef.current = props.isHumeVoicePlaying;\n    console.log(`[SimViewEffect isHumeVoicePlayingRef] Updated isHumeVoicePlayingRef.current to: ${isHumeVoicePlayingRef.current} (raw isHumeVoicePlaying: ${props.isHumeVoicePlaying})`);\n  }, [props.isHumeVoicePlaying]);\n\n  // Additional Refs\n  const messagesEndRef = useRef<HTMLDivElement | null>(null);\n  const avatarGroupRef = useRef<THREE.Group | null>(null); // For direct avatar manipulation if needed\n  const customSessionIdRef = useRef<string>(uuidv4()); // Generate a unique session ID for this component instance\n\n  const getTopEmotion = (emotions: { name: string; score: number }[]): { name: string; score: number } | undefined => {\n    if (!emotions || emotions.length === 0) return undefined;\n    return emotions.reduce((prev, current) => (prev.score > current.score) ? prev : current);\n  };\n\n  const stopVisemeAnimation = useCallback(() => {\n    if (animationFrameIdRef.current) {\n      cancelAnimationFrame(animationFrameIdRef.current);\n      animationFrameIdRef.current = null;\n    }\n    // Reset to idle blendshapes (neutral face, allows autonomous blinking)\n    setState(prev => ({ ...prev, currentVisemeShapes: idleBlendShapes, isSpeaking: false }));\n    console.log('[SimView stopVisemeAnimation] Viseme animation stopped and blendshapes reset to idle (allowing autonomous blinks).');\n  }, [setState, animationFrameIdRef, defaultBlendShapes]);\n\n  const prepareVisemeFrames = useCallback((visemeData: VisemeData): AnimationKeyframe[] => {\n    console.log('[SimView prepareVisemeFrames] Preparing viseme frames from data:', visemeData);\n    if (!visemeData || !visemeData.blendShapeFrames || visemeData.blendShapeFrames.length === 0) {\n      console.warn('[SimView prepareVisemeFrames] No blendshape data received from Azure.');\n      return [];\n    }\n\n    const frames: AnimationKeyframe[] = visemeData.blendShapeFrames.map(frame => {\n      const shapes: Record<string, number> = {};\n      ARKitBlendshapeNamesList.forEach((name: BlendshapeKey, index: number) => {\n        shapes[name] = frame.shapes[index]; // Assuming Azure provides 0-1 values directly\n      });\n      return {\n        time: frame.audioOffset, // Already in seconds from VisemeService.js\n        shapes: shapes,\n      };\n    });\n    console.log(`[SimView prepareVisemeFrames] Prepared ${frames.length} keyframes.`);\n    return frames;\n  }, []);\n\n  const humeVisemeEventsToAnimationKeyframes = useCallback((\n    visemeEvents: VisemeEvent[], // From VisemeService's convertHumeTimelineToAzureVisemes\n    audioDurationMs: number\n  ): AnimationKeyframe[] => {\n    console.log(`[SimView humeVisemeEventsToAnimationKeyframes] Called with ${visemeEvents.length} viseme events, duration: ${audioDurationMs}ms`);\n    if (!visemeEvents || visemeEvents.length === 0) {\n      if (audioDurationMs > 0) {\n        // If there's audio but no visemes, create bookend silence frames\n        console.log('[SimView humeVisemeEventsToAnimationKeyframes] No viseme events, creating silence bookends.');\n        return [\n          { time: 0, shapes: AZURE_VISEME_ID_TO_STATIC_BLENDSHAPES[0] || defaultBlendShapes },\n          { time: audioDurationMs / 1000, shapes: AZURE_VISEME_ID_TO_STATIC_BLENDSHAPES[0] || defaultBlendShapes }, // time in seconds\n        ];\n      }\n      console.log('[SimView humeVisemeEventsToAnimationKeyframes] No viseme events and no duration, returning empty keyframes.');\n      return [];\n    }\n\n    const keyframes: AnimationKeyframe[] = visemeEvents.map(event => ({\n      time: event.audioOffset / 10_000_000, // Convert 100ns ticks to seconds\n      shapes: AZURE_VISEME_ID_TO_STATIC_BLENDSHAPES[event.visemeId] || defaultBlendShapes, // Fallback to default neutral shapes\n    }));\n\n    // Ensure the animation covers the full audio duration with a final silence/neutral pose\n    const lastKeyframeTimeSeconds = keyframes.length > 0 ? keyframes[keyframes.length - 1].time : 0;\n    const audioDurationSeconds = audioDurationMs / 1000;\n\n    if (keyframes.length === 0 || lastKeyframeTimeSeconds < audioDurationSeconds) {\n        // Add a start frame if empty, or ensure the last frame extends to audio duration\n        if (keyframes.length === 0 && audioDurationSeconds > 0) {\n            keyframes.push({ time: 0, shapes: AZURE_VISEME_ID_TO_STATIC_BLENDSHAPES[0] || defaultBlendShapes });\n        }\n        keyframes.push({\n            time: audioDurationSeconds,\n            shapes: AZURE_VISEME_ID_TO_STATIC_BLENDSHAPES[0] || defaultBlendShapes, // End with silence\n        });\n    }\n    console.log(`[SimView humeVisemeEventsToAnimationKeyframes] Generated ${keyframes.length} keyframes.`);\n    return keyframes;\n  }, [defaultBlendShapes]);\n\n  const animateVisemes = useCallback((startTime: number) => {\n    const currentTime = (Date.now() - startTime) / 1000;\n    const currentFrames = visemeFramesRef.current;\n\n    if (!currentFrames || currentFrames.length === 0) {\n      console.log('[SimView animateVisemes] No frames to animate or animation ended.');\n      stopVisemeAnimation();\n      return;\n    }\n\n    let activeFrame = null;\n    for (let i = 0; i < currentFrames.length; i++) {\n      if (currentTime >= currentFrames[i].time) {\n        activeFrame = currentFrames[i];\n      } else {\n        break; \n      }\n    }\n\n    if (activeFrame) {\n      setState(prev => ({ ...prev, currentVisemeShapes: activeFrame.shapes as BlendShapeMap }));\n      console.log('[SimView animateVisemes] Updated state.currentVisemeShapes with:', activeFrame.shapes);\n    }\n\n    const lastFrameTime = currentFrames.length > 0 ? currentFrames[currentFrames.length - 1].time : 0;\n    if (currentTime > lastFrameTime + 0.5 || (!activeFrame && currentTime > 0.1 && currentFrames.length > 0)) { \n      console.log('[SimView animateVisemes] Animation sequence likely complete or past last frame time.');\n      stopVisemeAnimation();\n      return;\n    }\n    \n    animationFrameIdRef.current = requestAnimationFrame(() => animateVisemes(startTime));\n  }, [setState, stopVisemeAnimation, visemeFramesRef, animationFrameIdRef]);\n\n  const startVisemeAnimation = useCallback(() => {\n    console.log('[SimView startVisemeAnimation] CALLED. Frame count:', visemeFramesRef.current.length);\n    if (visemeFramesRef.current && visemeFramesRef.current.length > 0) {\n      console.log('[SimView startVisemeAnimation] Starting viseme animation with frames:', visemeFramesRef.current);\n      const startTime = Date.now();\n      if (animationFrameIdRef.current) {\n        cancelAnimationFrame(animationFrameIdRef.current);\n      }\n      animateVisemes(startTime);\n    } else {\n      console.log('[SimView startVisemeAnimation] No viseme frames to animate.');\n      stopVisemeAnimation();\n    }\n  }, [animateVisemes, stopVisemeAnimation, animationFrameIdRef, visemeFramesRef]);\n\n  const playHumeOutputWithVisemes = useCallback(async (audioUrl: string, timeline: HumeTimelineEvent[]) => {\n    console.log('[SimView playHumeOutputWithVisemes] Called with audio URL and timeline.');\n    setState(prev => ({ ...prev, isAzureAudioActive: false, isHumeAudioActive: true })); // Indicate Hume audio is now primary\n\n    // Stop any ongoing Azure speech and animation\n    if (azureAudioRef.current) {\n      azureAudioRef.current.pause();\n      azureAudioRef.current.src = ''; // Release resource\n    }\n    stopVisemeAnimation();\n\n    if (!humeAudioRef.current) {\n      humeAudioRef.current = new Audio();\n      humeAudioRef.current.addEventListener('ended', () => {\n        console.log('[SimView HumeAudio] Playback ended.');\n        stopVisemeAnimation();\n        setState(prev => ({ ...prev, isSpeaking: false, isHumeAudioActive: false }));\n      });\n      humeAudioRef.current.addEventListener('error', (e) => {\n        console.error('[SimView HumeAudio] Audio playback error:', e);\n        stopVisemeAnimation();\n        setState(prev => ({ ...prev, isSpeaking: false, isHumeAudioActive: false }));\n      });\n    }\n\n    humeAudioRef.current.src = audioUrl;\n    humeAudioRef.current.load();\n\n    try {\n      await humeAudioRef.current.play();\n      console.log('[SimView HumeAudio] Playback started.');\n      setState(prev => ({ ...prev, isSpeaking: true }));\n\n      // Once playback starts, audio duration should be available\n      const audioDurationSeconds = humeAudioRef.current.duration;\n      if (isNaN(audioDurationSeconds) || audioDurationSeconds === 0) {\n        console.warn('[SimView playHumeOutputWithVisemes] Hume audio duration not available or zero after play. Visemes might be incorrect.');\n        // Potentially fall back to a default or estimated duration if timeline is rich\n        // For now, we'll proceed, but this could be an issue.\n      }\n\n      console.log(`[SimView playHumeOutputWithVisemes] Hume audio duration: ${audioDurationSeconds}s`);\n\n      // Convert Hume timeline to Azure VisemeEvents (which are like standard visemes)\n      const converted = convertHumeTimelineToAzureVisemes(timeline, audioDurationSeconds);\n      \n      // Convert these VisemeEvents to AnimationKeyframes for our animation system\n      const keyframes = humeVisemeEventsToAnimationKeyframes(converted.visemeEvents, converted.audioDurationMs);\n\n      if (keyframes && keyframes.length > 0) {\n        visemeFramesRef.current = keyframes;\n        startVisemeAnimation(); // This will use visemeFramesRef.current\n      } else {\n        console.log('[SimView playHumeOutputWithVisemes] No keyframes generated for Hume output. Lip sync will not occur.');\n      }\n\n    } catch (error) {\n      console.error('[SimView playHumeOutputWithVisemes] Error playing Hume audio or processing visemes:', error);\n      stopVisemeAnimation();\n      setState(prev => ({ ...prev, isSpeaking: false, isHumeAudioActive: false }));\n    }\n  }, [stopVisemeAnimation, startVisemeAnimation, humeVisemeEventsToAnimationKeyframes, defaultBlendShapes, animateVisemes]); // Added animateVisemes to depsRef]);\n\n  const handleHumeSpeakingStarted = useCallback((message?: ChatMessage) => {\n    console.log('[SimView handleHumeSpeakingStarted] Called. Setting isSpeaking to true.');\n    setState(prev => ({ ...prev, isSpeaking: true, statusMessage: 'Bot Speaking (Hume)...' }));\n  }, [setState]);\n\n  const handleHumeSpeakingStopped = useCallback(() => {\n    console.log('[SimView handleHumeSpeakingStopped] Called.');\n    if (!stateRef.current.isAzureAudioActive) {\n      console.log('[SimView handleHumeSpeakingStopped] Azure audio not active. Stopping visemes and setting isSpeaking to false.');\n      stopVisemeAnimation(); \n      setState((prev: SimulationViewState) => ({ ...prev, isSpeaking: false, statusMessage: 'Idle' }));\n    } else {\n      console.log('[SimView handleHumeSpeakingStopped] Azure audio is active, not stopping visemes or changing speaking state from here.');\n    }\n  }, [stopVisemeAnimation, setState, stateRef]);\n\n  const handleAzureAudioEnded = useCallback(() => {\n    console.log('[SimView handleAzureAudioEnded] CALLED.');\n    console.log('[SimView handleAzureAudioEnded] Setting isAzurePlayingRef.current = false');\n    isAzurePlayingRef.current = false;\n    stopVisemeAnimation();\n    setState(prev => ({ ...prev, isAzureAudioActive: false }));\n    if (azureAudioRef.current && azureAudioRef.current.src.startsWith('blob:')) {\n      URL.revokeObjectURL(azureAudioRef.current.src);\n      azureAudioRef.current.src = ''; \n      azureAudioRef.current.srcObject = null;\n      console.log('[SimView handleAzureAudioEnded] Revoked blob URL and reset azureAudioRef src.');\n    }\n  }, [stopVisemeAnimation, setState, azureAudioRef]);\n\n  const handleAzureAudioError = useCallback((event: React.SyntheticEvent<HTMLAudioElement, Event>) => {\n    const audioElement = event.target as HTMLAudioElement;\n    console.error('[SimView handleAzureAudioError] Azure audio playback error:', audioElement.error);\n    stopVisemeAnimation();\n    setState(prev => ({ ...prev, isAzureAudioActive: false, statusMessage: 'Error playing Azure audio.' }));\n    if (azureAudioRef.current && azureAudioRef.current.src.startsWith('blob:')) {\n      URL.revokeObjectURL(azureAudioRef.current.src);\n      azureAudioRef.current.src = '';\n      azureAudioRef.current.srcObject = null;\n      console.log('[SimView handleAzureAudioError] Revoked blob URL and reset azureAudioRef src after error.');\n    }\n  }, [stopVisemeAnimation, setState, azureAudioRef]);\n\n  // Test function (can be removed or kept for debugging)\n  const generateAndAnimateVisemesFromText = useCallback(async (text: string, voiceName?: string) => {\n    console.log(`[SimView generateAndAnimateVisemesFromText] Test function called with text: \"${text}\", voice: ${voiceName}`);\n    if (!azureAudioRef.current) {\n      console.error(\"[SimView generateAndAnimateVisemesFromText] azureAudioRef.current is null.\");\n      return;\n    }\n    try {\n      setState(prev => ({ ...prev, statusMessage: 'Testing Visemes...', isSpeaking: true, isAzureAudioActive: true }));\n      if (!azureAudioRef.current.paused) {\n        azureAudioRef.current.pause();\n        azureAudioRef.current.currentTime = 0;\n      }\n      const serviceResult = await untypedSynthesizeSpeechWithVisemes(text, voiceName || stateRef.current.azureVoiceName);\n\n      // ===== DETAILED LOGGING START =====\n      console.log('[SimView generateAndAnimateVisemesFromText] Full serviceResult from VisemeService:', JSON.stringify(serviceResult, null, 2));\n      if (serviceResult?.visemeData?.blendShapeFrames && serviceResult.visemeData.blendShapeFrames.length > 0) {\n        console.log('[SimView generateAndAnimateVisemesFromText] First 3 blendShapeFrames from serviceResult:');\n        for (let i = 0; i < Math.min(3, serviceResult.visemeData.blendShapeFrames.length); i++) {\n          console.log(`  Frame ${i}: audioOffset=${serviceResult.visemeData.blendShapeFrames[i].audioOffset}, shapes=${JSON.stringify(serviceResult.visemeData.blendShapeFrames[i].shapes?.slice(0,5))}...`);\n        }\n        const lastFrameIdx = serviceResult.visemeData.blendShapeFrames.length - 1;\n        if (lastFrameIdx >= 0) {\n            console.log(`  Last Frame ${lastFrameIdx}: audioOffset=${serviceResult.visemeData.blendShapeFrames[lastFrameIdx].audioOffset}, shapes=${JSON.stringify(serviceResult.visemeData.blendShapeFrames[lastFrameIdx].shapes?.slice(0,5))}...`);\n        }\n      } else {\n        console.log('[SimView generateAndAnimateVisemesFromText] serviceResult.visemeData.blendShapeFrames is missing, empty, or not an array.');\n      }\n      if (serviceResult && 'audioDurationMs' in serviceResult) {\n        console.log('[SimView generateAndAnimateVisemesFromText] serviceResult.audioDurationMs:', serviceResult.audioDurationMs);\n      } else {\n        console.log('[SimView generateAndAnimateVisemesFromText] serviceResult.audioDurationMs is not available.');\n      }\n      // ===== DETAILED LOGGING END =====\n\n      if (serviceResult && serviceResult.audioData && serviceResult.visemeData) {\n        const preparedFrames = prepareVisemeFrames(serviceResult.visemeData);\n        if (preparedFrames && preparedFrames.length > 0) {\n          visemeFramesRef.current = preparedFrames;\n          startVisemeAnimation();\n        } else {\n          stopVisemeAnimation();\n        }\n        const audioBlob = new Blob([serviceResult.audioData], { type: 'audio/mpeg' });\n        const audioUrl = URL.createObjectURL(audioBlob);\n        azureAudioRef.current.srcObject = null;\n        azureAudioRef.current.src = audioUrl;\n        azureAudioRef.current.play().catch(e => {\n          console.error('[SimView generateAndAnimateVisemesFromText] Error playing test audio:', e);\n          handleAzureAudioEnded();\n        });\n      } else {\n        console.error('[SimView generateAndAnimateVisemesFromText] Failed to get audio/viseme data for test.');\n        handleAzureAudioEnded();\n      }\n    } catch (error) {\n      console.error('[SimView generateAndAnimateVisemesFromText] Error in test function:', error);\n      handleAzureAudioEnded();\n    }\n  }, [untypedSynthesizeSpeechWithVisemes, prepareVisemeFrames, startVisemeAnimation, stopVisemeAnimation, handleAzureAudioEnded, setState, stateRef, azureAudioRef, visemeFramesRef]);\n\n  const processAndPlayAzureSpeech = useCallback(async (text: string) => {\n    console.log('[SimView processAndPlayAzureSpeech] CALLED. Text:', text);\n\n  if (!azureAudioRef.current) {\n    console.error(\"[SimView processAndPlayAzureSpeech] azureAudioRef.current is null. Cannot play Azure audio.\");\n    handleAzureAudioEnded(); // Call to clean up state\n    return;\n  }\n\n  setState(prev => ({ ...prev, isSpeaking: true, isAzureAudioActive: true, statusMessage: 'Bot Speaking (Azure)...' }));\n\n    if (azureAudioRef.current && !azureAudioRef.current.paused) {\n      console.warn('[SimView processAndPlayAzureSpeech] Azure audio player (azureAudioRef) was active. Stopping existing playback.');\n      azureAudioRef.current.pause();\n      azureAudioRef.current.currentTime = 0;\n    }\n\n    try {\n      console.log('[SimView processAndPlayAzureSpeech] Attempting to synthesize with voice:', state.azureVoiceName);\n      const resolvedVoiceName = state.azureVoiceName;\n      // Ensure untypedSynthesizeSpeechWithVisemes is the imported one, not the placeholder\n      const serviceResult = await untypedSynthesizeSpeechWithVisemes(text, resolvedVoiceName);\n      console.log('[SimView processAndPlayAzureSpeech] serviceResult (summary):', JSON.stringify(serviceResult ? { audioDataLength: serviceResult.audioData?.byteLength, blendShapeFramesLength: serviceResult.visemeData?.blendShapeFrames?.length, standardVisemesLength: serviceResult.visemeData?.standardVisemes?.length } : null));\n\n      if (serviceResult && serviceResult.audioData && serviceResult.visemeData) {\n        console.log('[SimView processAndPlayAzureSpeech] VisemeData received from Azure. BlendShapeFrames count:', serviceResult.visemeData.blendShapeFrames?.length, 'StandardVisemes count:', serviceResult.visemeData.standardVisemes?.length, '. Now calling prepareVisemeFrames.');\n        const preparedFrames = prepareVisemeFrames(serviceResult.visemeData);\n        console.log('[SimView processAndPlayAzureSpeech] preparedFrames:', preparedFrames); // CASCADE ADDED LOG\n        if (preparedFrames && preparedFrames.length > 0) {\n          visemeFramesRef.current = preparedFrames;\n          console.log('[SimView processAndPlayAzureSpeech] visemeFramesRef.current SET. Length:', visemeFramesRef.current.length);\n        } else {\n          visemeFramesRef.current = [];\n          console.log('[SimView processAndPlayAzureSpeech] preparedFrames EMPTY or invalid. visemeFramesRef.current set to empty array.');\n        }\n        const audioBlob = new Blob([serviceResult.audioData], { type: 'audio/mpeg' });\n        const audioUrl = URL.createObjectURL(audioBlob);\n        azureAudioRef.current.srcObject = null;\n        azureAudioRef.current.src = audioUrl;\n        azureAudioRef.current.play().then(() => {\n          console.log('[SimView processAndPlayAzureSpeech] Azure audio playback REALLY STARTED.');\n          if (visemeFramesRef.current && visemeFramesRef.current.length > 0) {\n            console.log('[SimView processAndPlayAzureSpeech] Calling startVisemeAnimation after audio play().then()');\n            startVisemeAnimation();\n          } else {\n            console.log('[SimView processAndPlayAzureSpeech] No viseme frames to animate after audio play().then(), stopping animation.');\n            stopVisemeAnimation(); // Ensure any prior animation is stopped\n          }\n        }).catch(e => {\n          console.error('[SimView processAndPlayAzureSpeech] Error playing Azure audio:', e);\n          handleAzureAudioEnded(); // Ensure state is cleaned up\n        });\n        console.log('[SimView processAndPlayAzureSpeech] Azure audio playback INITIATED. Setting isAzurePlayingRef.current = true');\n        isAzurePlayingRef.current = true;\n      } else {\n        console.error('[SimView processAndPlayAzureSpeech] Failed to synthesize Azure speech or missing audio data. Result:', serviceResult);\n        handleAzureAudioEnded(); // Ensure state is cleaned up\n      }\n    } catch (error) {\n      console.error('[SimView processAndPlayAzureSpeech] Error during Azure TTS synthesis:', error);\n      handleAzureAudioEnded(); // Ensure state is cleaned up\n    }\n  }, [untypedSynthesizeSpeechWithVisemes, azureAudioRef, handleHumeSpeakingStopped, prepareVisemeFrames, startVisemeAnimation, stopVisemeAnimation, isHumeVoicePlayingRef, handleAzureAudioEnded, visemeFramesRef]);\n\n  // ... (rest of the code remains the same)\n\n  useEffect(() => {\n    if (props.humeMessages.length === 0) {\n      return;\n    }\n\n    const currentMessage = props.humeMessages[props.humeMessages.length - 1];\n    if (!currentMessage) return;\n\n    // Deduplication logic\n    if (currentMessage.type === 'user_message') {\n      if (lastProcessedUserMessageReceivedAtRef.current && currentMessage.receivedAt <= lastProcessedUserMessageReceivedAtRef.current) {\n        // console.log(`[SimView] User message at ${currentMessage.receivedAt} already processed or older. Skipping.`);\n        return;\n      }\n    } else if (currentMessage.type === 'assistant_message' && currentMessage.id) {\n      if (currentMessage.id === lastProcessedAssistantMessageIdRef.current) {\n        // console.log(`[SimView] Assistant message ID ${currentMessage.id} already processed. Skipping.`);\n        return;\n      }\n    }\n\n    // Log assistant_message to check for face/viseme data, specifically if 'models' are present\n    if (currentMessage.type === 'assistant_message' && currentMessage.models) {\n      console.log('[Hume EVI Message Inspector] Assistant Message with Models:', JSON.stringify(currentMessage, null, 2));\n    }\n\n    switch (currentMessage.type) {\n      case 'user_message': {\n        if (!currentMessage.message || typeof currentMessage.message.content !== 'string' || currentMessage.message.role !== 'user') {\n          console.warn('[SimView] User message content/role issue or type mismatch. Skipping. Full message:', JSON.stringify(currentMessage, null, 2));\n          break;\n        }\n        console.log(`[SimView] user_message: Role: ${currentMessage.message.role}. User text:`, currentMessage.message.content);\n        const newUserChatMessage: ChatMessage = {\n          id: uuidv4(),\n          text: currentMessage.message.content,\n          sender: 'user',\n          timestamp: currentMessage.receivedAt,\n        };\n        setState((prev: SimulationViewState) => ({ ...prev, messages: [...prev.messages, newUserChatMessage] }));\n        lastProcessedUserMessageReceivedAtRef.current = currentMessage.receivedAt;\n        break;\n      }\n      case 'assistant_message': {\n        console.log('[SimView] ENTERING assistant_message case block. Message ID:', currentMessage.id);\n        if (currentMessage.models?.prosody?.scores) {\n          console.log('[SimView] DETECTED PROSODY SCORES:', JSON.stringify(currentMessage.models.prosody.scores, null, 2));\n        } else {\n          console.log('[SimView] Assistant Message, but NO prosody.scores. Models:', JSON.stringify(currentMessage.models, null, 2));\n        }\n\n        if (!currentMessage.message || typeof currentMessage.message.content !== 'string' || currentMessage.message.role !== 'assistant') {\n          console.warn('[SimView] Assistant message content/role issue or type mismatch. Skipping. Full message:', JSON.stringify(currentMessage, null, 2));\n          break;\n        }\n        console.log(`[SimView] assistant_message: Role: ${currentMessage.message.role}. Assistant text:`, currentMessage.message.content);\n        const newAssistantChatMessage: ChatMessage = {\n          id: currentMessage.id || uuidv4(),\n          text: currentMessage.message.content,\n          sender: 'bot', // Changed 'assistant' to 'bot'\n          timestamp: currentMessage.receivedAt,\n          // emotion: getTopEmotion(currentMessage.models?.prosody?.scores)\n        };\n        let newEmotions: Emotion[] = [{ name: 'neutral', score: 1 }]; // Default\n        let emotionsSource = 'default (neutral)';\n\n        // According to Hume EVI docs and previous logs, prosody scores are the primary source for general emotion.\n        // The 'predictions' field under prosody or face might contain more detailed expression data if configured,\n        // but 'scores' under 'prosody' is for overall emotional tone of speech.\n        if (currentMessage.models?.prosody?.scores && Array.isArray(currentMessage.models.prosody.scores) && currentMessage.models.prosody.scores.length > 0) {\n          // Ensure the scores match the Emotion type structure (name, score)\n          const potentialEmotions = currentMessage.models.prosody.scores.filter(\n            (s: any) => typeof s.name === 'string' && typeof s.score === 'number'\n          );\n          if (potentialEmotions.length > 0) {\n            newEmotions = potentialEmotions;\n            emotionsSource = 'prosody.scores';\n          } else {\n            console.warn('[SimView] Hume prosody.scores present but items do not match Emotion structure:', JSON.stringify(currentMessage.models.prosody.scores));\n            emotionsSource = 'prosody.scores (invalid structure)';\n          }\n        } else {\n          console.log('[SimView] No prosody.scores found in Hume message. Using default neutral emotion.');\n        }\n        console.log(`[SimView] Extracted emotions from Hume message (source: ${emotionsSource}):`, JSON.stringify(newEmotions));\n\n        setState((prev: SimulationViewState) => {\n          const prosodyScoresForBlendshapes = newEmotions.reduce((acc: Record<string, number>, emotion: any) => {\n            acc[emotion.name] = emotion.score;\n            return acc;\n          }, {});\n          \n          console.log('[SimView assistant_message] newEmotions feeding into blendshapes:', JSON.stringify(newEmotions));\n          console.log('[SimView assistant_message] prosodyScoresForBlendshapes for prosodyToBlendshapes:', JSON.stringify(prosodyScoresForBlendshapes));\n          \n          const newProsodyBlendshapes = prosodyToBlendshapes(prosodyScoresForBlendshapes);\n          \n          console.log(`[SimView assistant_message] newProsodyBlendshapes from prosodyToBlendshapes (count: ${Object.keys(newProsodyBlendshapes).length}):`, JSON.stringify(newProsodyBlendshapes));\n\n          return {\n            ...prev,\n            messages: [...prev.messages, newAssistantChatMessage],\n            currentDetectedEmotions: newEmotions, // newEmotions is from the outer scope, reflecting Hume's detected emotion\n            isSpeaking: props.isHumeVoicePlaying,\n            statusMessage: props.isHumeVoicePlaying ? 'Bot Speaking (Hume EVI)' : (prev.statusMessage.includes('Azure') ? prev.statusMessage : 'Bot Idle'),\n            prosodyDrivenBlendshapes: newProsodyBlendshapes, // Use the dynamically calculated blendshapes\n          };\n        });\n\n        if (currentMessage.message.content) {\n          // console.log('[SimView] Assistant message content present. Azure viseme processing currently disabled for MVP.');\n          // processAndPlayAzureSpeech(currentMessage.message.content); // MVP Strategy: Disabled Azure visemes\n        }\n        if (currentMessage.id) {\n          lastProcessedAssistantMessageIdRef.current = currentMessage.id;\n        }\n        break;\n      }\n      case 'assistant_end': {\n        console.log('[SimView] assistant_end message received:', currentMessage);\n        // Perform any necessary cleanup or state changes for assistant ending speech.\n        break;\n      }\n      case 'error': { // This is for WebSocketError\n        console.error('[SimView] WebSocketError message received:', JSON.stringify(currentMessage, null, 2));\n        setState((prev: SimulationViewState) => ({ ...prev, statusMessage: `Hume EVI Error: ${currentMessage.message}`}));\n        break;\n      }\n      case 'chat_metadata': {\n        console.log('[SimView] chat_metadata message received:', currentMessage);\n        break;\n      }\n      case 'user_interruption': {\n        console.log('[SimView] user_interruption message received:', currentMessage);\n        if (azureAudioRef.current) {\n          azureAudioRef.current.pause();\n          azureAudioRef.current.src = '';\n        }\n        stopVisemeAnimation();\n        setState((prev: SimulationViewState) => ({ ...prev, isSpeaking: false, isAzureAudioActive: false, statusMessage: 'User interrupted bot.' }));\n        break;\n      }\n      case 'tool_call': {\n        console.log('[SimView] tool_call message received:', currentMessage);\n        break;\n      }\n      case 'tool_response': {\n        console.log('[SimView] tool_response message received:', currentMessage);\n        break;\n      }\n      case 'tool_error': {\n        console.log('[SimView] tool_error message received:', currentMessage);\n        break;\n      }\n      default: {\n        // const _exhaustiveCheck: never = currentMessage; // For exhaustive type checking\n        console.log('[SimView] Received unhandled or unexpected message type:', (currentMessage as any).type, 'Full message:', JSON.stringify(currentMessage, null, 2));\n        break;\n      }\n    }\n    // Cleanup function for this useEffect\n    return () => {\n      if (speakingDebounceTimerRef.current) {\n        clearTimeout(speakingDebounceTimerRef.current);\n        speakingDebounceTimerRef.current = null;\n      }\n    };\n  }, [props.humeMessages, processAndPlayAzureSpeech, props.isHumeVoicePlaying]);\n\n  // The useEffect that synchronized isSpeaking with props.isHumeVoicePlaying has been removed\n  // to prevent 'Maximum update depth exceeded' error. \n  // isSpeaking and statusMessage will be derived or handled by Azure's lifecycle directly.\n\n  // Effect to manage isSpeaking state based on Hume's voice activity\n  useEffect(() => {\n    if (props.isHumeVoicePlaying) {\n      // Hume started speaking\n      if (speakingDebounceTimerRef.current) {\n        clearTimeout(speakingDebounceTimerRef.current);\n        speakingDebounceTimerRef.current = null;\n      }\n      // Check if we are not already in 'isSpeaking' state from Hume to avoid redundant calls if handleHumeSpeakingStarted is complex\n      // However, handleHumeSpeakingStarted itself should be idempotent or handle this.\n      handleHumeSpeakingStarted(); \n    } else {\n      // Hume stopped speaking\n      // Only try to stop if we were previously speaking (stateRef.current.isSpeaking)\n      // and Azure is not playing (isAzurePlayingRef.current is false, which is always the case in MVP strategy)\n      if (stateRef.current.isSpeaking) { \n        if (speakingDebounceTimerRef.current) {\n          clearTimeout(speakingDebounceTimerRef.current);\n        }\n        speakingDebounceTimerRef.current = setTimeout(() => {\n          console.log('[SimView useEffect HumeVoicePlaying] Debounced: Hume stopped, calling handleHumeSpeakingStopped.');\n          handleHumeSpeakingStopped();\n        }, DEBOUNCE_DURATION);\n      }\n    }\n\n    // Cleanup timer on component unmount or if dependencies change causing effect re-run before timer fires\n    return () => {\n      if (speakingDebounceTimerRef.current) {\n        clearTimeout(speakingDebounceTimerRef.current);\n        speakingDebounceTimerRef.current = null;\n      }\n    };\n  }, [props.isHumeVoicePlaying, handleHumeSpeakingStarted, handleHumeSpeakingStopped, stateRef, speakingDebounceTimerRef]);\n\n  const handleInputChange = useCallback((event: React.ChangeEvent<HTMLInputElement>) => {\n    setState(prev => ({ ...prev, inputValue: event.target.value }));\n  }, [setState]);\n\n  const handleSendMessage = useCallback(async (event?: React.FormEvent<HTMLFormElement>) => {\n    if (event) {\n      event.preventDefault();\n    }\n    const text = stateRef.current.inputValue.trim();\n    if (!text) return;\n\n    const newUserMessage = {\n      id: uuidv4(),\n      text: text,\n      sender: 'user' as const,\n      timestamp: new Date(),\n    };\n\n    setState(prev => ({\n      ...prev,\n      messages: [...prev.messages, newUserMessage],\n      inputText: '', \n    }));\n\n    console.log('[SimView] Sending user message to Hume EVI:', text);\n    if (props.sendUserInputToVoice) {\n        props.sendUserInputToVoice(text); // Corrected: sendUserInput expects a string\n    } else {\n        console.warn('[SimView] sendUserInput is not available. Message not sent to Hume.');\n    }\n    \n    // if (props.simulationId && currentUser?.uid) {\n    //   try {\n    //     await saveChatMessageToFirebase(props.simulationId, currentUser.uid, newUserMessage);\n    //     console.log('[SimView] User message saved to Firebase.');\n    //   } catch (error) {\n    //     console.error('[SimView] Error saving user message to Firebase:', error);\n    //   }\n    // }\n  }, [setState, props.sendUserInputToVoice, props.simulationId, currentUser, stateRef, customSessionIdRef /*, saveChatMessageToFirebase */]);\n\n  const handleAvatarEmotionDetected = useCallback((emotionData: any) => {\n    console.log('[SimView] Avatar Emotion Detected:', emotionData);\n  }, []);\n\n  const handleAvatarError = useCallback((error: Error) => { // Changed 'any' to 'Error'\n    console.error(\"[SimView] Avatar Error:\", error);\n    // Log the full error object for more details\n    console.error(\"[SimView] Full avatar error object:\", error); \n    const errorMessage = error instanceof Error ? error.message : String(error);\n    setState(prev => ({ ...prev, statusMessage: `Avatar error: ${errorMessage}` }));\n  }, [setState]);\n\n  const handleAvatarLoad = useCallback(() => {\n    console.log('[SimView] Avatar Loaded successfully.');\n    setState(prev => ({ ...prev, statusMessage: 'Avatar loaded.' }));\n  }, [setState]);\n\n  const toggleCamera = useCallback(() => {\n    setState(prev => ({ ...prev, isCameraEnabled: !prev.isCameraEnabled })); // This was correct, assuming isCameraEnabled is in state\n  }, [setState]);\n\n  const toggleMic = useCallback(() => {\n    setState((prev: SimulationViewState) => {\n      console.log(`[SimView toggleMic] Clicked. Current voiceReadyState: ${getVoiceReadyStateName(props.voiceReadyState)}, current isMicMuted (before toggle): ${prev.isMicMuted}`);\n      const newMicMutedState = !prev.isMicMuted;\n      if (newMicMutedState) { // Mic is being muted\n        console.log('[SimView toggleMic] Mic muted. Disconnecting Hume Voice.');\n        props.disconnectVoice();\n      } else { // Mic is being unmuted\n        console.log(`[SimView toggleMic] Mic unmuted. Attempting to connect Hume Voice.`);\n        console.log(`[SimView toggleMic] Current state values - humeAccessToken: ${stateRef.current.humeAccessToken ? 'SET' : 'NOT SET'}, humeConfigId: ${stateRef.current.humeConfigId || 'NOT SET (uses default from Provider)'}`);\n        \n        props.connectVoice()\n          .then(() => {\n            console.log('[SimView toggleMic] Hume Voice connect() promise resolved successfully.');\n            // setState(s => ({...s, statusMessage: 'Mic active, connected to Hume.'})); // Status will be updated by readyState changes\n          })\n          .catch(error => {\n            console.error('[SimView toggleMic] Error connecting Hume Voice:', error);\n            setState((s: SimulationViewState) => ({...s, statusMessage: `Error connecting mic: ${(error as Error).message}`}));\n          });\n      }\n      // Update mic state for UI and internal logic\n      return { ...prev, isMicMuted: newMicMutedState, isMicOn: !newMicMutedState, statusMessage: newMicMutedState ? 'Mic muted.' : 'Mic unmuted, attempting connection...' };\n    });\n  }, [setState, props.connectVoice, props.disconnectVoice, stateRef, props.voiceReadyState]);\n\n  const toggleChat = useCallback(() => {\n    setState((prev: SimulationViewState) => ({ ...prev, showChat: !prev.showChat })); // This was correct, assuming showChat is in state\n  }, [setState]);\n\n  const toggleSound = useCallback(() => {\n    setState((prev: SimulationViewState) => {\n      const newIsSoundOn = !prev.isSoundOn;\n      if (azureAudioRef.current) {\n        azureAudioRef.current.muted = !newIsSoundOn;\n      }\n      return { ...prev, isSoundOn: newIsSoundOn, statusMessage: newIsSoundOn ? 'Sound unmuted.' : 'Sound muted.' };\n    });\n  }, [setState, azureAudioRef]);\n\n  const forceStopAllAudio = useCallback(() => {\n    console.log('[SimViewInternal] forceStopAllAudio called.');\n    // Stop Azure TTS Audio & Visemes\n    if (azureAudioRef.current && !azureAudioRef.current.paused) {\n      azureAudioRef.current.pause();\n      if (azureAudioRef.current.src && azureAudioRef.current.src.startsWith('blob:')) {\n        URL.revokeObjectURL(azureAudioRef.current.src);\n      }\n      azureAudioRef.current.src = '';\n    }\n    stopVisemeAnimation(); // This also sets isSpeaking to false\n\n    setState((prev: SimulationViewState) => ({\n      ...prev,\n      // isHumeVoicePlaying: false, // This state is more related to the Hume connection itself\n      isAzureAudioActive: false,\n      // isSpeaking is handled by stopVisemeAnimation\n      statusMessage: 'Local audio and speech visualization stopped.',\n    }));\n    // If stopping the Hume connection itself is desired from this button,\n    // SimulationViewInternal would need a prop callback from SimulationView to trigger voice.disconnect().\n  }, [azureAudioRef, stopVisemeAnimation, setState]);\n\n  useImperativeHandle(ref, () => ({\n    forceStopHumeSpeaking: () => {\n      console.log('[SimViewInternal forceStopHumeSpeaking] Imperatively called.');\n      if (azureAudioRef.current && !azureAudioRef.current.paused) {\n        azureAudioRef.current.pause();\n        if (azureAudioRef.current.src && azureAudioRef.current.src.startsWith('blob:')) {\n          URL.revokeObjectURL(azureAudioRef.current.src);\n        }\n        azureAudioRef.current.src = '';\n        // Consider azureAudioRef.current.load(); if issues persist after src reset\n      }\n      stopVisemeAnimation(); // This resets blendshapes and sets isSpeaking to false\n      setState((prevState: SimulationViewState) => ({\n        ...prevState,\n        // isSpeaking: false, // Handled by stopVisemeAnimation\n        isAzureAudioActive: false,\n        statusMessage: prevState.isAzureAudioActive ? 'Azure speech stopped via external call.' : 'Hume speech stop requested externally.',\n      }));\n      // Note: If Hume EVI has its own audio output that bypasses azureAudioRef (e.g. direct Web Audio API usage by useVoice),\n      // additional logic might be needed here to stop that. The current implementation assumes\n      // isHumeVoicePlaying reflects this and that the parent (SimulationView) handles disconnects/reconnects.\n    }\n  }), [azureAudioRef, stopVisemeAnimation, setState]);\n\n  // --- Derived state logic for isSpeaking and statusMessage ---\n  let derivedIsSpeaking = state.isSpeaking;\n  let derivedStatusMessage = state.statusMessage;\n\n  // Prioritize Azure's state if it's active\n  if (state.isAzureAudioActive) {\n    // Azure is active, its lifecycle methods (e.g., processAndPlayAzureSpeech) directly set state.isSpeaking and state.statusMessage.\n    // So, derivedIsSpeaking and derivedStatusMessage are already correctly reflecting Azure's state from state.isSpeaking and state.statusMessage.\n  } else if (props.isHumeVoicePlaying) {\n    // Hume is playing, and Azure is NOT active.\n    derivedIsSpeaking = true;\n    derivedStatusMessage = 'Bot Speaking (Hume EVI)';\n  } else {\n    // Neither Hume nor Azure is actively playing/speaking.\n    derivedIsSpeaking = false;\n    derivedStatusMessage = 'Idle';\n  }\n  // --- End of derived state logic ---\n\n  console.log('[SimViewInternal BEFORE RETURN] isHumeVoicePlaying:', props.isHumeVoicePlaying, 'Current state.isSpeaking:', state.isSpeaking, 'DerivedIsSpeaking:', derivedIsSpeaking, 'DerivedStatusMessage:', derivedStatusMessage);\n\n  return (\n    <div className=\"simulation-view\" style={{ display: 'flex', height: '100vh', flexDirection: 'column', background: '#282c34' }}>\n      {/* Header/Status Bar */}\n      <div className=\"status-bar\" style={{ padding: '10px', background: '#20232a', color: 'white', textAlign: 'center', fontSize: '0.9em', flexShrink: 0 }}>\n        Status: {derivedStatusMessage} | Hume: {(typeof props.voiceReadyState === 'number' && VoiceReadyState[props.voiceReadyState]) ? VoiceReadyState[props.voiceReadyState] : String(props.voiceReadyState)} | Speaking: {derivedIsSpeaking ? \"Yes\" : \"No\"} | Hume Playing: {props.isHumeVoicePlaying ? \"Yes\" : \"No\"}\n      </div>\n\n      {/* Test Mode Toggle */}\n      <TestModeToggle\n        onMockMessage={(message) => {\n          console.log('üß™ Mock message received:', message);\n          // Simulate processing the mock message like a real Hume message\n          if (message.prosody) {\n            // Convert array format [{name, score, timestamp}] to Record<string, number>\n            const prosodyRecord = message.prosody.reduce((acc: Record<string, number>, emotion: any) => {\n              acc[emotion.name] = emotion.score;\n              return acc;\n            }, {});\n            const prosodyBlendshapes = prosodyToBlendshapes(prosodyRecord);\n            setState(prev => ({ ...prev, prosodyDrivenBlendshapes: prosodyBlendshapes }));\n          }\n          if (message.timeline) {\n            // Simulate viseme processing\n            const mockVisemes = { jawOpen: 0.4, mouthFunnel: 0.3 };\n            setState(prev => ({ ...prev, currentVisemeShapes: mockVisemes }));\n          }\n          if (message.type === 'audio_output') {\n            setState(prev => ({ ...prev, isSpeaking: true }));\n            // Simulate audio ending after 2 seconds\n            setTimeout(() => {\n              setState(prev => ({ ...prev, isSpeaking: false, currentVisemeShapes: {} }));\n            }, 2000);\n          }\n        }}\n        onMockProsody={(emotions) => {\n          console.log('üß™ Mock prosody received:', emotions);\n          // Convert array format to Record format\n          const prosodyRecord = emotions.reduce((acc: Record<string, number>, emotion: any) => {\n            acc[emotion.name] = emotion.score;\n            return acc;\n          }, {});\n          const prosodyBlendshapes = prosodyToBlendshapes(prosodyRecord);\n          setState(prev => ({ ...prev, prosodyDrivenBlendshapes: prosodyBlendshapes }));\n        }}\n        onMockVisemes={(visemes) => {\n          console.log('üß™ Mock visemes received:', visemes);\n          setState(prev => ({ ...prev, currentVisemeShapes: visemes }));\n        }}\n      />\n\n      {/* Main Content Area (Avatar + Chat) */}\n      <div style={{ display: 'flex', flexGrow: 1, position: 'relative', overflow: 'hidden' }}>\n        {/* Avatar Display Area */}\n        <div className=\"avatar-container\" style={{ flexGrow: 1, display: 'flex', justifyContent: 'center', alignItems: 'center', position: 'relative', background: '#333740' }}>\n          {(() => { console.log('[SimViewInternal RENDER] Passing DERIVED isSpeaking to EmotionDrivenAvatar:', derivedIsSpeaking); return null; })()}\n          <EmotionDrivenAvatar\n            directBlendshapes={{ ...state.manualBlendshapes, ...state.prosodyDrivenBlendshapes }} // Combine manual and prosody blendshapes\n            key={state.avatarUrl} // Add stable key to prevent remounts\n            ref={avatarGroupRef} // Pass the ref\n            avatarUrl={state.avatarUrl} // Use state.avatarUrl\n            isSpeaking={derivedIsSpeaking} // Pass DERIVED speaking state\n            visemeData={state.currentVisemeShapes} // Attempting to use visemeData to resolve TS error\n            currentEmotion={state.currentEmotion} // Pass current emotion\n            idleShapes={idleBlendShapes} // Pass idle shapes for blinking/restingolve TS error\n            detectedEmotions={state.currentEmotion ? [{ name: state.currentEmotion, score: 1.0 }] : []} // Pass current emotion\n            cameraEnabled={state.isCameraEnabled} // Pass camera enabled state\n            talkAnimationPaths={ALL_TALK_ANIMATION_GLBS} \n            idleAnimationPaths={ALL_IDLE_ANIMATION_GLBS} \n            onError={handleAvatarErrorCb}\n            onLoad={handleAvatarLoadCb}\n          />\n          <button \n            onClick={() => generateAndAnimateVisemesFromText(\"Hello world, this is a test.\", state.azureVoiceName || \"en-US-JennyNeural\")}\n            style={{ ...buttonStyle, position: 'absolute', top: '10px', left: '10px', zIndex: 10 }}\n          >\n            Test Visemes\n          </button>\n        </div>\n\n        {/* Chat UI Area */} \n        {state.showChat && (\n          <div className=\"chat-ui\" style={{\n            width: '350px',\n            minWidth: '300px',\n            background: 'rgba(30, 33, 40, 0.95)',\n            color: 'white',\n            display: 'flex',\n            flexDirection: 'column',\n            padding: '15px',\n            borderLeft: '1px solid #4f5461',\n            boxSizing: 'border-box',\n            position: 'absolute',\n            right: 0,\n            top: 0,\n            bottom: 0,\n            zIndex: 5,\n            transition: 'transform 0.3s ease-in-out',\n            transform: 'translateX(0%)'\n          }}>\n            <div style={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center', marginBottom: '15px', flexShrink: 0 }}>\n              <h3 style={{ margin: 0, fontSize: '1.2em' }}>Conversation</h3>\n              <button onClick={toggleChat} style={{ background: 'none', border: 'none', color: '#aaa', cursor: 'pointer', fontSize: '1.5em', padding: '5px' }}>\n                X\n              </button>\n            </div>\n            <div className=\"messages-area\" style={{ flexGrow: 1, overflowY: 'auto', marginBottom: '15px', paddingRight: '10px' }}>\n              {state.messages.map((msg) => (\n                <div key={msg.id} className={`message ${msg.sender}`} style={{\n                  marginBottom: '12px',\n                  padding: '10px 15px',\n                  borderRadius: '18px',\n                  maxWidth: '85%',\n                  alignSelf: msg.sender === 'user' ? 'flex-end' : 'flex-start',\n                  background: msg.sender === 'user' ? '#007bff' : '#495057',\n                  color: 'white',\n                  textAlign: msg.sender === 'user' ? 'right' : 'left',\n                  marginLeft: msg.sender === 'bot' ? '0' : 'auto',\n                  marginRight: msg.sender === 'user' ? '0' : 'auto',\n                  wordWrap: 'break-word',\n                  fontSize: '0.95em'\n                }}>\n                  {msg.text}\n                  {msg.emotion && <em style={{fontSize: '0.8em', display: 'block', opacity: 0.7, marginTop: '4px'}}>({msg.emotion})</em>}\n                </div>\n              ))}\n              <div ref={messagesEndRef} />\n            </div>\n            <audio\n              ref={azureAudioRef}\n              onEnded={handleAzureAudioEnded}\n              onError={handleAzureAudioError}\n              style={{ display: 'none' }}\n            />\n            <form onSubmit={handleSendMessage} style={{ display: 'flex', marginTop: 'auto', paddingTop: '10px', borderTop: '1px solid #4f5461', flexShrink: 0 }}>\n              <input\n                type=\"text\"\n                value={state.inputValue}\n                onChange={handleInputChange}\n                placeholder=\"Type your message...\"\n                style={{\n                  flexGrow: 1,\n                  padding: '12px 18px',\n                  border: '1px solid #555',\n                  borderRadius: '25px',\n                  marginRight: '10px',\n                  outline: 'none',\n                  color: '#e0e0e0',\n                  background: '#3a3f47',\n                  fontSize: '1em'\n                }}\n              />\n              <button\n                type=\"submit\"\n                disabled={!state.inputValue.trim()}\n                style={{\n                  background: state.inputValue.trim() ? '#007bff' : '#555',\n                  color: 'white',\n                  border: 'none',\n                  borderRadius: '50%',\n                  width: '48px',\n                  height: '48px',\n                  display: 'flex',\n                  alignItems: 'center',\n                  justifyContent: 'center',\n                  cursor: state.inputValue.trim() ? 'pointer' : 'not-allowed',\n                  opacity: state.inputValue.trim() ? 1 : 0.6,\n                  transition: 'background-color 0.2s ease'\n                }}\n              >\n                &gt;\n              </button>\n            </form>\n          </div>\n        )}\n        \n        {!state.showChat && (\n          <button\n            onClick={toggleChat}\n            style={{\n              position: 'absolute',\n              bottom: '30px',\n              right: '30px',\n              background: '#007bff',\n              color: 'white',\n              border: 'none',\n              borderRadius: '50%',\n              width: '60px',\n              height: '60px',\n              display: 'flex',\n              alignItems: 'center',\n              justifyContent: 'center',\n              cursor: 'pointer',\n              boxShadow: '0 4px 12px rgba(0,0,0,0.3)',\n              zIndex: 10,\n              transition: 'transform 0.2s ease'\n            }}\n            onMouseOver={(e) => e.currentTarget.style.transform = 'scale(1.1)'}\n            onMouseOut={(e) => e.currentTarget.style.transform = 'scale(1)'}\n          >\n            Chat\n          </button>\n        )}\n      </div>\n\n      {/* Controls Bar at the bottom */}\n      <div className=\"controls-bar\" style={{\n        display: 'flex',\n        justifyContent: 'center',\n        alignItems: 'center',\n        padding: '10px',\n        background: '#20232a',\n        flexShrink: 0,\n        gap: '10px'\n      }}>\n        <button onClick={toggleMic} style={buttonStyle} title={state.isMicMuted ? \"Unmute Microphone\" : \"Mute Microphone\"}>\n          {state.isMicMuted ? <FaMicrophoneSlash /> : <FaMicrophone />} {state.isMicMuted ? \"Mic Off\" : \"Mic On\"}\n        </button>\n        <button onClick={toggleSound} style={buttonStyle} title={state.isSoundOn ? \"Mute Sound\" : \"Unmute Sound\"}>\n          {state.isSoundOn ? <FaVolumeUp /> : <FaVolumeMute />} {state.isSoundOn ? \"Sound On\" : \"Sound Off\"}\n        </button>\n        <button onClick={forceStopAllAudio} style={{...buttonStyle, background: '#dc3545'}} title=\"Force stop all audio and speech\">\n          <FaStopCircle /> Stop All Audio\n        </button>\n      </div>\n    </div>\n  );\n});\n\nexport const SimulationView: React.FC<SimulationViewProps> = (props) => {\n  const [humeAudioIndicatorMessage, setHumeAudioIndicatorMessage] = useState<string>('Hume Audio: Idle');\n  const internalCompRef = useRef<SimulationViewInternalHandle>(null);\n  const humeApiKey = React.useMemo(() => {\n    const key = process.env.REACT_APP_HUME_API_KEY;\n    if (!key) {\n      console.error(\"CRITICAL: REACT_APP_HUME_API_KEY is not set. Hume EVI will not work.\");\n    }\n    return key || '';\n  }, []);\n\n  const humeConfigId = process.env.REACT_APP_HUME_CONFIG_ID || '405fe2ff-0cf5-4ff9-abf9-fc09f4625ed8';\n\n  const {\n    messages: humeMessages,\n    sendUserInput: sendUserInputToVoice,\n    readyState: voiceReadyState,\n    isPlaying: isHumeVoicePlaying,\n    disconnect: disconnectVoice,\n    connect: connectVoice,\n    status,\n  } = useVoice();\n\n  console.log(\n    '[SimulationView useVoice] isHumeVoicePlaying:', isHumeVoicePlaying, \n    'status:', status\n  );\n\n  const handleVoiceOpen = useCallback(() => {\n    console.log('[Hume EVI] Connection opened via VoiceProvider.');\n  }, []);\n\n  const voiceAuth = useMemo(() => ({\n    type: 'apiKey' as const,\n    value: humeApiKey,\n  }), [humeApiKey]);\n\n  const handleVoiceMessage = useCallback((message: JSONMessage) => {\n    const messageType = message.type;\n    // Log the type of every message to give an overview without flooding with full objects\n    console.log(`[Hume EVI] Message received - Type: ${messageType}`);\n\n    switch (messageType) {\n      case 'assistant_message':\n        console.log('[Hume EVI] Assistant Message details:', message); // Log the full assistant_message object\n        if (message.models) {\n          const modelsAsAny = message.models as any; // Cast to any for easier access to dynamic model structure\n          if (modelsAsAny.face && modelsAsAny.face.predictions) {\n            // Log only if face predictions are present\n            console.log('[Hume EVI] Face model predictions:', modelsAsAny.face.predictions);\n          }\n          // Optional: Log if models exist but face/predictions are missing for debugging model structure\n          // else {\n          //   console.log('[Hume EVI] message.models.face or .predictions not found. message.models structure:', message.models);\n          // }\n        }\n        break;\n      case 'user_interruption':\n        console.log('[Hume EVI] User Interruption details:', message);\n        break;\n      case 'tool_call':\n        console.log('[Hume EVI] Tool Call details:', message);\n        break;\n      case 'tool_response':\n        console.log('[Hume EVI] Tool Response details:', message);\n        break;\n      case 'tool_error':\n        console.error('[Hume EVI] Tool Error details:', message);\n        break;\n      // Add cases for other specific message types if they become important for debugging\n      default:\n        // For any other message types, only their type is logged by the initial console.log.\n        // This significantly reduces noise from less critical or very frequent messages.\n        // If deeper debugging for other types is needed, uncomment the line below:\n        // console.log(`[Hume EVI] Other message type '${messageType}', content:`, message);\n        break;\n    }\n  }, []);\n\n  const handleVoiceClose = useCallback((e: any) => {\n    console.log('[Hume EVI] Connection closed via VoiceProvider. Code:', e.code, 'Reason:', e.reason, 'WasClean:', e.wasClean);\n    internalCompRef.current?.forceStopHumeSpeaking();\n  }, []);\n\n  const handleVoiceError = useCallback((error: any) => {\n    let logMessage = `[Hume EVI] VoiceProvider error - Type: ${error.type}, Code: ${error.code}, Message: ${error.message}`;\n    console.error(logMessage, error);\n  }, []);\n\n  if (!humeApiKey) {\n     console.warn(\"[SimulationView] Hume API Key is not available. VoiceProvider will likely fail to connect.\");\n     // Optionally, render a message to the user or a disabled state for the component\n     // return <div>Hume API Key is missing. Please configure REACT_APP_HUME_API_KEY in your environment.</div>;\n  }\n\n  console.log(`[SimulationView] Attempting to connect VoiceProvider with API Key: \"${humeApiKey}\" and Config ID: \"${humeConfigId}\"`);\n  console.log(`[SimulationView] voiceAuth object being passed:`, voiceAuth);\n\n  return (\n    <>\n      <div style={{ padding: '8px', backgroundColor: '#ff69b4', color: 'white', textAlign: 'center', fontWeight: 'bold', fontSize: '1.1em', zIndex: 9999, position: 'relative' }}>\n        HUME AUDIO EVENT STATUS: {humeAudioIndicatorMessage}\n      </div>\n      <VoiceProvider\n      onAudioStart={() => {\n        console.log('[Hume EVI VoiceProvider] onAudioStart triggered');\n        setHumeAudioIndicatorMessage('Hume Audio: STARTED');\n      }}\n      onAudioEnd={() => {\n        console.log('[Hume EVI VoiceProvider] onAudioEnd triggered');\n        setHumeAudioIndicatorMessage('Hume Audio: ENDED');\n      }}\n      auth={voiceAuth}\n      configId={humeConfigId}\n      debug={true}\n      onOpen={handleVoiceOpen}\n      onMessage={handleVoiceMessage}\n      onClose={handleVoiceClose}\n      onError={handleVoiceError}\n    >\n      <SimulationViewInternal\n          ref={internalCompRef}\n          simulationId={props.simulationId} // Pass down from parent props\n          avatarModelUrl={props.avatarModelUrl} // Pass down from parent props\n          humeMessages={humeMessages}\n          sendUserInputToVoice={sendUserInputToVoice}\n          voiceReadyState={voiceReadyState}\n          isHumeVoicePlaying={isHumeVoicePlaying}\n          disconnectVoice={disconnectVoice}\n          connectVoice={connectVoice}\n          // lastHumeVoiceMessage is removed as it's not provided by useVoice()\n          humeConfigId={humeConfigId} // Ensure humeConfigId is available in this scope\n        />\n    </VoiceProvider>\n    </>\n  );\n};\n","import { ARKitBlendshapeNamesList, type BlendshapeKey, type BlendShapeMap } from '../types/blendshapes';\n\nconst PROSODY_BLENDSHAPE_AMPLIFICATION_FACTOR = 7.0; // Increased for more dramatic expressions\n\n// Map each prosody emotion to blendshape contributions (values are multipliers for score)\nconst PROSODY_EMOTION_TO_BLENDSHAPE: Record<string, Partial<BlendShapeMap>> = {\n  joy: {\n    mouthSmileLeft: 1.2,\n    mouthSmileRight: 1.2,\n    cheekSquintLeft: 0.8,\n    cheekSquintRight: 0.8,\n    eyeWideLeft: 0.3,\n    eyeWideRight: 0.3,\n    browOuterUpLeft: 1.0, // Strong eyebrow raise for joy\n    browOuterUpRight: 1.0,\n    cheekPuff: 0.2, // Slight cheek puff for happiness\n  },\n  sadness: {\n    mouthFrownLeft: 1.2,\n    mouthFrownRight: 1.2,\n    browInnerUp: 1.0, // Strong inner brow raise (sad puppy eyes)\n    browDownLeft: 0.6,\n    browDownRight: 0.6,\n    eyeLookDownLeft: 0.4,\n    eyeLookDownRight: 0.4,\n    mouthLowerDownLeft: 0.5, // Droopy mouth corners\n    mouthLowerDownRight: 0.5,\n  },\n  anger: {\n    browDownLeft: 1.2, // Very strong brow furrow\n    browDownRight: 1.2,\n    browInnerUp: 0.3, // Slight inner brow tension\n    mouthPressLeft: 0.8,\n    mouthPressRight: 0.8,\n    eyeSquintLeft: 0.6,\n    eyeSquintRight: 0.6,\n    noseSneerLeft: 0.4, // Nostril flare for anger\n    noseSneerRight: 0.4,\n    mouthUpperUpLeft: 0.3, // Slight snarl\n    mouthUpperUpRight: 0.3,\n  },\n  surprise: {\n    jawOpen: 0.9,\n    eyeWideLeft: 1.2, // Very wide eyes\n    eyeWideRight: 1.2,\n    browOuterUpLeft: 1.2, // Dramatic eyebrow raise\n    browOuterUpRight: 1.2,\n    browInnerUp: 0.8, // Full brow raise\n    mouthFunnel: 0.3, // Slight \"O\" mouth shape\n  },\n  fear: {\n    mouthStretchLeft: 0.9,\n    mouthStretchRight: 0.9,\n    eyeWideLeft: 1.0,\n    eyeWideRight: 1.0,\n    browInnerUp: 1.0, // Strong worried brow\n    browOuterUpLeft: 0.7,\n    browOuterUpRight: 0.7,\n    jawOpen: 0.4, // Slight jaw drop in fear\n  },\n  disgust: {\n    noseSneerLeft: 0.8,\n    noseSneerRight: 0.8,\n    mouthUpperUpLeft: 0.7, // Upper lip curl\n    mouthUpperUpRight: 0.7,\n    eyeSquintLeft: 0.5,\n    eyeSquintRight: 0.5,\n    browDownLeft: 0.4,\n    browDownRight: 0.4,\n  },\n  contempt: {\n    mouthSmileLeft: 0.2, // Asymmetric smirk\n    mouthSmileRight: 0.7,\n    eyeSquintLeft: 0.3,\n    eyeSquintRight: 0.6,\n    browDownLeft: 0.2,\n    browDownRight: 0.5,\n  },\n  excitement: {\n    mouthSmileLeft: 1.0,\n    mouthSmileRight: 1.0,\n    eyeWideLeft: 0.8,\n    eyeWideRight: 0.8,\n    browOuterUpLeft: 0.9,\n    browOuterUpRight: 0.9,\n    cheekSquintLeft: 0.6,\n    cheekSquintRight: 0.6,\n    jawOpen: 0.3, // Slight excitement jaw drop\n  },\n  contentment: {\n    mouthSmileLeft: 0.4,\n    mouthSmileRight: 0.4,\n    eyeSquintLeft: 0.2, // Gentle squint for contentment\n    eyeSquintRight: 0.2,\n    browOuterUpLeft: 0.2,\n    browOuterUpRight: 0.2,\n  },\n  confusion: {\n    browDownLeft: 0.6,\n    browOuterUpRight: 0.8, // Asymmetric brow for confusion\n    browInnerUp: 0.4,\n    mouthPucker: 0.4,\n    eyeSquintLeft: 0.3,\n    jawOpen: 0.2,\n  },\n  calm: {\n    mouthSmileLeft: 0.15,\n    mouthSmileRight: 0.15,\n    eyeSquintLeft: 0.1, // Very subtle relaxed expression\n    eyeSquintRight: 0.1,\n  },\n  // Add more emotions that might come from Hume\n  amusement: {\n    mouthSmileLeft: 0.8,\n    mouthSmileRight: 0.8,\n    cheekSquintLeft: 0.7,\n    cheekSquintRight: 0.7,\n    browOuterUpLeft: 0.5,\n    browOuterUpRight: 0.5,\n  },\n  concentration: {\n    browDownLeft: 0.5,\n    browDownRight: 0.5,\n    browInnerUp: 0.3,\n    eyeSquintLeft: 0.4,\n    eyeSquintRight: 0.4,\n  },\n};\n\nexport const prosodyToBlendshapes = (prosodyScores: Record<string, number>): BlendShapeMap => {\n  console.log('[prosodyToBlendshapes] Received prosodyScore keys:', Object.keys(prosodyScores).join(', '));\n\n  // Start with all zeros  // Initialize blendshapes to 0\n  const blendshapes: BlendShapeMap = ARKitBlendshapeNamesList.reduce((acc: BlendShapeMap, shapeName: BlendshapeKey) => {\n    acc[shapeName] = 0;\n    return acc;\n  }, {} as BlendShapeMap);\n\n  console.log('[prosodyToBlendshapes] Processing emotions...');\n  for (const [emotion, score] of Object.entries(prosodyScores)) {\n    const weights = PROSODY_EMOTION_TO_BLENDSHAPE[emotion.toLowerCase()];\n    if (!weights) {\n      console.log(`[prosodyToBlendshapes] No weights found for emotion: ${emotion}. Skipping.`);\n      continue;\n    }\n    console.log(`[prosodyToBlendshapes] Processing emotion: ${emotion}, Score: ${score.toFixed(4)}`);\n    for (const [blend, weight] of Object.entries(weights)) {\n      const shapeKey = blend as BlendshapeKey;\n      const currentVal = blendshapes[shapeKey] || 0;\n      const addition = score * (weight as number) * PROSODY_BLENDSHAPE_AMPLIFICATION_FACTOR;\n      blendshapes[shapeKey] = Math.min(1, Math.max(0, currentVal + addition));\n      // Optional detailed logging for each blendshape change:\n      // console.log(`[prosodyToBlendshapes]   - Blendshape: ${shapeKey}, Weight: ${weight}, Addition: ${addition.toFixed(4)}, New Value: ${blendshapes[shapeKey].toFixed(4)}`);\n    }\n  }\n  console.log(`[prosodyToBlendshapes] Final combined blendshapes count: ${Object.keys(blendshapes).length}, sum of values: ${Object.values(blendshapes).reduce((s, v) => s + v, 0).toFixed(4)}`);\n  return blendshapes;\n}\n"],"names":["TestModeToggle","_ref","onMockMessage","onMockProsody","onMockVisemes","isTestMode","setIsTestMode","useState","isExpanded","setIsExpanded","mockEVI","setMockEVI","handleTestModeToggle","useCallback","console","log","stop","evi","MockHumeEVI","message","prosody","timeline","visemes","reduce","acc","item","type","value","jawOpen","mouthFunnel","mouthSmileLeft","mouthSmileRight","Math","max","start","handleTestEmotion","emotionSetKey","emotionSet","mockEmotionSets","concat","_jsxs","style","position","top","left","background","color","padding","borderRadius","zIndex","fontSize","width","maxHeight","overflowY","transition","border","children","display","alignItems","justifyContent","marginBottom","_jsx","margin","onClick","cursor","fontWeight","gridTemplateColumns","gap","Object","keys","map","emotionKey","textTransform","onMouseEnter","e","currentTarget","onMouseLeave","getEmotionEmoji","opacity","lineHeight","borderTop","paddingTop","marginTop","emotion","happy","sad","angry","surprised","scared","disgusted","confused","content","HUME_SAPI_TO_IPA","IPA_TO_AZURE_VISEME_ID_MAP","ipaSymbols","id","convertHumeTimelineToAzureVisemes","humeTimeline","audioDurationSeconds","visemeEvents","audioDurationMs","round","length","forEach","humeEvent","index","humePhonemeKey","phoneme","toUpperCase","visemeId","ipaSymbol","warn","entry","includes","getAzureVisemeIdFromIpa","undefined","audioOffsetTicks","time","push","audioOffset","isLastViseme","sort","a","b","speechConfig","synthesizeSpeechWithVisemes","async","text","voiceName","arguments","Date","now","Promise","resolve","reject","speechKey","process","REACT_APP_AZURE_SPEECH_KEY","speechRegion","REACT_APP_AZURE_SPEECH_REGION","error","Error","SpeechSDK","fromSubscription","speechSynthesisOutputFormat","Audio16Khz32KBitRateMonoMp3","ssml","synthesizer","collectedVisemeData","standardVisemes","blendShapeFrames","promiseHandled","cleanup","close","closeError","visemeReceived","s","animation","trim","substring","animationData","JSON","parse","BlendShapes","FrameIndex","baseFrameIndex","shapeFrame","frameIdxInChunk","frameIndex","shapes","FrameRate","visemeID","speakSsmlAsync","result","reason","SynthesizingAudioCompleted","audioDuration","audioData","visemeData","Canceled","cancellation","fromResult","reasonText","errorDetails","ErrorCode","initialEmptyBlendshapes","ARKitBlendshapeNamesList","shapeName","AZURE_VISEME_ID_TO_STATIC_BLENDSHAPES","mouthClose","mouthPucker","mouthShrugUpper","tongueUp","mouthLowerDownLeft","mouthLowerDownRight","mouthPressLeft","mouthPressRight","mouthShrugLower","defaultBlendShapes","key","idleBlendShapes","ALL_TALK_ANIMATION_GLBS","ALL_IDLE_ANIMATION_GLBS","buttonStyle","SimulationViewInternal","React","props","ref","humeMessages","sendUserInputToVoice","voiceReadyState","isHumeVoicePlaying","disconnectVoice","connectVoice","humeConfigId","VoiceReadyState","String","currentUser","useAuth","user","loading","userLoading","userError","useUser","simulationId","useNavigate","useParams","state","setState","avatarModelUrl","avatarUrl","initial","isMicOn","isCameraOn","isSoundOn","isChatOpen","messages","inputValue","isSpeaking","isAzureAudioActive","isHumeAudioActive","currentEmotion","statusMessage","humeVoiceName","azureVoiceName","currentVisemeShapes","manualBlendshapes","simulationData","isSending","isCameraEnabled","isMicMuted","showChat","prosodyDrivenBlendshapes","stateRef","useRef","handleEmotionData","prev","_objectSpread","name","toLowerCase","humeApiKey","sendVideoFrame","sendEmotionVideoFrame","connectionState","humeEmotionStreamConnectionState","lastError","humeEmotionStreamLastError","useHumeEmotionStream","isEmotionDetectionActive","isVideoOn","useEffect","azureAudioRef","humeAudioRef","visemeFramesRef","animationFrameIdRef","speakingDebounceTimerRef","lastProcessedUserMessageReceivedAtRef","lastProcessedAssistantMessageIdRef","isAzurePlayingRef","isHumeVoicePlayingRef","current","handleAvatarErrorCb","handleAvatarLoadCb","messagesEndRef","avatarGroupRef","customSessionIdRef","uuidv4","stopVisemeAnimation","cancelAnimationFrame","prepareVisemeFrames","frames","frame","humeVisemeEventsToAnimationKeyframes","keyframes","event","lastKeyframeTimeSeconds","animateVisemes","startTime","currentTime","currentFrames","activeFrame","i","requestAnimationFrame","startVisemeAnimation","handleHumeSpeakingStarted","audioUrl","pause","src","Audio","addEventListener","load","play","duration","isNaN","converted","handleHumeSpeakingStopped","handleAzureAudioEnded","startsWith","URL","revokeObjectURL","srcObject","handleAzureAudioError","audioElement","target","generateAndAnimateVisemesFromText","_serviceResult$viseme","paused","serviceResult","untypedSynthesizeSpeechWithVisemes","stringify","min","_serviceResult$viseme2","slice","lastFrameIdx","_serviceResult$viseme3","preparedFrames","audioBlob","Blob","createObjectURL","catch","processAndPlayAzureSpeech","_serviceResult$audioD","_serviceResult$viseme4","_serviceResult$viseme5","_serviceResult$viseme6","_serviceResult$viseme7","resolvedVoiceName","audioDataLength","byteLength","blendShapeFramesLength","standardVisemesLength","_serviceResult$viseme8","_serviceResult$viseme9","then","currentMessage","receivedAt","models","role","newUserChatMessage","sender","timestamp","_currentMessage$model","_currentMessage$model2","_currentMessage$model3","_currentMessage$model4","scores","newAssistantChatMessage","newEmotions","score","emotionsSource","Array","isArray","potentialEmotions","filter","prosodyScoresForBlendshapes","newProsodyBlendshapes","prosodyToBlendshapes","currentDetectedEmotions","clearTimeout","setTimeout","handleInputChange","handleSendMessage","preventDefault","newUserMessage","inputText","toggleMic","emotionData","errorMessage","IDLE","CONNECTING","OPEN","CLOSED","getVoiceReadyStateName","newMicMutedState","humeAccessToken","toggleChat","toggleSound","newIsSoundOn","muted","forceStopAllAudio","useImperativeHandle","forceStopHumeSpeaking","prevState","derivedIsSpeaking","derivedStatusMessage","className","height","flexDirection","textAlign","flexShrink","prosodyRecord","prosodyBlendshapes","mockVisemes","emotions","flexGrow","overflow","EmotionDrivenAvatar","directBlendshapes","idleShapes","detectedEmotions","cameraEnabled","talkAnimationPaths","idleAnimationPaths","onError","onLoad","minWidth","borderLeft","boxSizing","right","bottom","transform","paddingRight","msg","maxWidth","alignSelf","marginLeft","marginRight","wordWrap","onEnded","onSubmit","onChange","placeholder","outline","disabled","boxShadow","onMouseOver","onMouseOut","title","FaMicrophoneSlash","FaMicrophone","FaVolumeUp","FaVolumeMute","FaStopCircle","SimulationView","humeAudioIndicatorMessage","setHumeAudioIndicatorMessage","internalCompRef","sendUserInput","readyState","isPlaying","disconnect","connect","status","useVoice","handleVoiceOpen","voiceAuth","useMemo","handleVoiceMessage","messageType","modelsAsAny","face","predictions","handleVoiceClose","_internalCompRef$curr","code","wasClean","handleVoiceError","logMessage","_Fragment","backgroundColor","VoiceProvider","onAudioStart","onAudioEnd","auth","configId","debug","onOpen","onMessage","onClose","PROSODY_EMOTION_TO_BLENDSHAPE","joy","cheekSquintLeft","cheekSquintRight","eyeWideLeft","eyeWideRight","browOuterUpLeft","browOuterUpRight","cheekPuff","sadness","mouthFrownLeft","mouthFrownRight","browInnerUp","browDownLeft","browDownRight","eyeLookDownLeft","eyeLookDownRight","anger","eyeSquintLeft","eyeSquintRight","noseSneerLeft","noseSneerRight","mouthUpperUpLeft","mouthUpperUpRight","surprise","fear","mouthStretchLeft","mouthStretchRight","disgust","contempt","excitement","contentment","confusion","calm","amusement","concentration","prosodyScores","join","blendshapes","entries","weights","toFixed","blend","weight","shapeKey","currentVal","addition","values","v"],"sourceRoot":""}