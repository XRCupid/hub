{"version":3,"file":"static/js/3401.8edbf2aa.chunk.js","mappings":"+IAAO,MAAMA,EAKXC,WAAAA,GAAe,KAJPC,kBAAY,OACZC,cAAQ,OACRC,WAAgD,KAGtDC,KAAKH,aAAe,IAAII,aACxBD,KAAKF,SAAWE,KAAKH,aAAaK,iBAClCF,KAAKF,SAASK,QAAU,GAC1B,CAEA,oBAAMC,CAAeC,GACnB,IACE,MAAMC,QAAeC,UAAUC,aAAaC,aAAa,CAAEC,OAAO,IAClEV,KAAKD,WAAaC,KAAKH,aAAac,wBAAwBL,GAC5DN,KAAKD,WAAWa,QAAQZ,KAAKF,UAE7B,MAAMe,EAAeb,KAAKF,SAASgB,kBAC7BC,EAAY,IAAIC,WAAWH,GAE3BI,EAAeA,KACnBjB,KAAKF,SAASoB,qBAAqBH,GAGnC,MACMI,EADUJ,EAAUK,QAAO,CAACC,EAAKC,IAAUD,EAAMC,GAAO,GAAKT,EAC7B,IAElCM,EAAsB,IACxBd,EAAiBc,GAGnBI,sBAAsBN,IAGxBA,GACF,CAAE,MAAOO,GACPC,QAAQD,MAAM,0BAA2BA,EAC3C,CACF,CAGAE,kBAAAA,CAAmBC,GACjB,MAAM,UAAEC,EAAS,MAAEC,GAAUF,EAE7B,OAAIC,EAAY,IAAOC,EAAQ,GAAY,UACvCD,EAAY,IAAOC,EAAQ,GAAY,MACvCD,EAAY,IAAOC,EAAQ,GAAY,QACvCD,EAAY,IAAOC,EAAQ,GAAY,UAEpC,SACT,E,eC1CK,MAAMC,EAAkDC,IAExD,IAFyD,eAC9DC,EAAiB,WAClBD,EACC,MAAOE,EAAgBC,IAAqBC,EAAAA,EAAAA,UAASH,IAC9CI,EAAYC,IAAiBF,EAAAA,EAAAA,WAAS,GACvCG,GAAoBC,EAAAA,EAAAA,QAA8B,MA2BxD,OAzBAC,EAAAA,EAAAA,YAAU,KAER,MAAMC,EAAiB,IAAI9C,EAkB3B,OAjBA2C,EAAkBI,QAAUD,EAE5BA,EAAerC,gBAAgBwB,IAC7BS,GAAc,GAGd,MAAMM,EAAsBF,EAAef,mBAAmB,CAC5DE,YACAC,MAAO,KAGTK,EAAkBS,GAGlBC,YAAW,IAAMP,GAAc,IAAQ,QAGlC,SAGN,KAGDQ,EAAAA,EAAAA,KAAA,OACEC,UAAS,oBAAAC,OAAsBd,GAC/B,kCAAAc,OAAiCd,EAAc,YAAWe,UAE1DC,EAAAA,EAAAA,MAAA,OACEC,MAAM,6BACNC,MAAM,MACNC,OAAO,MACPC,QAAQ,cAAaL,SAAA,EAGrBH,EAAAA,EAAAA,KAAA,WAASS,GAAG,MAAMC,GAAG,MAAMC,GAAG,MAAMC,GAAG,MAAMC,KAAK,aAGlDT,EAAAA,EAAAA,MAAA,KAAGH,UAAS,eAAAC,OAAiBX,EAAa,QAAU,IAAKY,SAAA,EACvDH,EAAAA,EAAAA,KAAA,UAAQS,GAAG,MAAMC,GAAG,MAAMI,EAAE,KAAKD,KAAK,aACtCb,EAAAA,EAAAA,KAAA,UAAQS,GAAG,MAAMC,GAAG,MAAMI,EAAE,KAAKD,KAAK,gBAIxCb,EAAAA,EAAAA,KAAA,QACEe,EAAE,4BACFC,OAAO,UACPC,YAAY,IACZJ,KAAK,OACLZ,UAAS,gBAAAC,OAAkBX,EAAa,WAAa,W","sources":["utils/speech-detector.ts","components/EmotionalAvatar.tsx"],"sourcesContent":["export class SpeechDetector {\n  private audioContext: AudioContext;\n  private analyser: AnalyserNode;\n  private microphone: MediaStreamAudioSourceNode | null = null;\n\n  constructor() {\n    this.audioContext = new AudioContext();\n    this.analyser = this.audioContext.createAnalyser();\n    this.analyser.fftSize = 256;\n  }\n\n  async startListening(onSpeechDetected: (intensity: number) => void) {\n    try {\n      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n      this.microphone = this.audioContext.createMediaStreamSource(stream);\n      this.microphone.connect(this.analyser);\n\n      const bufferLength = this.analyser.frequencyBinCount;\n      const dataArray = new Uint8Array(bufferLength);\n\n      const detectSpeech = () => {\n        this.analyser.getByteFrequencyData(dataArray);\n        \n        // Calculate speech intensity\n        const average = dataArray.reduce((sum, value) => sum + value, 0) / bufferLength;\n        const normalizedIntensity = average / 255;\n\n        if (normalizedIntensity > 0.1) {  // Threshold for speech detection\n          onSpeechDetected(normalizedIntensity);\n        }\n\n        requestAnimationFrame(detectSpeech);\n      };\n\n      detectSpeech();\n    } catch (error) {\n      console.error('Speech detection error:', error);\n    }\n  }\n\n  // Emotion approximation based on speech characteristics\n  approximateEmotion(speechData: { intensity: number, pitch: number }): string {\n    const { intensity, pitch } = speechData;\n\n    if (intensity > 0.8 && pitch > 0.7) return 'excited';\n    if (intensity < 0.3 && pitch < 0.4) return 'sad';\n    if (intensity > 0.5 && pitch > 0.5) return 'happy';\n    if (intensity < 0.4 && pitch > 0.6) return 'nervous';\n\n    return 'neutral';\n  }\n}\n","import React, { useState, useEffect, useRef } from 'react';\nimport { SpeechDetector } from '../utils/speech-detector';\nimport '../styles/emotional-avatar.css';\n\ninterface EmotionalAvatarProps {\n  initialEmotion?: 'neutral' | 'happy' | 'sad' | 'excited' | 'nervous';\n}\n\nexport const EmotionalAvatar: React.FC<EmotionalAvatarProps> = ({ \n  initialEmotion = 'neutral' \n}) => {\n  const [currentEmotion, setCurrentEmotion] = useState(initialEmotion);\n  const [isSpeaking, setIsSpeaking] = useState(false);\n  const speechDetectorRef = useRef<SpeechDetector | null>(null);\n\n  useEffect(() => {\n    // Initialize speech detection\n    const speechDetector = new SpeechDetector();\n    speechDetectorRef.current = speechDetector;\n\n    speechDetector.startListening((intensity) => {\n      setIsSpeaking(true);\n      \n      // Approximate emotion based on speech\n      const approximatedEmotion = speechDetector.approximateEmotion({\n        intensity,\n        pitch: 0.6  // Placeholder pitch value\n      });\n\n      setCurrentEmotion(approximatedEmotion as 'neutral' | 'happy' | 'sad' | 'excited' | 'nervous');\n\n      // Reset speaking state after a delay\n      setTimeout(() => setIsSpeaking(false), 500);\n    });\n\n    return () => {\n      // Cleanup logic\n    };\n  }, []);\n\n  return (\n    <div \n      className={`emotional-avatar ${currentEmotion}`}\n      aria-label={`Avatar expressing ${currentEmotion} emotion`}\n    >\n      <svg \n        xmlns=\"http://www.w3.org/2000/svg\" \n        width=\"300\" \n        height=\"400\" \n        viewBox=\"0 0 300 400\"\n      >\n        {/* Base Avatar SVG from previous implementation */}\n        <ellipse cx=\"150\" cy=\"200\" rx=\"120\" ry=\"150\" fill=\"#F5D4A0\"/>\n        \n        {/* Dynamic Eye Animation */}\n        <g className={`avatar-eyes ${isSpeaking ? 'blink' : ''}`}>\n          <circle cx=\"110\" cy=\"180\" r=\"15\" fill=\"#4A4A4A\"/>\n          <circle cx=\"190\" cy=\"180\" r=\"15\" fill=\"#4A4A4A\"/>\n        </g>\n\n        {/* Dynamic Mouth Animation */}\n        <path \n          d=\"M110,250 Q150,280 190,250\" \n          stroke=\"#FF6B6B\" \n          strokeWidth=\"5\" \n          fill=\"none\"\n          className={`avatar-mouth ${isSpeaking ? 'speaking' : ''}`}\n        />\n      </svg>\n    </div>\n  );\n};\n"],"names":["SpeechDetector","constructor","audioContext","analyser","microphone","this","AudioContext","createAnalyser","fftSize","startListening","onSpeechDetected","stream","navigator","mediaDevices","getUserMedia","audio","createMediaStreamSource","connect","bufferLength","frequencyBinCount","dataArray","Uint8Array","detectSpeech","getByteFrequencyData","normalizedIntensity","reduce","sum","value","requestAnimationFrame","error","console","approximateEmotion","speechData","intensity","pitch","EmotionalAvatar","_ref","initialEmotion","currentEmotion","setCurrentEmotion","useState","isSpeaking","setIsSpeaking","speechDetectorRef","useRef","useEffect","speechDetector","current","approximatedEmotion","setTimeout","_jsx","className","concat","children","_jsxs","xmlns","width","height","viewBox","cx","cy","rx","ry","fill","r","d","stroke","strokeWidth"],"sourceRoot":""}