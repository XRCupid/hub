"use strict";(self.webpackChunkhub=self.webpackChunkhub||[]).push([[7450],{47450:(e,n,o)=>{o.r(n),o.d(n,{default:()=>f});var t=o(89379),r=o(9950),a=o(52111),i=o(69104),s=o(68057),c=o(13437),l=o(98905),u=o(15625),m=o(78711),d=o(89592),g=o(44414);const E=["/animations/M_Talking_Variations_001.glb","/animations/M_Talking_Variations_002.glb","/animations/M_Talking_Variations_003.glb","/animations/M_Talking_Variations_004.glb","/animations/M_Talking_Variations_005.glb","/animations/M_Talking_Variations_006.glb","/animations/M_Talking_Variations_007.glb","/animations/M_Talking_Variations_008.glb","/animations/M_Talking_Variations_009.glb","/animations/M_Talking_Variations_010.glb"],S=["/animations/M_Standing_Idle_001.glb","/animations/M_Standing_Idle_002.glb","/animations/M_Standing_Idle_Variations_001.glb","/animations/M_Standing_Idle_Variations_002.glb","/animations/M_Standing_Idle_Variations_003.glb","/animations/M_Standing_Idle_Variations_004.glb","/animations/M_Standing_Idle_Variations_005.glb","/animations/M_Standing_Idle_Variations_006.glb","/animations/M_Standing_Idle_Variations_007.glb","/animations/M_Standing_Idle_Variations_008.glb","/animations/M_Standing_Idle_Variations_009.glb","/animations/M_Standing_Idle_Variations_010.glb"];function v(e){(0,r.useEffect)((()=>{e&&e.length>0&&i.p.preload(e)}),[e]);const n=(0,i.p)(e||[]);return(0,r.useMemo)((()=>{if(!e||0===e.length||!n||0===n.length)return{clips:[],names:[]};const o=[],t=[];return(Array.isArray(n)?n:[n]).forEach(((n,r)=>{n&&n.animations?n.animations.forEach((e=>{o.push(e),t.push(e.name)})):console.warn("[useExternalAnimations] GLTF result at index ".concat(r," (path: ").concat(e[r],") is invalid or has no animations."))})),{clips:o,names:[...new Set(t)]}}),[n,e])}const p=(e,n)=>{var o,i,p;console.log("[EDA] Props received - detectedEmotions:",JSON.stringify(e.detectedEmotions)),console.log("[EDA] Props received - isSpeaking:",e.isSpeaking),console.log("[EDA] Props received - visemeData:",JSON.stringify(e.visemeData)),console.log("[EDA] Props received - directBlendshapes:",JSON.stringify(e.directBlendshapes)),e.visemeData&&Object.keys(e.visemeData).length>0&&(console.log("[EDA] \u26a0\ufe0f VISEME DATA DETECTED - This may override face tracking!"),void 0!==e.visemeData.jawOpen&&console.log("[EDA] \u26a0\ufe0f jawOpen in visemeData: ".concat(e.visemeData.jawOpen," (this will override ML5)")));const{humeApiKey:b,avatarUrl:f,activeBodyAnimation:_,onError:h,onLoad:A,isSpeaking:y,detectedEmotions:D,directBlendshapes:T,emotionBlendshapes:k,cameraEnabled:M,talkAnimation:O,idleAnimation:I,talkAnimationPaths:R=E,idleAnimationPaths:C=S,visemeData:w}=e,N=(0,r.useRef)(null);(0,r.useImperativeHandle)(n,(()=>N.current));const[H,V]=(0,r.useState)(null),[W,P]=(0,r.useState)({eyeBlinkLeft:0,eyeBlinkRight:0}),j=(0,r.useCallback)((n=>{const o=(0,t.A)((0,t.A)({},n),{},{timestamp:Date.now()});V(o),e.onEmotions&&e.onEmotions([o])}),[e.onEmotions]),{connectionState:B,lastError:x,sendVideoFrame:J}=(0,l.S)(b,j,{isEmotionDetectionActive:M,isVideoOn:M});(0,r.useEffect)((()=>{x&&h&&h(new Error(x))}),[x,h]),(0,r.useEffect)((()=>{let e,n;const o=()=>{P({eyeBlinkLeft:1,eyeBlinkRight:1}),n=setTimeout((()=>{P({eyeBlinkLeft:0,eyeBlinkRight:0})}),150);const t=4e3*Math.random()+3e3;e=setTimeout(o,t)},t=4e3*Math.random()+1e3;return e=setTimeout(o,t),()=>{clearTimeout(e),clearTimeout(n)}}),[]);const L=(0,r.useMemo)((()=>e.currentEmotion?[{name:e.currentEmotion,score:1}]:D&&D.length>0?D:H?[H]:[]),[e.currentEmotion,D,H]);console.log("[EDA] emotionsToProcess:",JSON.stringify(L,null,2));const F=(0,r.useMemo)((()=>(0,m.F)(L||[])),[L]),U=(0,r.useMemo)((()=>{const e=(0,m.R)(F?[F]:[]);return console.log("[EDA] Blendshapes from mapEmotionsToBlendshapes:",JSON.stringify(e,null,2)),e}),[F]),G=(0,r.useMemo)((()=>new d.I({emotionMouthReduction:.3,emotionFaceBlending:.8,smoothingFactor:.1})),[]),K=(0,r.useMemo)((()=>{const e=y||w&&Object.entries(w).some((e=>{let[n,o]=e;return n.startsWith("mouth")&&(o||0)>.1})),n={visemes:e&&w||{},emotions:U||{},manual:T||{},base:W||{}};!e&&w&&Object.keys(w).length>0&&console.log("[EDA] \ud83d\udeab Ignoring visemeData (not speaking and no significant mouth movement):",JSON.stringify(w));const o=G.compose(n);if(y){!Object.entries(o).some((e=>{let[n,o]=e;return n.startsWith("mouth")&&(o||0)>.1}))&&(o.jawOpen||0)<.1&&(o.jawOpen=.4,console.log("[EDA] Applied fallback jaw animation for speech"))}return console.log("[EDA] Composed blendshapes:",JSON.stringify(o,null,2)),o}),[G,w,U,T,W,y]),{clips:z,names:Z}=v(R),{clips:q,names:Q}=v(C),X=(0,r.useMemo)((()=>{const e=new Map;return[...z,...q].forEach((n=>{n&&n.name&&!e.has(n.name)&&e.set(n.name,n)})),Array.from(e.values())}),[z,q]),[Y,$]=(0,r.useState)(0),[ee,ne]=(0,r.useState)(0),oe=(0,r.useMemo)((()=>_||(y?Z.length>0?Z[Y%Z.length]:O||"Talk_0":Q.length>0?Q[ee%Q.length]:I||"Idle_0")),[y,_,Z,Q,Y,ee,O,I]),te=(0,r.useCallback)((e=>{A&&A()}),[A]);return f?(console.log("[EmotionDrivenAvatar] Checking IMPORTED THREE instance before Canvas render:"),console.log("[EmotionDrivenAvatar] IMPORTED THREE object:",c),console.log("[EmotionDrivenAvatar] IMPORTED THREE.REVISION:",null===c||void 0===c?void 0:c.REVISION),console.log("[EmotionDrivenAvatar] Is IMPORTED THREE.Cache available (added by threejs core)?",!(null===c||void 0===c||!c.Cache)),console.log("[EmotionDrivenAvatar] Is IMPORTED THREE.CanvasTexture available (used by R3F Canvas)?",!(null===c||void 0===c||!c.CanvasTexture)),console.log("[EmotionDrivenAvatar] Checking WINDOW.THREE instance before Canvas render:"),console.log("[EmotionDrivenAvatar] WINDOW.THREE object:",window.THREE),console.log("[EmotionDrivenAvatar] WINDOW.THREE.REVISION:",null===(o=window.THREE)||void 0===o?void 0:o.REVISION),console.log("[EmotionDrivenAvatar] Is WINDOW.THREE.Cache available?",!(null===(i=window.THREE)||void 0===i||!i.Cache)),console.log("[EmotionDrivenAvatar] Is WINDOW.THREE.CanvasTexture available?",!(null===(p=window.THREE)||void 0===p||!p.CanvasTexture)),console.log("[EDA] Values passed to SimAvatar3D - finalEmotionShapes (COMPOSED):",JSON.stringify(K)),console.log("[EDA] Values passed to SimAvatar3D - visemeData (RAW, for reference):",JSON.stringify(w)),console.log("[EDA] Values passed to SimAvatar3D - currentAnimationName:",oe),(0,g.jsxs)(a.Hl,{camera:{position:[0,0,2],fov:50},style:{touchAction:"none"},shadows:!0,gl:{antialias:!0,alpha:!0,toneMapping:c.ACESFilmicToneMapping,toneMappingExposure:1,outputColorSpace:c.SRGBColorSpace},children:[(0,g.jsx)("ambientLight",{intensity:.8}),(0,g.jsx)("pointLight",{position:[5,5,5],intensity:.8}),(0,g.jsx)(s.N,{enableRotate:!1,enablePan:!1,enableZoom:!1,autoRotate:!1}),(0,g.jsx)(u.default,{avatarUrl:f,animationClips:X,currentAnimationName:oe,emotionShapes:K,visemeShapes:{},position:[0,-.8,0],scale:1,onModelLoaded:te})]})):(console.warn("EmotionDrivenAvatar: avatarUrl is not provided. Rendering null."),h&&h(new Error("EmotionDrivenAvatar: avatarUrl is not provided.")),null)};p.displayName="EmotionDrivenAvatarComponentBody";const b=r.forwardRef(p),f=r.memo(b)},98905:(e,n,o)=>{o.d(n,{S:()=>r});var t=o(9950);const r=(e,n,o)=>{const r=(0,t.useRef)(null),a=(0,t.useRef)(!0),i=(0,t.useRef)(0),s=(0,t.useRef)(),{isEmotionDetectionActive:c=!1,isVideoOn:l=!1}=o||{},[u,m]=(0,t.useState)("disconnected"),[d,g]=(0,t.useState)(null);(0,t.useEffect)((()=>(a.current=!0,()=>{a.current=!1,s.current&&(clearTimeout(s.current),s.current=void 0)})),[]),(0,t.useEffect)((()=>{if(!e||!c||!l)return r.current&&(console.log("[Hume Stream] Conditions not met or changed. Closing WebSocket."),r.current.onopen=null,r.current.onmessage=null,r.current.onerror=null,r.current.onclose=null,r.current.close(),r.current=null,i.current=0,s.current&&(clearTimeout(s.current),s.current=void 0)),void m("disconnected");if(r.current&&(r.current.readyState===WebSocket.OPEN||r.current.readyState===WebSocket.CONNECTING))return void console.log("[Hume Stream] WebSocket already open or connecting. No action needed.");console.log("[Hume Stream] Conditions met. Attempting to establish WebSocket connection."),m("connecting"),g(null);const o="wss://api.hume.ai/v0/stream/models?apiKey=".concat(encodeURIComponent(e)),t=new WebSocket(o);t.binaryType="arraybuffer",r.current=t,console.log("[Hume Stream] WebSocket instance created. URL:",o),t.onopen=()=>{if(!a.current||r.current!==t)return;console.log("[Hume Stream] WebSocket connected."),i.current=0,m("connected"),g(null);const e={models:{face:{},prosody:{}},data:"dGVzdA=="};try{console.log("[Hume Stream] Sending initial message:",JSON.stringify(e,null,2)),t.send(JSON.stringify(e))}catch(n){console.error("[Hume Stream] Error sending initial message:",n),g(n instanceof Error?n.message:String(n))}},t.onmessage=e=>{if(a.current&&r.current===t)try{var o,i,s;const t=JSON.parse(e.data);if(t.error)return console.error("[Hume Stream] Hume API error:",t.error),m("error"),void g("Hume API Error: ".concat(t.error.message||t.error));if(null!==(o=t.predictions)&&void 0!==o&&null!==(i=o[0])&&void 0!==i&&null!==(s=i.emotions)&&void 0!==s&&s.length){const e=t.predictions[0].emotions.reduce(((e,n)=>e.score>n.score?e:n),{name:"neutral",score:0});n({name:e.name.toLowerCase(),score:e.score})}}catch(c){console.error("[Hume Stream] Error processing message:",c),g(c instanceof Error?c.message:"Unknown error")}};return t.onerror=e=>{a.current&&r.current===t&&(console.error("[Hume Stream] WebSocket error:",e),g("WebSocket error occurred."))},t.onclose=e=>{a.current&&r.current===t&&(console.log("[Hume Stream] WebSocket closed. Code: ".concat(e.code,", Reason: '").concat(e.reason,"', Clean: ").concat(e.wasClean)),r.current=null,1e3===e.code||1005===e.code?(console.log("[Hume Stream] WebSocket closed normally or intentionally."),m("disconnected"),i.current=0,s.current&&(clearTimeout(s.current),s.current=void 0)):a.current&&c&&l?(console.log("[Hume Stream] WebSocket closed abnormally. Attempting reconnect."),g("WebSocket closed abnormally: ".concat(e.code," ").concat(e.reason)),(()=>{if(!a.current||!c||!l||i.current>=5)return void(i.current>=5&&(console.error("[Hume Stream] Max reconnection attempts reached."),m("error"),g("Max reconnection attempts reached.")));i.current++;const e=Math.min(1e3*Math.pow(2,i.current-1),3e4);console.log("[Hume Stream] Attempting to reconnect (".concat(i.current,"/5) in ").concat(e,"ms...")),m("reconnecting"),s.current&&clearTimeout(s.current),s.current=window.setTimeout((()=>{a.current&&c&&l&&(console.log("[Hume Stream] Reconnect timeout: Triggering new connection attempt."),r.current&&r.current.readyState!==WebSocket.OPEN&&(r.current=null,m("connecting")))}),e)})()):(m("disconnected"),i.current=0))},()=>{console.log("[Hume Stream] useEffect cleanup: Closing WebSocket."),s.current&&(clearTimeout(s.current),s.current=void 0),r.current&&(r.current.onopen=null,r.current.onmessage=null,r.current.onerror=null,r.current.onclose=null,r.current.readyState!==WebSocket.OPEN&&r.current.readyState!==WebSocket.CONNECTING||r.current.close(1e3),r.current=null),m("disconnected"),i.current=0}}),[e,c,l,n]);return{sendVideoFrame:(0,t.useCallback)((async e=>{if(!r.current||r.current.readyState!==WebSocket.OPEN)return;const n={models:{face:{},prosody:{}},data:function(e){let n="";const o=new Uint8Array(e),t=o.byteLength;for(let r=0;r<t;r++)n+=String.fromCharCode(o[r]);return window.btoa(n)}(e instanceof Blob?await e.arrayBuffer():e)};try{r.current.send(JSON.stringify(n))}catch(o){console.error("[Hume Stream] Error sending video frame:",o),g(o instanceof Error?o.message:String(o))}}),[]),connectionState:u,lastError:d}}}}]);