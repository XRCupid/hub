{"version":3,"file":"static/js/5107.727c75eb.chunk.js","mappings":"2HA2BO,MAAMA,EAAuBA,CAClCC,EACAC,EACAC,KAKA,MAAMC,GAAQC,EAAAA,EAAAA,QAAyB,MACjCC,GAAYD,EAAAA,EAAAA,SAAO,GACnBE,GAAoBF,EAAAA,EAAAA,QAAO,GAC3BG,GAAmBH,EAAAA,EAAAA,WAEnB,yBAAEI,GAA2B,EAAK,UAAEC,GAAY,GAAUP,GAAU,CAAC,GACpEQ,EAAiBC,IAAsBC,EAAAA,EAAAA,UAA0B,iBACjEC,EAAWC,IAAgBF,EAAAA,EAAAA,UAAwB,OAG1DG,EAAAA,EAAAA,YAAU,KACRV,EAAUW,SAAU,EACb,KACLX,EAAUW,SAAU,EAEhBT,EAAiBS,UACnBC,aAAaV,EAAiBS,SAC9BT,EAAiBS,aAAUE,MAG9B,KAEHH,EAAAA,EAAAA,YAAU,KAER,IAAKf,IAAWQ,IAA6BC,EAkB3C,OAjBIN,EAAMa,UACRG,QAAQC,IAAI,mEAEZjB,EAAMa,QAAQK,OAAS,KACvBlB,EAAMa,QAAQM,UAAY,KAC1BnB,EAAMa,QAAQO,QAAU,KACxBpB,EAAMa,QAAQQ,QAAU,KACxBrB,EAAMa,QAAQS,QACdtB,EAAMa,QAAU,KAEhBV,EAAkBU,QAAU,EACxBT,EAAiBS,UACjBC,aAAaV,EAAiBS,SAC9BT,EAAiBS,aAAUE,SAGjCP,EAAmB,gBAOrB,GAAIR,EAAMa,UAAYb,EAAMa,QAAQU,aAAeC,UAAUC,MAAQzB,EAAMa,QAAQU,aAAeC,UAAUE,YAE1G,YADAV,QAAQC,IAAI,yEAIdD,QAAQC,IAAI,+EACZT,EAAmB,cACnBG,EAAa,MAEb,MAAMgB,EAAK,6CAAAC,OAAgDC,mBAAmBhC,IACxEiC,EAAK,IAAIN,UAAUG,GACzBG,EAAGC,WAAa,cAChB/B,EAAMa,QAAUiB,EAEhBd,QAAQC,IAAI,iDAAkDU,GAE9DG,EAAGZ,OAAS,KACV,IAAKhB,EAAUW,SAAWb,EAAMa,UAAYiB,EAAI,OAChDd,QAAQC,IAAI,sCACZd,EAAkBU,QAAU,EAC5BL,EAAmB,aACnBG,EAAa,MAEb,MACMqB,EAAiB,CACrBC,OAAQ,CAAEC,KAAM,CAAC,EAAGC,QAAS,CAAC,GAC9BC,KAHyB,YAK3B,IACEpB,QAAQC,IAAI,yCAA0CoB,KAAKC,UAAUN,EAAe,KAAK,IACzFF,EAAGS,KAAKF,KAAKC,UAAUN,GACzB,CAAE,MAAOQ,GACPxB,QAAQyB,MAAM,+CAAgDD,GAC9D7B,EAAa6B,aAAeE,MAAQF,EAAIG,QAAUC,OAAOJ,GAE3D,GAGFV,EAAGX,UAAa0B,IACd,GAAK3C,EAAUW,SAAWb,EAAMa,UAAYiB,EAC5C,IAAK,IAADgB,EAAAC,EAAAC,EACF,MAAMZ,EAAOC,KAAKY,MAAMJ,EAAMT,MAE9B,GAAIA,EAAKK,MAIP,OAHAzB,QAAQyB,MAAM,gCAAiCL,EAAKK,OACpDjC,EAAmB,cACnBG,EAAa,mBAADiB,OAAoBQ,EAAKK,MAAME,SAAWP,EAAKK,QAI7D,GAAoB,QAApBK,EAAIV,EAAKc,mBAAW,IAAAJ,GAAK,QAALC,EAAhBD,EAAmB,UAAE,IAAAC,GAAU,QAAVC,EAArBD,EAAuBI,gBAAQ,IAAAH,GAA/BA,EAAiCI,OAAQ,CAC3C,MACMC,EADWjB,EAAKc,YAAY,GAAGC,SACTG,QAAO,CAACC,EAAOC,IAAWD,EAAEE,MAAQD,EAAEC,MAAQF,EAAIC,GAAI,CAAEE,KAAM,UAAWD,MAAO,IAC5G3D,EAAc,CACZ4D,KAAML,EAAWK,KAAKC,cACtBF,MAAOJ,EAAWI,OAEtB,CACF,CAAE,MAAOhB,GACPzB,QAAQyB,MAAM,0CAA2CA,GACzD9B,EAAa8B,aAAiBC,MAAQD,EAAME,QAAU,gBACxD,GAkFF,OAlCAb,EAAGV,QAAWyB,IACP3C,EAAUW,SAAWb,EAAMa,UAAYiB,IAC5Cd,QAAQyB,MAAM,iCAAkCI,GAChDlC,EAAa,+BAKfmB,EAAGT,QAAWwB,IACP3C,EAAUW,SAAWb,EAAMa,UAAYiB,IAC5Cd,QAAQC,IAAI,yCAADW,OAA0CiB,EAAMe,KAAI,eAAAhC,OAAciB,EAAMgB,OAAM,cAAAjC,OAAaiB,EAAMiB,WAC5G9D,EAAMa,QAAU,KAEG,MAAfgC,EAAMe,MAAgC,OAAff,EAAMe,MAC/B5C,QAAQC,IAAI,6DACZT,EAAmB,gBACnBL,EAAkBU,QAAU,EACxBT,EAAiBS,UACjBC,aAAaV,EAAiBS,SAC9BT,EAAiBS,aAAUE,IAEtBb,EAAUW,SAAWR,GAA4BC,GAE1DU,QAAQC,IAAI,oEACZN,EAAa,gCAADiB,OAAiCiB,EAAMe,KAAI,KAAAhC,OAAIiB,EAAMgB,SArE5CE,MACrB,IAAK7D,EAAUW,UAAYR,IAA6BC,GAAaH,EAAkBU,SAAW,EAM9F,YALIV,EAAkBU,SAAU,IAC5BG,QAAQyB,MAAM,oDACdjC,EAAmB,SACnBG,EAAa,wCAKrBR,EAAkBU,UAClB,MAAMmD,EAAQC,KAAKC,IAAI,IAAOD,KAAKE,IAAI,EAAGhE,EAAkBU,QAAS,GAAK,KAC1EG,QAAQC,IAAI,0CAADW,OAA2CzB,EAAkBU,QAAO,WAAAe,OAAUoC,EAAK,UAC9FxD,EAAmB,gBAGfJ,EAAiBS,SACjBC,aAAaV,EAAiBS,SAGlCT,EAAiBS,QAAUuD,OAAOC,YAAW,KACrCnE,EAAUW,SAAWR,GAA4BC,IAOjDU,QAAQC,IAAI,uEAMTjB,EAAMa,SAAWb,EAAMa,QAAQU,aAAeC,UAAUC,OACzDzB,EAAMa,QAAU,KAGhBL,EAAmB,kBAI1BwD,IA4BHD,KAGAvD,EAAmB,gBACnBL,EAAkBU,QAAU,KAKzB,KACLG,QAAQC,IAAI,uDAERb,EAAiBS,UACnBC,aAAaV,EAAiBS,SAC9BT,EAAiBS,aAAUE,GAEzBf,EAAMa,UAERb,EAAMa,QAAQK,OAAS,KACvBlB,EAAMa,QAAQM,UAAY,KAC1BnB,EAAMa,QAAQO,QAAU,KACxBpB,EAAMa,QAAQQ,QAAU,KACpBrB,EAAMa,QAAQU,aAAeC,UAAUC,MAAQzB,EAAMa,QAAQU,aAAeC,UAAUE,YACtF1B,EAAMa,QAAQS,MAAM,KAExBtB,EAAMa,QAAU,MAIlBL,EAAmB,gBACnBL,EAAkBU,QAAU,KAK7B,CAAChB,EAAQQ,EAA0BC,EAAWR,IAoBjD,MAAO,CAAEwE,gBAlBcC,EAAAA,EAAAA,cAAYC,UACjC,IAAKxE,EAAMa,SAAWb,EAAMa,QAAQU,aAAeC,UAAUC,KAE3D,OAEF,MACMkB,EAAU,CACdV,OAAQ,CAAEC,KAAM,CAAC,EAAGC,QAAS,CAAC,GAC9BC,KArPN,SAA6BqC,GAC3B,IAAIC,EAAS,GACb,MAAMC,EAAQ,IAAIC,WAAWH,GACvBI,EAAMF,EAAMG,WAClB,IAAK,IAAIC,EAAI,EAAGA,EAAIF,EAAKE,IACvBL,GAAU9B,OAAOoC,aAAaL,EAAMI,IAEtC,OAAOX,OAAOa,KAAKP,EACrB,CA0OuBQ,CAAoBC,aAAiBC,WAAaD,EAAME,cAAgBF,IAK3F,IACEnF,EAAMa,QAAQ0B,KAAKF,KAAKC,UAAUK,GACpC,CAAE,MAAOH,GACPxB,QAAQyB,MAAM,2CAA4CD,GAC1D7B,EAAa6B,aAAeE,MAAQF,EAAIG,QAAUC,OAAOJ,GAC3D,IACC,IAEsBjC,kBAAiBG,a,6KC3P5C,MAAM4E,EAAoC,CACxC,2CACA,2CACA,2CACA,2CACA,2CACA,2CACA,2CACA,2CACA,2CACA,4CAGIC,EAAiC,CACrC,sCACA,sCACA,iDACA,iDACA,iDACA,iDACA,iDACA,iDACA,iDACA,iDACA,iDACA,kDAYF,SAASC,EAAsBC,IAE7B7E,EAAAA,EAAAA,YAAU,KACJ6E,GAAkBA,EAAerC,OAAS,GAC5CsC,EAAAA,EAAQC,QAAQF,KAEjB,CAACA,IAEJ,MAAMG,GAAcF,EAAAA,EAAAA,GAAQD,GAAkB,IAyB9C,OAvB4BI,EAAAA,EAAAA,UAAQ,KAClC,IAAKJ,GAA4C,IAA1BA,EAAerC,SAAiBwC,GAAsC,IAAvBA,EAAYxC,OAChF,MAAO,CAAE0C,MAAO,GAAIC,MAAO,IAG7B,MAAMC,EAAkC,GAClCC,EAAqB,GAc3B,OAZuBC,MAAMC,QAAQP,GAAeA,EAAc,CAACA,IAEpDQ,SAAQ,CAACC,EAAMtB,KACvBsB,GAASA,EAAKC,WAInBD,EAAKC,WAAWF,SAASG,IACvBP,EAASQ,KAAKD,GACdN,EAASO,KAAKD,EAAK7C,SALnB1C,QAAQyF,KAAK,gDAAD7E,OAAiDmD,EAAC,YAAAnD,OAAW6D,EAAeV,GAAE,0CAQvF,CAAEe,MAAOE,EAAUD,MAAO,IAAI,IAAIW,IAAIT,OAC5C,CAACL,EAAaH,GAGnB,CAyGA,MAwBMkB,EAAmCA,CACvCC,EACAC,KACwB,IAADC,EAAAC,EAAAC,EACvBhG,QAAQC,IAAI,2CAA4CoB,KAAKC,UAAUsE,EAAMK,mBAC7EjG,QAAQC,IAAI,qCAAsC2F,EAAMM,YACxDlG,QAAQC,IAAI,qCAAsCoB,KAAKC,UAAUsE,EAAMO,aACvEnG,QAAQC,IAAI,4CAA6CoB,KAAKC,UAAUsE,EAAMQ,oBAG1ER,EAAMO,YAAcE,OAAOC,KAAKV,EAAMO,YAAY/D,OAAS,IAC7DpC,QAAQC,IAAI,mFACqBF,IAA7B6F,EAAMO,WAAWI,SACnBvG,QAAQC,IAAI,6CAADW,OAAoCgF,EAAMO,WAAWI,QAAO,+BAI3E,MAAM,WACJC,EAAU,UACVC,EAAS,oBACTC,EAAmB,QACnBC,EAAO,OACPC,EAAM,WACNV,EACAD,iBAAkBY,EAAoB,kBACtCT,EAAiB,mBACjBU,EAAkB,cAClBC,EAAa,cACbC,EAAa,cACbC,EAAa,mBACbC,EAAqB5C,EAAiC,mBACtD6C,EAAqB5C,EAA8B,WACnD4B,GACEP,EAEEwB,GAAWnI,EAAAA,EAAAA,QAAoB,OAGrCoI,EAAAA,EAAAA,qBAAoBxB,GAAK,IAAMuB,EAASvH,UAExC,MAAOyH,EAAqBC,IAA0B9H,EAAAA,EAAAA,UAAyB,OACxE+H,EAAaC,IAAkBhI,EAAAA,EAAAA,UAAiC,CAAEiI,aAAc,EAAGC,cAAe,IAEnGC,GAA0BrE,EAAAA,EAAAA,cAAasE,IAC3C,MAAMC,GAAmBC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GAAQF,GAAO,IAAEG,UAAWC,KAAKC,QAC1DX,EAAuBO,GACnBlC,EAAMuC,YACRvC,EAAMuC,WAAW,CAACL,MAEnB,CAAClC,EAAMuC,cAEJ,gBAAE5I,EAAe,UAAEG,EAAS,eAAE4D,IAAmB1E,EAAAA,EAAAA,GACrD4H,EACAoB,EACA,CACEvI,yBAA0B0H,EAC1BzH,UAAWyH,KAIfnH,EAAAA,EAAAA,YAAU,KACJF,GAAaiH,GAEfA,EAAQ,IAAIjF,MAAMhC,MAEnB,CAACA,EAAWiH,KAEf/G,EAAAA,EAAAA,YAAU,KACR,IAAIwI,EACAC,EAEJ,MAAMC,EAAeA,KACnBb,EAAe,CAAEC,aAAc,EAAGC,cAAe,IACjDU,EAAyBhF,YAAW,KAClCoE,EAAe,CAAEC,aAAc,EAAGC,cAAe,MAChD,KAGH,MAAMY,EAAiC,IAAhBtF,KAAKuF,SAAkB,IAC9CJ,EAAiB/E,WAAWiF,EAAcC,IAItCE,EAAoC,IAAhBxF,KAAKuF,SAAkB,IAGjD,OAFAJ,EAAiB/E,WAAWiF,EAAcG,GAEnC,KACL3I,aAAasI,GACbtI,aAAauI,MAEd,IAEH,MAAMK,GAAoB7D,EAAAA,EAAAA,UAAQ,IAC5Be,EAAM+C,eAED,CAAC,CAAEjG,KAAMkD,EAAM+C,eAAgBlG,MAAO,IAG3CoE,GAAwBA,EAAqBzE,OAAS,EAEjDyE,EAELS,EAEK,CAACA,GAGH,IACN,CAAC1B,EAAM+C,eAAgB9B,EAAsBS,IAChDtH,QAAQC,IAAI,2BAA4BoB,KAAKC,UAAUoH,EAAmB,KAAM,IAChF,MAAMrG,GAAawC,EAAAA,EAAAA,UAAQ,KAAM+D,EAAAA,EAAAA,GAAcF,GAAqB,KAAK,CAACA,IAEpEG,GAAsBhE,EAAAA,EAAAA,UAAQ,KAIlC,MAAMiE,GAAcC,EAAAA,EAAAA,GAAyB1G,EAAa,CAACA,GAAc,IAEzE,OADArC,QAAQC,IAAI,mDAAoDoB,KAAKC,UAAUwH,EAAa,KAAM,IAC3FA,IACN,CAACzG,IAEE2G,GAAanE,EAAAA,EAAAA,UAAQ,IAClB,IAAIoE,EAAAA,EAAqB,CAC9BC,sBAAuB,GACvBC,oBAAqB,GACrBC,gBAAiB,MAElB,IAEGC,GAAqBxE,EAAAA,EAAAA,UAAQ,KAGjC,MAAMyE,EAAmBpD,GAAeC,GAAcE,OAAOkD,QAAQpD,GAAYqD,MAAKC,IAAA,IAAEC,EAAKC,GAAMF,EAAA,OACjGC,EAAIE,WAAW,WAAaD,GAAS,GAAK,MAGtCE,EAA0B,CAC9BC,QAASR,GAAoBnD,GAAoB,CAAC,EAClDhE,SAAU0G,GAAuB,CAAC,EAClCkB,OAAQ3D,GAAqB,CAAC,EAC9B4D,KAAMxC,GAAe,CAAC,IAGnB8B,GAAoBnD,GAAcE,OAAOC,KAAKH,GAAY/D,OAAS,GACtEpC,QAAQC,IAAI,2FAAkFoB,KAAKC,UAAU6E,IAG/G,MAAM8D,EAAiBjB,EAAWkB,QAAQL,GAG1C,GAAI3D,EAAY,EACsBG,OAAOkD,QAAQU,GAAgBT,MAAKW,IAAA,IAAET,EAAKC,GAAMQ,EAAA,OACnFT,EAAIE,WAAW,WAAaD,GAAS,GAAK,QAGPM,EAAe1D,SAAW,GAAK,KAClE0D,EAAe1D,QAAU,GACzBvG,QAAQC,IAAI,mDAEhB,CAGA,OADAD,QAAQC,IAAI,8BAA+BoB,KAAKC,UAAU2I,EAAgB,KAAM,IACzEA,IACN,CAACjB,EAAY7C,EAAY0C,EAAqBzC,EAAmBoB,EAAatB,KAEzEpB,MAAOsF,EAAWrF,MAAOsF,GAAkB7F,EAAsB0C,IACjEpC,MAAOwF,EAAWvF,MAAOwF,GAAkB/F,EAAsB2C,GACnEqD,GAAqB3F,EAAAA,EAAAA,UAAQ,KAC/B,MAAM4F,EAAc,IAAIC,IAMxB,MALA,IAAIN,KAAcE,GAAWlF,SAAQG,IAC7BA,GAAQA,EAAK7C,OAAS+H,EAAYE,IAAIpF,EAAK7C,OAC3C+H,EAAYG,IAAIrF,EAAK7C,KAAM6C,MAG5BL,MAAM2F,KAAKJ,EAAYK,YAC/B,CAACV,EAAWE,KAERS,EAAsBC,IAA2BvL,EAAAA,EAAAA,UAAS,IAC1DwL,GAAsBC,KAA2BzL,EAAAA,EAAAA,UAAS,GAE3D0L,IAAuBtG,EAAAA,EAAAA,UAAQ,IAC/B6B,IACAR,EACKmE,EAAcjI,OAAS,EAAIiI,EAAcU,EAAuBV,EAAcjI,QAAU4E,GAAiB,SAE3GuD,EAAcnI,OAAS,EAAImI,EAAcU,GAAuBV,EAAcnI,QAAU6E,GAAiB,WAC/G,CAACf,EAAYQ,EAAqB2D,EAAeE,EAAeQ,EAAsBE,GAAsBjE,EAAeC,IAExHmE,IAAoB7H,EAAAA,EAAAA,cAAa8H,IAC/BzE,GACAA,MAEL,CAACA,IAEJ,OAAKH,GAMLzG,QAAQC,IAAI,gFACZD,QAAQC,IAAI,+CAAgDqL,GAC5DtL,QAAQC,IAAI,iDAAuD,OAALqL,QAAK,IAALA,OAAK,EAALA,EAAAA,UAC9DtL,QAAQC,IAAI,qFAA2F,OAALqL,QAAK,IAALA,IAAAA,EAAAA,QAClGtL,QAAQC,IAAI,0FAAgG,OAALqL,QAAK,IAALA,IAAAA,EAAAA,gBAEvGtL,QAAQC,IAAI,8EACZD,QAAQC,IAAI,6CAA+CmD,OAAekI,OAC1EtL,QAAQC,IAAI,+CAAqE,QAAvB6F,EAAG1C,OAAekI,aAAK,IAAAxF,OAAA,EAArBA,EAAuByF,UACnFvL,QAAQC,IAAI,2DAAiF,QAAtB8F,EAAE3C,OAAekI,aAAK,IAAAvF,IAArBA,EAAuByF,QAC/FxL,QAAQC,IAAI,mEAAyF,QAAtB+F,EAAE5C,OAAekI,aAAK,IAAAtF,IAArBA,EAAuByF,gBAGvGzL,QAAQC,IAAI,sEAAuEoB,KAAKC,UAAU+H,IAClGrJ,QAAQC,IAAI,wEAAyEoB,KAAKC,UAAU6E,IACpGnG,QAAQC,IAAI,6DAA8DkL,KAGxEO,EAAAA,EAAAA,MAACC,EAAAA,GAAM,CACLC,OAAQ,CAAEC,SAAU,CAAC,EAAG,EAAG,GAAIC,IAAK,IACpCC,MAAO,CAAEC,YAAa,QACtBC,SAAO,EACPC,GAAI,CACFC,WAAW,EACXC,OAAO,EACPC,YAAaf,EAAAA,sBACbgB,oBAAqB,EACrBC,iBAAkBjB,EAAAA,gBAClBkB,SAAA,EAEFC,EAAAA,EAAAA,KAAA,gBAAcC,UAAW,MACzBD,EAAAA,EAAAA,KAAA,cAAYZ,SAAU,CAAC,EAAG,EAAG,GAAIa,UAAW,MAC5CD,EAAAA,EAAAA,KAACE,EAAAA,EAAa,CACZC,cAAc,EACdC,WAAW,EACXC,YAAY,EACZC,YAAY,KAEdN,EAAAA,EAAAA,KAACO,EAAAA,QAAmB,CAClBvG,UAAWA,EACXwG,eAAgBzC,EAChBW,qBAAsBA,GACtB+B,cAAe7D,EACf8D,aAAc,CAAC,EACftB,SAAU,CAAC,GAAI,GAAK,GACpBuB,MAAO,EACPC,cAAejC,UAnDnBpL,QAAQyF,KAAK,mEACTkB,GAASA,EAAQ,IAAIjF,MAAM,oDACxB,OAsDXiE,EAAiC2H,YAAc,mCAI/C,MAAMC,EAA+BC,EAAAA,WAA+D7H,GACpG,EAAe6H,EAAAA,KAAWD,E","sources":["hooks/useHumeEmotionStream.ts","components/EmotionDrivenAvatar.tsx"],"sourcesContent":["import { useEffect, useRef, useCallback, useState } from 'react';\n\n// Add a type for WebSocket connection state\ntype ConnectionState = 'disconnected' | 'connecting' | 'connected' | 'error' | 'reconnecting';\n\ntype EmotionData = {\n  predictions?: Array<{\n    emotions: Array<{\n      name: string;\n      score: number;\n    }>;\n  }>;\n};\n\ntype OnEmotionDataCallback = (emotion: { name: string; score: number }) => void;\n\n// Helper function to convert ArrayBuffer to Base64 string\nfunction arrayBufferToBase64(buffer: ArrayBuffer): string {\n  let binary = '';\n  const bytes = new Uint8Array(buffer);\n  const len = bytes.byteLength;\n  for (let i = 0; i < len; i++) {\n    binary += String.fromCharCode(bytes[i]);\n  }\n  return window.btoa(binary);\n}\n\nexport const useHumeEmotionStream = (\n  apiKey: string | undefined,\n  onEmotionData: OnEmotionDataCallback,\n  config?: {\n    isEmotionDetectionActive?: boolean;\n    isVideoOn?: boolean;\n  }\n) => {\n  const wsRef = useRef<WebSocket | null>(null);\n  const isMounted = useRef(true); // Tracks component mount state\n  const reconnectAttempts = useRef(0); // Tracks attempts for current connection cycle\n  const reconnectTimeout = useRef<number | undefined>(); // Stores timeout ID for clearing\n\n  const { isEmotionDetectionActive = false, isVideoOn = false } = config || {};\n  const [connectionState, setConnectionState] = useState<ConnectionState>('disconnected');\n  const [lastError, setLastError] = useState<string | null>(null);\n\n  // Effect for component mount/unmount detection\n  useEffect(() => {\n    isMounted.current = true;\n    return () => {\n      isMounted.current = false;\n      // Clear any pending reconnect timeout when the component unmounts\n      if (reconnectTimeout.current) {\n        clearTimeout(reconnectTimeout.current);\n        reconnectTimeout.current = undefined;\n      }\n    };\n  }, []);\n\n  useEffect(() => {\n    // WebSocket should only be active if API key is present AND emotion detection/video are on\n    if (!apiKey || !isEmotionDetectionActive || !isVideoOn) {\n      if (wsRef.current) {\n        console.log('[Hume Stream] Conditions not met or changed. Closing WebSocket.');\n        // Clear handlers to prevent them from firing during or after close\n        wsRef.current.onopen = null;\n        wsRef.current.onmessage = null;\n        wsRef.current.onerror = null;\n        wsRef.current.onclose = null; \n        wsRef.current.close();\n        wsRef.current = null;\n        // Reset reconnect attempts when connection is intentionally closed\n        reconnectAttempts.current = 0; \n        if (reconnectTimeout.current) {\n            clearTimeout(reconnectTimeout.current);\n            reconnectTimeout.current = undefined;\n        }\n      }\n      setConnectionState('disconnected');\n      return; // Exit early if no connection should be active\n    }\n\n    // If a WebSocket connection already exists and is open or connecting, do nothing.\n    // This check prevents re-creating a WebSocket if this effect re-runs due to other dependency changes\n    // while a connection is already being established or is active.\n    if (wsRef.current && (wsRef.current.readyState === WebSocket.OPEN || wsRef.current.readyState === WebSocket.CONNECTING)) {\n      console.log('[Hume Stream] WebSocket already open or connecting. No action needed.');\n      return;\n    }\n\n    console.log('[Hume Stream] Conditions met. Attempting to establish WebSocket connection.');\n    setConnectionState('connecting');\n    setLastError(null);\n\n    const wsUrl = `wss://api.hume.ai/v0/stream/models?apiKey=${encodeURIComponent(apiKey)}`;\n    const ws = new WebSocket(wsUrl);\n    ws.binaryType = 'arraybuffer';\n    wsRef.current = ws; // Assign new WebSocket to ref immediately\n\n    console.log('[Hume Stream] WebSocket instance created. URL:', wsUrl);\n\n    ws.onopen = () => {\n      if (!isMounted.current || wsRef.current !== ws) return; // Stale closure check\n      console.log('[Hume Stream] WebSocket connected.');\n      reconnectAttempts.current = 0; // Reset on successful connection\n      setConnectionState('connected');\n      setLastError(null);\n\n      const initialFrameBase64 = \"dGVzdA==\"; // Placeholder \"test\"\n      const initialMessage = {\n        models: { face: {}, prosody: {} },\n        data: initialFrameBase64,\n      };\n      try {\n        console.log('[Hume Stream] Sending initial message:', JSON.stringify(initialMessage,null,2));\n        ws.send(JSON.stringify(initialMessage));\n      } catch (err) {\n        console.error('[Hume Stream] Error sending initial message:', err);\n        setLastError(err instanceof Error ? err.message : String(err));\n        // Consider closing if initial send fails, or let onclose/onerror handle it\n      }\n    };\n\n    ws.onmessage = (event) => {\n      if (!isMounted.current || wsRef.current !== ws) return; // Stale closure check\n      try {\n        const data = JSON.parse(event.data as string);\n        // console.log('[Hume Stream] Received message:', data);\n        if (data.error) {\n          console.error('[Hume Stream] Hume API error:', data.error);\n          setConnectionState('error');\n          setLastError(`Hume API Error: ${data.error.message || data.error}`);\n          return;\n        }\n        // Process emotion data\n        if (data.predictions?.[0]?.emotions?.length) {\n          const emotions = data.predictions[0].emotions;\n          const topEmotion = emotions.reduce((a:any, b:any) => (a.score > b.score ? a : b), { name: 'neutral', score: 0 });\n          onEmotionData({\n            name: topEmotion.name.toLowerCase(),\n            score: topEmotion.score\n          });\n        }\n      } catch (error) {\n        console.error('[Hume Stream] Error processing message:', error);\n        setLastError(error instanceof Error ? error.message : 'Unknown error');\n      }\n    };\n\n    const attemptReconnect = () => {\n        if (!isMounted.current || !isEmotionDetectionActive || !isVideoOn || reconnectAttempts.current >= 5) {\n            if (reconnectAttempts.current >=5) {\n                console.error('[Hume Stream] Max reconnection attempts reached.');\n                setConnectionState('error');\n                setLastError('Max reconnection attempts reached.');\n            }\n            return; // Don't reconnect if component unmounted, feature disabled, or max attempts reached\n        }\n\n        reconnectAttempts.current++;\n        const delay = Math.min(1000 * Math.pow(2, reconnectAttempts.current -1 ), 30000); // Adjust delay calculation\n        console.log(`[Hume Stream] Attempting to reconnect (${reconnectAttempts.current}/5) in ${delay}ms...`);\n        setConnectionState('reconnecting');\n        \n        // Clear previous timeout before setting a new one\n        if (reconnectTimeout.current) {\n            clearTimeout(reconnectTimeout.current);\n        }\n\n        reconnectTimeout.current = window.setTimeout(() => {\n            if (isMounted.current && isEmotionDetectionActive && isVideoOn) {\n                // Explicitly call the effect's logic to re-initiate connection\n                // This is a bit of a hack; ideally, the main useEffect re-runs.\n                // For now, we'll just create a new WebSocket instance directly if conditions are still met.\n                // This requires duplicating the ws creation logic or refactoring ws creation into a separate function.\n                // To avoid immediate complexity, let's re-evaluate if this manual reconnect is even needed\n                // given the main useEffect will try to establish connection if wsRef.current is null and conditions are met.\n                console.log('[Hume Stream] Reconnect timeout: Triggering new connection attempt.');\n                // Effectively, the main useEffect should handle this by finding wsRef.current is null.\n                // So, we just need to ensure wsRef.current is null after a definitive close.\n                // The main useEffect will then take over if conditions (apiKey, isVideoOn, etc.) are still met.\n                // Forcing a re-run or a direct call to a 'createAndConnectSocket' function might be cleaner.\n                // Let's ensure wsRef.current is null and let the main effect re-evaluate.\n                if(wsRef.current && wsRef.current.readyState !== WebSocket.OPEN) {\n                  wsRef.current = null; // This will make the main useEffect re-evaluate and try to connect\n                  // Manually trigger a state change to encourage re-render of the hook's consumer, potentially re-running this effect.\n                  // This is a bit hacky. A better way would be for `connect` to be a stable function that can be called.\n                  setConnectionState('connecting'); // Force re-evaluation, not ideal\n                }\n\n            }\n        }, delay);\n    };\n\n    ws.onerror = (event) => {\n      if (!isMounted.current || wsRef.current !== ws) return; // Stale closure check\n      console.error('[Hume Stream] WebSocket error:', event);\n      setLastError('WebSocket error occurred.'); // Avoid complex object in state\n      // Don't set to 'error' state immediately, let onclose handle definitive state or attempt reconnect\n      // attemptReconnect(); // onerror often followed by onclose, let onclose manage reconnect attempt\n    };\n\n    ws.onclose = (event) => {\n      if (!isMounted.current || wsRef.current !== ws) return; // Stale closure check\n      console.log(`[Hume Stream] WebSocket closed. Code: ${event.code}, Reason: '${event.reason}', Clean: ${event.wasClean}`);\n      wsRef.current = null; // Important: clear the ref when the socket is definitively closed.\n\n      if (event.code === 1000 || event.code === 1005) { // Normal closure or no status recieved (often client-side intentional close)\n        console.log('[Hume Stream] WebSocket closed normally or intentionally.');\n        setConnectionState('disconnected');\n        reconnectAttempts.current = 0; // Reset attempts on clean close\n        if (reconnectTimeout.current) {\n            clearTimeout(reconnectTimeout.current);\n            reconnectTimeout.current = undefined;\n        }\n      } else if (isMounted.current && isEmotionDetectionActive && isVideoOn) {\n        // Abnormal closure, and we should be connected\n        console.log('[Hume Stream] WebSocket closed abnormally. Attempting reconnect.');\n        setLastError(`WebSocket closed abnormally: ${event.code} ${event.reason}`);\n        attemptReconnect();\n      } else {\n        // Closed, but we shouldn't be connected (e.g., feature toggled off during connection attempt)\n        setConnectionState('disconnected');\n        reconnectAttempts.current = 0;\n      }\n    };\n\n    // Cleanup function for this effect\n    return () => {\n      console.log('[Hume Stream] useEffect cleanup: Closing WebSocket.');\n      // Clear any pending reconnect timeout first\n      if (reconnectTimeout.current) {\n        clearTimeout(reconnectTimeout.current);\n        reconnectTimeout.current = undefined;\n      }\n      if (wsRef.current) {\n        // Remove handlers to prevent them from firing after this cleanup logic\n        wsRef.current.onopen = null;\n        wsRef.current.onmessage = null;\n        wsRef.current.onerror = null;\n        wsRef.current.onclose = null;\n        if (wsRef.current.readyState === WebSocket.OPEN || wsRef.current.readyState === WebSocket.CONNECTING) {\n            wsRef.current.close(1000); // Send a normal closure code\n        }\n        wsRef.current = null;\n      }\n       // When the effect cleans up because dependencies changed (e.g., video toggled off),\n       // or component unmounts, ensure connection state is 'disconnected'.\n      setConnectionState('disconnected'); \n      reconnectAttempts.current = 0; // Reset reconnect attempts\n    };\n  // Dependencies: apiKey ensures re-connection if it changes.\n  // isEmotionDetectionActive and isVideoOn control the active state.\n  // onEmotionData is needed to correctly set up the onmessage handler with the latest callback.\n  }, [apiKey, isEmotionDetectionActive, isVideoOn, onEmotionData]); \n\n  const sendVideoFrame = useCallback(async (frame: Blob | ArrayBuffer) => {\n    if (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) {\n      // console.warn('[Hume Stream] WebSocket not open. Cannot send video frame.');\n      return;\n    }\n    const base64Data = arrayBufferToBase64(frame instanceof Blob ? await frame.arrayBuffer() : frame);\n    const message = {\n      models: { face: {}, prosody: {} },\n      data: base64Data,\n    };\n    try {\n      wsRef.current.send(JSON.stringify(message));\n    } catch (err) {\n      console.error('[Hume Stream] Error sending video frame:', err);\n      setLastError(err instanceof Error ? err.message : String(err));\n    }\n  }, []); // No dependencies, relies on wsRef.current which is managed by useEffect\n\n  return { sendVideoFrame, connectionState, lastError };\n};\n","import process from 'process';\nimport React, { useState, useEffect, useRef, useCallback, useMemo, useImperativeHandle, forwardRef } from 'react';\nimport { Canvas, useFrame, useThree, extend } from '@react-three/fiber';\nimport { OrbitControls, useAnimations, useGLTF } from '@react-three/drei'; // Keep useGLTF for animations if still used there\nimport { GLTFLoader, KTX2Loader, MeshoptDecoder, type GLTF } from 'three-stdlib';\n\n\nimport { DRACOLoader } from 'three/examples/jsm/loaders/DRACOLoader.js';\n\nimport * as THREE from 'three';\nimport { ARKitBlendshapeNamesList, BlendshapeKey } from '../types/blendshapes';\nimport { Group, Mesh, SkinnedMesh, Vector3Tuple, AnimationClip, LoopRepeat } from 'three';\nimport { useHumeEmotionStream } from '../hooks/useHumeEmotionStream';\nimport SimulationAvatar3D from './TestAvatar'; // Re-enable for testing with logs, now points to TestAvatar.tsx\nimport { GroupProps } from '@react-three/fiber';\n\n// We don't need ReadyPlayerMeAvatarProps here directly anymore, SimulationAvatar3D handles its own prop types.\nimport { mapEmotionsToBlendshapes, getTopEmotion } from '../utils/emotionMappings';\nimport { BlendshapeCompositor, BlendshapeInput } from '../utils/blendshapeCompositor';\n\n// Animation file lists (relative to /public directory)\nconst MASCULINE_TALKING_ANIMATION_FILES = [\n  \"/animations/M_Talking_Variations_001.glb\",\n  \"/animations/M_Talking_Variations_002.glb\",\n  \"/animations/M_Talking_Variations_003.glb\",\n  \"/animations/M_Talking_Variations_004.glb\",\n  \"/animations/M_Talking_Variations_005.glb\",\n  \"/animations/M_Talking_Variations_006.glb\",\n  \"/animations/M_Talking_Variations_007.glb\",\n  \"/animations/M_Talking_Variations_008.glb\",\n  \"/animations/M_Talking_Variations_009.glb\",\n  \"/animations/M_Talking_Variations_010.glb\",\n];\n\nconst MASCULINE_IDLE_ANIMATION_FILES = [\n  \"/animations/M_Standing_Idle_001.glb\",\n  \"/animations/M_Standing_Idle_002.glb\",\n  \"/animations/M_Standing_Idle_Variations_001.glb\",\n  \"/animations/M_Standing_Idle_Variations_002.glb\",\n  \"/animations/M_Standing_Idle_Variations_003.glb\",\n  \"/animations/M_Standing_Idle_Variations_004.glb\",\n  \"/animations/M_Standing_Idle_Variations_005.glb\",\n  \"/animations/M_Standing_Idle_Variations_006.glb\",\n  \"/animations/M_Standing_Idle_Variations_007.glb\",\n  \"/animations/M_Standing_Idle_Variations_008.glb\",\n  \"/animations/M_Standing_Idle_Variations_009.glb\",\n  \"/animations/M_Standing_Idle_Variations_010.glb\",\n];\n\n// Helper to extract filename for unique naming\nconst getUniqueAnimName = (path: string, prefix: string) => {\n  const filename = path.split('/').pop()?.replace('.glb', '');\n  return `${prefix}_${filename}`;\n};\n\n// Helper to get a filename without extension for prefixing, if needed\n// const getBaseFilename = (path: string) => path.split('/').pop()?.split('.')[0] || 'anim';\n\nfunction useExternalAnimations(animationPaths?: string[]): { clips: THREE.AnimationClip[], names: string[] } {\n  // Preload all paths. useGLTF.preload is not a hook.\n  useEffect(() => {\n    if (animationPaths && animationPaths.length > 0) {\n      useGLTF.preload(animationPaths);\n    }\n  }, [animationPaths]);\n\n  const gltfResults = useGLTF(animationPaths || []) as GLTF[]; // Cast to GLTF[] for type safety\n\n  const processedAnimations = useMemo(() => {\n    if (!animationPaths || animationPaths.length === 0 || !gltfResults || gltfResults.length === 0) {\n      return { clips: [], names: [] };\n    }\n\n    const allClips: THREE.AnimationClip[] = [];\n    const allNames: string[] = [];\n\n    const gltfsToProcess = Array.isArray(gltfResults) ? gltfResults : [gltfResults];\n\n    gltfsToProcess.forEach((gltf, i) => {\n      if (!gltf || !gltf.animations) {\n        console.warn(`[useExternalAnimations] GLTF result at index ${i} (path: ${animationPaths[i]}) is invalid or has no animations.`);\n        return;\n      }\n      gltf.animations.forEach((clip) => {\n        allClips.push(clip); \n        allNames.push(clip.name);\n      });\n    });\n    return { clips: allClips, names: [...new Set(allNames)] }; \n  }, [gltfResults, animationPaths]);\n\n  return processedAnimations;\n}\n\n// ======================== TYPES ========================\n\n// Standard ARKit blendshape names\nexport type ARKitBlendshapeName =\n  | 'browDownLeft'\n  | 'browDownRight'\n  | 'browInnerUp'\n  | 'browOuterUpLeft'\n  | 'browOuterUpRight'\n  | 'cheekPuff'\n  | 'cheekSquintLeft'\n  | 'cheekSquintRight'\n  | 'eyeBlinkLeft'\n  | 'eyeBlinkRight'\n  | 'eyeLookDownLeft'\n  | 'eyeLookDownRight'\n  | 'eyeLookInLeft'\n  | 'eyeLookInRight'\n  | 'eyeLookOutLeft'\n  | 'eyeLookOutRight'\n  | 'eyeLookUpLeft'\n  | 'eyeLookUpRight'\n  | 'eyeSquintLeft'\n  | 'eyeSquintRight'\n  | 'eyeWideLeft'\n  | 'eyeWideRight'\n  | 'jawForward'\n  | 'jawLeft'\n  | 'jawOpen'\n  | 'jawRight'\n  | 'mouthClose'\n  | 'mouthDimpleLeft'\n  | 'mouthDimpleRight'\n  | 'mouthFrownLeft'\n  | 'mouthFrownRight'\n  | 'mouthFunnel'\n  | 'mouthLeft'\n  | 'mouthLowerDownLeft'\n  | 'mouthLowerDownRight'\n  | 'mouthPressLeft'\n  | 'mouthPressRight'\n  | 'mouthPucker'\n  | 'mouthRight'\n  | 'mouthRollLower'\n  | 'mouthRollUpper'\n  | 'mouthShrugLower'\n  | 'mouthShrugUpper'\n  | 'mouthSmileLeft'\n  | 'mouthSmileRight'\n  | 'mouthStretchLeft'\n  | 'mouthStretchRight'\n  | 'mouthUpperUpLeft'\n  | 'mouthUpperUpRight'\n  | 'noseSneerLeft'\n  | 'noseSneerRight'\n  | 'tongueOut';\n\nexport type BlendShapeMap = Partial<Record<ARKitBlendshapeName, number>>;\n\n\nexport interface Emotion {\n  name: string;\n  score: number;\n  timestamp?: number; // Added timestamp\n}\n\ninterface Avatar3DProps {\n  url: string;\n  visemeShapes?: Partial<BlendShapeMap>; // For viseme-driven facial expressions\n  emotionShapes?: Partial<BlendShapeMap>; // For emotion-driven facial expressions\n  isSpeaking?: boolean; // To indicate if avatar is currently speaking (for visemes)\n  position?: Vector3Tuple;\n  scale?: number | Vector3Tuple;\n  onLoaded?: () => void;\n  onModelLoaded?: (model: Group) => void;\n  currentAnimationName?: string; // For body animation\n  additionalClips?: THREE.AnimationClip[]; // For externally loaded animations\n}\n\ninterface EmotionDrivenAvatarProps {\n  humeApiKey?: string;\n  avatarUrl: string;\n  visemeBlendshapes?: Partial<BlendShapeMap>; // Added for viseme-driven blendshapes\n  activeBodyAnimation?: string; // New prop for body animation\n  onError?: (error: Error) => void;\n  onLoad?: () => void;\n  onEmotionDetected?: (emotion: Emotion) => void;\n  isSpeaking: boolean; // Added to control avatar speaking state from parent\n  visemeData?: Record<string, number>; // Added for direct viseme data input\n  detectedEmotions?: Emotion[]; // New prop for receiving emotion data\n  directBlendshapes?: Partial<BlendShapeMap>; // For direct blendshape control\n  emotionBlendshapes?: Partial<BlendShapeMap>; // For emotion-driven blendshapes (e.g., from prosody)\n  cameraEnabled?: boolean;\n  talkAnimation?: string; // Fallback if paths are not provided\n  idleAnimation?: string; // Fallback if paths are not provided\n  talkAnimationPaths?: string[]; // Paths to GLBs for talking animations\n  idleAnimationPaths?: string[]; // Paths to GLBs for idle animations\n  onEmotions?: (emotions: Emotion[]) => void; // Added onEmotions callback\n  currentEmotion?: string; // For direct emotion string input from parent\n  idleShapes?: Partial<BlendShapeMap>; // For idle blinking or resting expression\n}\n\n// Default light properties for Avatar3D\nconst AVATAR_AMBIENT_LIGHT_INTENSITY = 0.9; // Slightly increased\nconst AVATAR_DIRECTIONAL_LIGHT_POSITION = { x: 5, y: 5, z: 5 }; // Simple object for position\nconst AVATAR_DIRECTIONAL_LIGHT_INTENSITY = 0.8; // Reduced intensity\nconst AVATAR_HEMISPHERE_SKY_COLOR = 0xffffbb;\nconst AVATAR_HEMISPHERE_GROUND_COLOR = 0x080820;\nconst AVATAR_HEMISPHERE_INTENSITY = 0.6;\n\n// Local emotion mapping helpers (ARKIT_EMOTION_MAP_MINIMAL_V2 and local mapEmotionsToBlendshapes) removed.\n// Using imported mapEmotionsToBlendshapes from ../utils/emotionMappings.ts which has its own internal map.\n\n// ======================== COMPONENTS ========================\n\n// Define the InlineBox component here, within the same file scope\nconst InlineBox = (props: GroupProps) => {\n  return (\n    <group {...props}>\n      <mesh position={[0, 0.5, 0]}> {/* Centered, assuming pivot is at feet */}\n        <boxGeometry args={[0.5, 1, 0.5]} /> {/* Approx human-like proportions */}\n        <meshStandardMaterial color=\"purple\" /> {/* Distinct color */}\n      </mesh>\n    </group>\n  );\n};\n\nconst EmotionDrivenAvatarComponentBody = (\n  props: EmotionDrivenAvatarProps,\n  ref: React.ForwardedRef<THREE.Group | null>\n): JSX.Element | null => {\n  console.log('[EDA] Props received - detectedEmotions:', JSON.stringify(props.detectedEmotions));\n  console.log('[EDA] Props received - isSpeaking:', props.isSpeaking);\n  console.log('[EDA] Props received - visemeData:', JSON.stringify(props.visemeData));\n  console.log('[EDA] Props received - directBlendshapes:', JSON.stringify(props.directBlendshapes));\n\n  // DEBUG: Check for problematic viseme data overriding face tracking\n  if (props.visemeData && Object.keys(props.visemeData).length > 0) {\n    console.log('[EDA] ⚠️ VISEME DATA DETECTED - This may override face tracking!');\n    if (props.visemeData.jawOpen !== undefined) {\n      console.log(`[EDA] ⚠️ jawOpen in visemeData: ${props.visemeData.jawOpen} (this will override ML5)`);\n    }\n  }\n\n  const {\n    humeApiKey,\n    avatarUrl,\n    activeBodyAnimation,\n    onError,\n    onLoad,\n    isSpeaking,\n    detectedEmotions: propDetectedEmotions,\n    directBlendshapes,\n    emotionBlendshapes,\n    cameraEnabled,\n    talkAnimation, \n    idleAnimation, \n    talkAnimationPaths = MASCULINE_TALKING_ANIMATION_FILES,\n    idleAnimationPaths = MASCULINE_IDLE_ANIMATION_FILES,\n    visemeData, // Added to destructure from props\n  } = props;\n\n  const modelRef = useRef<THREE.Group>(null); // Local ref for SimulationAvatar3D\n  \n  // Forward the ref to the internal modelRef if the parent needs access to the THREE.Group\n  useImperativeHandle(ref, () => modelRef.current as THREE.Group);\n\n  const [latestStreamEmotion, setLatestStreamEmotion] = useState<Emotion | null>(null);\n  const [blinkShapes, setBlinkShapes] = useState<Partial<BlendShapeMap>>({ eyeBlinkLeft: 0, eyeBlinkRight: 0 });\n\n  const handleStreamEmotionData = useCallback((emotion: { name: string; score: number; }) => {\n    const newEmotion: Emotion = { ...emotion, timestamp: Date.now() };\n    setLatestStreamEmotion(newEmotion);\n    if (props.onEmotions) {\n      props.onEmotions([newEmotion]);\n    }\n  }, [props.onEmotions]);\n\n  const { connectionState, lastError, sendVideoFrame } = useHumeEmotionStream(\n    humeApiKey, // Pass apiKey directly, hook handles undefined\n    handleStreamEmotionData, // Pass the new callback\n    { // Pass config object\n      isEmotionDetectionActive: cameraEnabled,\n      isVideoOn: cameraEnabled \n    }\n  );\n\n  useEffect(() => {\n    if (lastError && onError) {\n      // lastError from useHumeEmotionStream is string | null\n      onError(new Error(lastError)); \n    }\n  }, [lastError, onError]);\n\n  useEffect(() => {\n    let blinkTimeoutId: NodeJS.Timeout;\n    let blinkDurationTimeoutId: NodeJS.Timeout;\n\n    const triggerBlink = () => {\n      setBlinkShapes({ eyeBlinkLeft: 1, eyeBlinkRight: 1 });\n      blinkDurationTimeoutId = setTimeout(() => {\n        setBlinkShapes({ eyeBlinkLeft: 0, eyeBlinkRight: 0 });\n      }, 150); // Blink duration: 150ms\n\n      // Schedule next blink randomly between 3 to 7 seconds\n      const nextBlinkDelay = Math.random() * 4000 + 3000;\n      blinkTimeoutId = setTimeout(triggerBlink, nextBlinkDelay);\n    };\n\n    // Start the first blink after a short delay\n    const initialBlinkDelay = Math.random() * 4000 + 1000; // Initial delay 1-5 seconds\n    blinkTimeoutId = setTimeout(triggerBlink, initialBlinkDelay);\n\n    return () => {\n      clearTimeout(blinkTimeoutId);\n      clearTimeout(blinkDurationTimeoutId);\n    };\n  }, []); // Empty dependency array ensures this runs once on mount and cleans up on unmount\n\n  const emotionsToProcess = useMemo(() => {\n    if (props.currentEmotion) { // Prioritize direct string from parent\n      // console.log(`[EDA] Using props.currentEmotion: ${props.currentEmotion}`);\n      return [{ name: props.currentEmotion, score: 1.0 }]; // Convert to Emotion[]\n    }\n    // Fallback to existing logic if props.currentEmotion is not provided\n    if (propDetectedEmotions && propDetectedEmotions.length > 0) {\n      // console.log('[EDA] Using props.propDetectedEmotions');\n      return propDetectedEmotions;\n    }\n    if (latestStreamEmotion) {\n      // console.log('[EDA] Using internal latestStreamEmotion');\n      return [latestStreamEmotion];\n    }\n    // console.log('[EDA] No emotion source found, returning empty array.');\n    return [];\n  }, [props.currentEmotion, propDetectedEmotions, latestStreamEmotion]);\n  console.log('[EDA] emotionsToProcess:', JSON.stringify(emotionsToProcess, null, 2));\n  const topEmotion = useMemo(() => getTopEmotion(emotionsToProcess || []), [emotionsToProcess]);\n\n  const activeEmotionShapes = useMemo(() => {\n    // Use the imported mapEmotionsToBlendshapes function\n    // It expects an array of emotions and uses its internal mapping.\n    // Pass an empty array if topEmotion is null to avoid errors and get a neutral/zeroed map.\n    const blendshapes = mapEmotionsToBlendshapes(topEmotion ? [topEmotion] : []);\n    console.log('[EDA] Blendshapes from mapEmotionsToBlendshapes:', JSON.stringify(blendshapes, null, 2));\n    return blendshapes;\n  }, [topEmotion]);\n\n  const compositor = useMemo(() => {\n    return new BlendshapeCompositor({\n      emotionMouthReduction: 0.3,  // Reduce emotion mouth shapes when visemes are active\n      emotionFaceBlending: 0.8,    // Keep emotion eye/brow shapes strong\n      smoothingFactor: 0.1         // Light smoothing for transitions\n    });\n  }, []);\n\n  const finalEmotionShapes = useMemo(() => {\n    // Only use visemeData when actually speaking or when it contains significant mouth movement\n    // This prevents idle/test visemes from overriding ML5 face tracking\n    const shouldUseVisemes = isSpeaking || (visemeData && Object.entries(visemeData).some(([key, value]) => \n      key.startsWith('mouth') && (value || 0) > 0.1\n    ));\n    \n    const inputs: BlendshapeInput = {\n      visemes: shouldUseVisemes ? (visemeData || {}) : {}, // Only use visemes when speaking or with significant mouth movement\n      emotions: activeEmotionShapes || {}, // Emotional expressions\n      manual: directBlendshapes || {},     // Manual overrides (highest priority overall)\n      base: blinkShapes || {}              // Base/idle state (blinking)\n    };\n    \n    if (!shouldUseVisemes && visemeData && Object.keys(visemeData).length > 0) {\n      console.log('[EDA] 🚫 Ignoring visemeData (not speaking and no significant mouth movement):', JSON.stringify(visemeData));\n    }\n    \n    const composedShapes = compositor.compose(inputs);\n    \n    // Add fallback jaw animation if speaking but no significant mouth movement\n    if (isSpeaking) {\n      const hasSignificantMouthMovement = Object.entries(composedShapes).some(([key, value]) => \n        key.startsWith('mouth') && (value || 0) > 0.1\n      );\n      \n      if (!hasSignificantMouthMovement && (composedShapes.jawOpen || 0) < 0.1) {\n        composedShapes.jawOpen = 0.4; // Fallback jaw animation\n        console.log('[EDA] Applied fallback jaw animation for speech');\n      }\n    }\n    \n    console.log('[EDA] Composed blendshapes:', JSON.stringify(composedShapes, null, 2));\n    return composedShapes;\n  }, [compositor, visemeData, activeEmotionShapes, directBlendshapes, blinkShapes, isSpeaking]);\n\n  const { clips: talkClips, names: talkAnimNames } = useExternalAnimations(talkAnimationPaths);\n  const { clips: idleClips, names: idleAnimNames } = useExternalAnimations(idleAnimationPaths);\n  const allAdditionalClips = useMemo(() => {\n      const uniqueClips = new Map<string, THREE.AnimationClip>();\n      [...talkClips, ...idleClips].forEach(clip => {\n          if (clip && clip.name && !uniqueClips.has(clip.name)) {\n              uniqueClips.set(clip.name, clip);\n          }\n      });\n      return Array.from(uniqueClips.values());\n  }, [talkClips, idleClips]);\n\n  const [currentTalkAnimIndex, setCurrentTalkAnimIndex] = useState(0);\n  const [currentIdleAnimIndex, setCurrentIdleAnimIndex] = useState(0);\n  \n  const currentAnimationName = useMemo(() => {\n    if (activeBodyAnimation) return activeBodyAnimation;\n    if (isSpeaking) {\n      return talkAnimNames.length > 0 ? talkAnimNames[currentTalkAnimIndex % talkAnimNames.length] : talkAnimation || 'Talk_0';\n    }\n    return idleAnimNames.length > 0 ? idleAnimNames[currentIdleAnimIndex % idleAnimNames.length] : idleAnimation || 'Idle_0';\n  }, [isSpeaking, activeBodyAnimation, talkAnimNames, idleAnimNames, currentTalkAnimIndex, currentIdleAnimIndex, talkAnimation, idleAnimation]);\n\n  const handleModelLoaded = useCallback((loadedModel: THREE.Group) => {\n      if (onLoad) {\n          onLoad();\n      }\n  }, [onLoad]);\n\n  if (!avatarUrl) {\n    console.warn(\"EmotionDrivenAvatar: avatarUrl is not provided. Rendering null.\");\n    if (onError) onError(new Error(\"EmotionDrivenAvatar: avatarUrl is not provided.\"));\n    return null; \n  }\n\n  console.log(\"[EmotionDrivenAvatar] Checking IMPORTED THREE instance before Canvas render:\");\n  console.log(\"[EmotionDrivenAvatar] IMPORTED THREE object:\", THREE);\n  console.log(\"[EmotionDrivenAvatar] IMPORTED THREE.REVISION:\", THREE?.REVISION);\n  console.log(\"[EmotionDrivenAvatar] Is IMPORTED THREE.Cache available (added by threejs core)?\", !!THREE?.Cache);\n  console.log(\"[EmotionDrivenAvatar] Is IMPORTED THREE.CanvasTexture available (used by R3F Canvas)?\", !!THREE?.CanvasTexture);\n\n  console.log(\"[EmotionDrivenAvatar] Checking WINDOW.THREE instance before Canvas render:\");\n  console.log(\"[EmotionDrivenAvatar] WINDOW.THREE object:\", (window as any).THREE);\n  console.log(\"[EmotionDrivenAvatar] WINDOW.THREE.REVISION:\", (window as any).THREE?.REVISION);\n  console.log(\"[EmotionDrivenAvatar] Is WINDOW.THREE.Cache available?\", !!(window as any).THREE?.Cache);\n  console.log(\"[EmotionDrivenAvatar] Is WINDOW.THREE.CanvasTexture available?\", !!(window as any).THREE?.CanvasTexture);\n\n  // Log states being passed to SimulationAvatar3D\n  console.log('[EDA] Values passed to SimAvatar3D - finalEmotionShapes (COMPOSED):', JSON.stringify(finalEmotionShapes));\n  console.log('[EDA] Values passed to SimAvatar3D - visemeData (RAW, for reference):', JSON.stringify(visemeData));\n  console.log('[EDA] Values passed to SimAvatar3D - currentAnimationName:', currentAnimationName);\n\n  return (\n    <Canvas\n      camera={{ position: [0, 0, 2], fov: 50 }} // Values from before the bad edit\n      style={{ touchAction: 'none' }}\n      shadows\n      gl={{\n        antialias: true,\n        alpha: true,\n        toneMapping: THREE.ACESFilmicToneMapping,\n        toneMappingExposure: 1.0,\n        outputColorSpace: THREE.SRGBColorSpace,\n      }}\n    >\n      <ambientLight intensity={0.8} />\n      <pointLight position={[5, 5, 5]} intensity={0.8} />\n      <OrbitControls \n        enableRotate={false}\n        enablePan={false}\n        enableZoom={false}\n        autoRotate={false}\n      />\n      <SimulationAvatar3D // This is TestAvatar\n        avatarUrl={avatarUrl} // Pass the avatarUrl\n        animationClips={allAdditionalClips} // Pass the loaded animation clips\n        currentAnimationName={currentAnimationName} // Pass the determined current animation name\n        emotionShapes={finalEmotionShapes} // Pass the composed blendshapes (includes visemes, emotions, manual, blinks)\n        visemeShapes={{}} // Empty since finalEmotionShapes now contains composed visemes\n        position={[0, -0.8, 0]}\n        scale={1.0}\n        onModelLoaded={handleModelLoaded} // Pass onModelLoaded callback\n      />\n    </Canvas>\n  );\n}; // Closing EmotionDrivenAvatarComponentBody\nEmotionDrivenAvatarComponentBody.displayName = 'EmotionDrivenAvatarComponentBody';\n\n// Assuming EmotionDrivenAvatarProps is defined above in the file or imported.\n// The export structure from before the error:\nconst ForwardedEmotionDrivenAvatar = React.forwardRef<THREE.Group | null, EmotionDrivenAvatarProps>(EmotionDrivenAvatarComponentBody as any); // Using 'as any' to bridge potential ref type mismatches for now, this complex export needs review later.\nexport default React.memo(ForwardedEmotionDrivenAvatar);\n"],"names":["useHumeEmotionStream","apiKey","onEmotionData","config","wsRef","useRef","isMounted","reconnectAttempts","reconnectTimeout","isEmotionDetectionActive","isVideoOn","connectionState","setConnectionState","useState","lastError","setLastError","useEffect","current","clearTimeout","undefined","console","log","onopen","onmessage","onerror","onclose","close","readyState","WebSocket","OPEN","CONNECTING","wsUrl","concat","encodeURIComponent","ws","binaryType","initialMessage","models","face","prosody","data","JSON","stringify","send","err","error","Error","message","String","event","_data$predictions","_data$predictions$","_data$predictions$$em","parse","predictions","emotions","length","topEmotion","reduce","a","b","score","name","toLowerCase","code","reason","wasClean","attemptReconnect","delay","Math","min","pow","window","setTimeout","sendVideoFrame","useCallback","async","buffer","binary","bytes","Uint8Array","len","byteLength","i","fromCharCode","btoa","arrayBufferToBase64","frame","Blob","arrayBuffer","MASCULINE_TALKING_ANIMATION_FILES","MASCULINE_IDLE_ANIMATION_FILES","useExternalAnimations","animationPaths","useGLTF","preload","gltfResults","useMemo","clips","names","allClips","allNames","Array","isArray","forEach","gltf","animations","clip","push","warn","Set","EmotionDrivenAvatarComponentBody","props","ref","_THREE","_THREE2","_THREE3","detectedEmotions","isSpeaking","visemeData","directBlendshapes","Object","keys","jawOpen","humeApiKey","avatarUrl","activeBodyAnimation","onError","onLoad","propDetectedEmotions","emotionBlendshapes","cameraEnabled","talkAnimation","idleAnimation","talkAnimationPaths","idleAnimationPaths","modelRef","useImperativeHandle","latestStreamEmotion","setLatestStreamEmotion","blinkShapes","setBlinkShapes","eyeBlinkLeft","eyeBlinkRight","handleStreamEmotionData","emotion","newEmotion","_objectSpread","timestamp","Date","now","onEmotions","blinkTimeoutId","blinkDurationTimeoutId","triggerBlink","nextBlinkDelay","random","initialBlinkDelay","emotionsToProcess","currentEmotion","getTopEmotion","activeEmotionShapes","blendshapes","mapEmotionsToBlendshapes","compositor","BlendshapeCompositor","emotionMouthReduction","emotionFaceBlending","smoothingFactor","finalEmotionShapes","shouldUseVisemes","entries","some","_ref","key","value","startsWith","inputs","visemes","manual","base","composedShapes","compose","_ref2","talkClips","talkAnimNames","idleClips","idleAnimNames","allAdditionalClips","uniqueClips","Map","has","set","from","values","currentTalkAnimIndex","setCurrentTalkAnimIndex","currentIdleAnimIndex","setCurrentIdleAnimIndex","currentAnimationName","handleModelLoaded","loadedModel","THREE","REVISION","Cache","CanvasTexture","_jsxs","Canvas","camera","position","fov","style","touchAction","shadows","gl","antialias","alpha","toneMapping","toneMappingExposure","outputColorSpace","children","_jsx","intensity","OrbitControls","enableRotate","enablePan","enableZoom","autoRotate","SimulationAvatar3D","animationClips","emotionShapes","visemeShapes","scale","onModelLoaded","displayName","ForwardedEmotionDrivenAvatar","React"],"sourceRoot":""}